[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Stat340: Bayesian Statistics",
    "section": "",
    "text": "MEETINGS:\nMWF 3a\n11:10-12:20 MW; 12-1 F\nCMC 306\n\n\nPROFESSOR:\nAmanda Luby\naluby@carleton\nCMC 223\n\n\nDROP-IN HOURS:\nMon 12:30-1:30\nTues 2-3\nWed 3-4 (120 priority)\nFri 10-11 (340 priority)\nCMC 307\n\n\nWEBSITE:\nmoodle.carleton.edu\naluby.github.io/stat340-s25\n\n\n\nTEXT:\nBayes Rules! An Introduction to Applied Bayesian Modeling\n\n\n\nSOFTWARE\nR: free download from r-project.org/\nRStudio: free download from rstudio.com/downloads\n{rstan}: R package for MCMC sampling",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#meetings",
    "href": "course-syllabus.html#meetings",
    "title": "Stat340: Bayesian Statistics",
    "section": "Meetings",
    "text": "Meetings\nThere will be three course meetings per week (Mondays, Wednesdays, and Fridays). Daily attendance and active participation is expected. Course meetings will combine demonstrations/lecture and in-class group exercises. Each course day has an associated reading from the textbook. I expect you to complete the reading before class, and returning to the textbook afterwards if you have any lingering confusion about the material from the day.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#individual-assignments",
    "href": "course-syllabus.html#individual-assignments",
    "title": "Stat340: Bayesian Statistics",
    "section": "Individual Assignments",
    "text": "Individual Assignments\nIndividual homework will be assigned once-ish per week, typically due on Fridays at the beginning of class. You will submit homework assignments via gradescope. You will use {quarto} for all assignments. Individual assignments will be graded on completion.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#group-assignments",
    "href": "course-syllabus.html#group-assignments",
    "title": "Stat340: Bayesian Statistics",
    "section": "Group Assignments",
    "text": "Group Assignments\nEach course day, you’ll be assigned a few problems as “group homework”. On the day homework is due (typically Fridays), you’ll submit solutions to these problems as a group via gradescope. Group assignments will be graded on correctness and completion.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#homework-quizzes",
    "href": "course-syllabus.html#homework-quizzes",
    "title": "Stat340: Bayesian Statistics",
    "section": "Homework quizzes",
    "text": "Homework quizzes\nOn days that homework is due, we will begin class with a short “homework quiz”. I will ask you 1-2 questions that are based entirely on the homework you just submitted. These will be graded on correctness, and your lowest score will be dropped.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#exams",
    "href": "course-syllabus.html#exams",
    "title": "Stat340: Bayesian Statistics",
    "section": "Exams",
    "text": "Exams\nThere will be two in-class exams, tentatively scheduled for Fridays of Week 4 and 8.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#projects",
    "href": "course-syllabus.html#projects",
    "title": "Stat340: Bayesian Statistics",
    "section": "Projects",
    "text": "Projects\nThere will be two projects (one midterm project and one final project). You will work in groups of 2-3 for each project.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#regrade-requests",
    "href": "course-syllabus.html#regrade-requests",
    "title": "Stat340: Bayesian Statistics",
    "section": "Regrade requests",
    "text": "Regrade requests\nGrading is often a tedious task, and the grading team will sometimes make mistakes. I am always happy to fix these mistakes, and gradescope makes it easy to do so. Regrade requests must be submitted on gradescope within two weekdays after an assignment or exam has been returned to you. Regrade requests are for administrative errors or obvious grading mistakes. I will not consider regrade requests for anything that applied to the entire class (e.g. “I think this mistake should only be worth 1 point instead of 2” or “I didn’t realize we had to do X”). If you submit two or more inappropriate regrade requests, I will not consider additional regrade requests from you for the remainder of the term. If you’re unsure whether you should file a regrade request or not, just ask! You are always welcome to discuss any grading questions with me in office hours.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#exam-revisions",
    "href": "course-syllabus.html#exam-revisions",
    "title": "Stat340: Bayesian Statistics",
    "section": "Exam revisions",
    "text": "Exam revisions\nI offer the opportunity to submit exam revisions for two reasons: (1) to correct any major misunderstandings and have a chance to revisit material that you struggled with, (2) to mitigate the impact that a bad exam day can have on your final course grade. You can submit corrections to your exam to earn back 50% of your missed points. Corrected exam grades are capped at 80%. Exam revisions are due 1 week after graded exams are returned to you, or the last day of class at 11:59pm, whichever is earlier.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#missing-homework-quizzes-and-exams",
    "href": "course-syllabus.html#missing-homework-quizzes-and-exams",
    "title": "Stat340: Bayesian Statistics",
    "section": "Missing homework quizzes and exams",
    "text": "Missing homework quizzes and exams\nIf you miss a class when there is an exam or homework quiz and don’t make arrangements with me at least a week in advance, then there are no makeups offered. If must miss an exam due to an illness or other last minute emergency, please let me know in advance to arrange an alternative. There are no makeup homework quizzes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbook",
    "href": "course-syllabus.html#textbook",
    "title": "Stat340: Bayesian Statistics",
    "section": "Textbook",
    "text": "Textbook\nOur textbook is freely available online:\n\nBayes Rules! An Introduction to Applied Bayesian Mdoeling https://www.bayesrulesbook.com/\n\nIf you prefer a hard copy, they are also available for purchase through the bookstore and/or publisher.\nI may also periodically assign or recommend readings from other sources. These will either be freely available online or I will provide a PDF. If you require readings in another format, please let me know during the first week of class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#software",
    "href": "course-syllabus.html#software",
    "title": "Stat340: Bayesian Statistics",
    "section": "Software",
    "text": "Software\nThe use of the R programming language, with the RStudio interface is an essential component of this course. Bayesian Statistics is more computationally intense than traditional frequentist methods, and so you will need to install local versions of the software on your own computer, or plan to complete assignments on a lab computer.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#communication",
    "href": "course-syllabus.html#communication",
    "title": "Stat340: Bayesian Statistics",
    "section": "Communication",
    "text": "Communication\nAssignments and slides will be shared publicly on our course website. Grades will be posted on gradescope and/or Moodle. Please use our slack workspace for any homework or course content questions; email me privately with any personal matters (grade discussions, illness, emergency, etc.). Any time-sensitive announcements will be sent via email. It is your responsibility to make sure that your notification settings allow time-sensitive announcements to reach you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#professor-availability",
    "href": "course-syllabus.html#professor-availability",
    "title": "Stat340: Bayesian Statistics",
    "section": "Professor availability",
    "text": "Professor availability\nPlease reach out to me if you have any questions! You can reach me on slack, email, drop-in hours, or make an appointment. Slack is typically the best venue for homework/content questions outside of drop-in hours. I’ll respond to slack messages at least 3 times each week day, and try to respond to emails within 48 hours You will receive responses faster on slack than email. I am online irregularly over the weekend to devote time to family and rest, and I hope that you also find time over the weekend to recharge and reset!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accomodations",
    "href": "course-syllabus.html#accomodations",
    "title": "Stat340: Bayesian Statistics",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#title-ix",
    "href": "course-syllabus.html#title-ix",
    "title": "Stat340: Bayesian Statistics",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#take-care-of-yourself",
    "href": "course-syllabus.html#take-care-of-yourself",
    "title": "Stat340: Bayesian Statistics",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this term by wearing a mask when you’re sick, eating a vegetable every now and then, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your physical and mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. For more information, see Student Health and Counseling (SHAC), the Office of Health Promotion, or the Office of the Chaplain. If you are experiencing physical or mental health symptoms as a result of coursework, please speak with me so we can address the problem together.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "groupwork/gw01.html",
    "href": "groupwork/gw01.html",
    "title": "Group Work 01",
    "section": "",
    "text": "Syllabus Q\nWhat is the weirdest/most unique policy or course component that you saw in the syllabus?\n\n\nBR Exercise 2.3\n\n\nBR Exercise 2.5\n\n\nBR Exercise 2.13\n\n\n\n\n\n\nImportant\n\n\n\nThis problem moved to HW2!",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 01"
    ]
  },
  {
    "objectID": "groupwork/gw02.html",
    "href": "groupwork/gw02.html",
    "title": "Group Work 02",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 2.13\nBR Exercise 2.18\nBR Exercise 2.17\nMLE\nIf \\(Y \\sim \\text{Binom}(n, \\pi)\\), show that \\(\\hat{\\pi}_{MLE} = \\frac{Y}{n}\\)\nBR Exercise 3.1\nFor each part, use plot_beta and/or summarize_beta to justify your answer\nBR Exercise 3.12\nChoice of prior\nI am interviewing Carleton students about whether or not they have used (knowingly) used ChatGPT on coursework in a non-approved way. I think the proportion has a 90% chance of being less than .25.\n\nChoose an informative prior that you think is reasonable for this belief\n\nI then ask 20 students this question and 15 respond “yes”. Find the posterior using the prior from above, then using the 3 non/weakly informative priors below:\n\nUnif(0,1) prior\nBeta(2,2) prior\n“Reference” prior Beta(.5, .5)\n\nCompare the posteriors for each of the priors above. Do results change if we instead observe 150/200 students responding “yes”?\nBR 4.13\n\n\n\n\n\n\nNote\n\n\n\nFor part (a), you do not need to include your sketch in your submission!",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 02"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html",
    "href": "computing/rstudio-stat340.html",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "",
    "text": "In Stat340, I will distribute homework assignment templates in.qmd format.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#uploading-files-to-maize",
    "href": "computing/rstudio-stat340.html#uploading-files-to-maize",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Uploading files to maize",
    "text": "Uploading files to maize\n\nClick the “upload” button in the files pane\nSelect the file from your local computer (likely in your Downloads folder if you downloaded from moodle)\nChoose the “Target Directory” – this is the folder on your maize account where the file will be saved. I recommend creating a separate folder for this class.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#accessing-maize-from-off-campus",
    "href": "computing/rstudio-stat340.html#accessing-maize-from-off-campus",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Accessing maize from off campus",
    "text": "Accessing maize from off campus\nIf you are using the maize server, you should be able to access it on campus with only your Carleton ID and password. If you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "href": "computing/rstudio-stat340.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Installing LaTeX (not needed if you are using the maize server)",
    "text": "Installing LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a standalone LaTeX package that will work with programs other than RStudio, you could install the basic installations of either:\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\n\nMacTeX for Mac (3.2GB!)\n\nMiKTeX for Windows (190MB)",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "activities/09-activity.html",
    "href": "activities/09-activity.html",
    "title": "Class Examples - Day 9",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\nlibrary(bayesrules)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\nAttaching package: 'rstan'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n- Online documentation and vignettes at mc-stan.org/bayesplot\n- bayesplot theme set to bayesplot::theme_default()\n   * Does _not_ affect other ggplot2 plots\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nExample 1\n\nfn_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 1000);\n    sigma ~ exponential(.0001);\n  }\n\"\n\nfn_sim &lt;- stan(model_code = fn_model, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\n\nui_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real alpha;\n    real beta;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(alpha + beta, sigma);\n    alpha ~ normal(0, 1000);\n    beta ~ normal(0, 1000);\n    sigma ~ exponential(1);\n  }\n\"\n\nui_sim &lt;- stan(model_code = ui_model, \n               data = list(N=100, y = rnorm(100)), \n               chains = 4, iter = 1000*2, seed = 84735)"
  },
  {
    "objectID": "homework/hw02.html",
    "href": "homework/hw02.html",
    "title": "Individual HW02",
    "section": "",
    "text": "BR Exercise 2.16\n\n\n\n\n\n\nNote\n\n\n\nIf the link from the textbook doesn’t work, try this link from the internet archive\n\n\n\n\nBR Exercise 2.21\n\n\nBR Exercise 3.8\n\n\nsummarize_beta_binomial\nWrite your own R function that does the same thing as summarize_beta_binomial. It’s OK if your output is formatted differently and you do not need to write formal checks.\n\n\n\n\n\n\nNote\n\n\n\nIf you need a refresher on writing functions in R, I recommend:\n\nR4DS Ch25 for an overview of functions in R with practice problems\nMy Stat220 slides for a quick refresher\n\n\n\n\n\nBR Exercise 3.9\n\n\nBR Exercise 3.10\n\n\nBR Exercise 4.10\n\n\nBR Exercise 4.14\n\n\nBR Exercise 4.18",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW02"
    ]
  },
  {
    "objectID": "homework/gw02-sols.html",
    "href": "homework/gw02-sols.html",
    "title": "gw02-sols",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 2.13\nSee handwritten\n\npi &lt;- c(.4, .5, .6, .7)\nprior &lt;- c(.1, .2, .44, .26)\nlik &lt;- dbinom(47, size = 80, prob = pi)\npost_unnorm &lt;- prior*lik\npost_norm &lt;- prior*lik/sum(post_unnorm)\n\ntibble(pi, prior, lik, post_unnorm, post_norm)\n\n# A tibble: 4 × 5\n     pi prior      lik post_unnorm post_norm\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1   0.4  0.1  0.000301   0.0000301  0.000649\n2   0.5  0.2  0.0264     0.00527    0.114   \n3   0.6  0.44 0.0880     0.0387     0.834   \n4   0.7  0.26 0.00929    0.00242    0.0520  \n\n\nBR Exercise 2.18\nThe smallest Y where \\(P(\\pi = .6 |Y) &gt; .4\\) is 6\n\ntibble(\n  y = 0:10, # possible values of Y\n  p_y = dbinom(y, size = 10, prob = .6)) |&gt; # f(Y|pi = .6)\n  rowwise() |&gt;\n  mutate(\n    norm_constant = (.25*dbinom(y, 10, .2) + .5*dbinom(y, 10, .4) + .25*dbinom(y, 10, .6)), # Normalizing constant for each possible value of Y\n    posterior = (.25*p_y)/norm_constant # posterior: f(pi = .6 | Y)\n  )\n\n# A tibble: 11 × 4\n# Rowwise: \n       y      p_y norm_constant posterior\n   &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0 0.000105       0.0299   0.000877\n 2     1 0.00157        0.0877   0.00449 \n 3     2 0.0106         0.139    0.0191  \n 4     3 0.0425         0.168    0.0630  \n 5     4 0.111          0.175    0.159   \n 6     5 0.201          0.157    0.319   \n 7     6 0.251          0.120    0.523   \n 8     7 0.215          0.0752   0.715   \n 9     8 0.121          0.0356   0.850   \n10     9 0.0403         0.0109   0.928   \n11    10 0.00605        0.00156  0.966   \n\n\nBR Exercise 2.17\n\n# Define possible trees\ntrees &lt;- data.frame(type = c(\"mold\", \"not mold\"))\n\n# Define the prior model\nprior &lt;- c(0.18, .82)\n\n# Simulate 10000 trees \nset.seed(84735)\ntrees_sim &lt;- sample_n(trees, size = 10000, \n                        weight = prior, replace = TRUE)\n\ntrees_sim &lt;- trees_sim %&gt;% \n  mutate(data_model = case_when(type == \"mold\" ~ 0.8,\n                                type == \"not mold\" ~ 0.1))\n\n# Define  tree species\ndata &lt;- c(\"maple\", \"not maple\")\n\n# Simulate species\n\ntrees_sim &lt;- trees_sim %&gt;%\n  group_by(1:n()) %&gt;% \n  mutate(species = sample(data, size = 1, \n                        prob = c(data_model, 1-data_model)))\n\ntrees_sim %&gt;% \n  filter(species == \"maple\") %&gt;% \n  tabyl(type) %&gt;% \n  adorn_totals(\"row\")\n\n     type    n   percent\n     mold 1482 0.6327925\n not mold  860 0.3672075\n    Total 2342 1.0000000\n\n\nMLE\nIf \\(Y \\sim \\text{Binom}(n, \\pi)\\), show that \\(\\hat{\\pi}_{MLE} = \\frac{Y}{n}\\)\n(see handwritten)\nBR Exercise 3.1\nFor each part, use plot_beta and/or summarize_beta to justify your answer\nBR Exercise 3.12\nChoice of prior\nI am interviewing Carleton students about whether or not they have used (knowingly) used ChatGPT on coursework in a non-approved way. I think the proportion has a 90% chance of being less than .25.\n\nChoose an informative prior that you think is reasonable for this belief\n\nI then ask 20 students this question and 15 respond “yes”. Find the posterior using the prior from above, then using the 3 non/weakly informative priors below:\n\nUnif(0,1) prior\nBeta(2,2) prior\n“Reference” prior Beta(.5, .5)\n\nCompare the posteriors for each of the priors above. Do results change if we instead observe 150/200 students responding “yes”?\nBR 4.13\n(see handwritten)",
    "crumbs": [
      "Assignments",
      "Homework",
      "gw02-sols"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "The best place to come for help is Amanda’s office hours! I can help with anything related to the course. You can also check my calendar for one-on-one appointment slots.\n\n\n\nStudents needing help with their Statistics coursework and R/Rstudio questions can get help from Stats Lab Assistants in the Stats Lab in CMC 304. Stats Lab Assistants are primiarily for Stat 120 (Intro Stats), but they may also be able to assist with general R/Rstudio questions."
  },
  {
    "objectID": "course-faq.html#where-can-i-get-help",
    "href": "course-faq.html#where-can-i-get-help",
    "title": "FAQ",
    "section": "",
    "text": "The best place to come for help is Amanda’s office hours! I can help with anything related to the course. You can also check my calendar for one-on-one appointment slots.\n\n\n\nStudents needing help with their Statistics coursework and R/Rstudio questions can get help from Stats Lab Assistants in the Stats Lab in CMC 304. Stats Lab Assistants are primiarily for Stat 120 (Intro Stats), but they may also be able to assist with general R/Rstudio questions."
  },
  {
    "objectID": "slides/slides06.html#beta-binomial-review",
    "href": "slides/slides06.html#beta-binomial-review",
    "title": "Gamma-Poisson model",
    "section": "Beta-Binomial review",
    "text": "Beta-Binomial review\n\nPrior: \\(f(\\pi) \\sim \\text{Beta}(\\alpha, \\beta)\\)\nLikelihood: \\(L(\\pi | Y) \\sim \\text{Binomial}(n, \\pi)\\)\nPosterior: \\(f(\\pi |Y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\)\n\n \n\n\n\n\n\n\nConjugate Prior\n\n\nThe Beta distribution is a conjugate prior for the Binomial likelihood, since the posterior is also Beta"
  },
  {
    "objectID": "slides/slides06.html#non-conjugate-prior-for-binomial-likelihood",
    "href": "slides/slides06.html#non-conjugate-prior-for-binomial-likelihood",
    "title": "Gamma-Poisson model",
    "section": "Non-conjugate prior for Binomial likelihood",
    "text": "Non-conjugate prior for Binomial likelihood\n\\[f(\\pi) = e - e^\\pi\\]"
  },
  {
    "objectID": "slides/slides06.html#finding-the-posterior",
    "href": "slides/slides06.html#finding-the-posterior",
    "title": "Gamma-Poisson model",
    "section": "Finding the posterior",
    "text": "Finding the posterior\nAssume \\(Y=10\\) and \\(n=50\\)"
  },
  {
    "objectID": "slides/slides06.html#can-we-use-beta-binomial-model",
    "href": "slides/slides06.html#can-we-use-beta-binomial-model",
    "title": "Gamma-Poisson model",
    "section": "Can we use Beta-Binomial Model?",
    "text": "Can we use Beta-Binomial Model?\nAt a certain hospital, an average of 6 babies are born each hour. Let \\(Y\\) be the number of babies born between 9 a.m. and 10 a.m. tomorrow."
  },
  {
    "objectID": "slides/slides06.html#poisson-data-model",
    "href": "slides/slides06.html#poisson-data-model",
    "title": "Gamma-Poisson model",
    "section": "Poisson Data model",
    "text": "Poisson Data model\nLet \\(Y\\) be the number of independent events that occur in a fixed amount of time or space, where \\(\\lambda &gt; 0\\) is the rate at which these events occur. Then the dependence of \\(Y\\) on parameter \\(\\lambda\\) can be modeled by the Poisson. In mathematical notation:\n\\[Y | \\lambda \\sim \\text{Pois}(\\lambda) \\]\nand\n\\[ f(y|\\lambda) =  \\frac{\\lambda^y e^{-\\lambda}}{y!}\\;\\; \\text{ for } y \\in \\{0,1,2,\\ldots\\}\\]\n\\[E(Y | \\lambda) = V(Y | \\lambda) = \\lambda\\]"
  },
  {
    "objectID": "slides/slides06.html#poisson-pmfs",
    "href": "slides/slides06.html#poisson-pmfs",
    "title": "Gamma-Poisson model",
    "section": "Poisson PMFs",
    "text": "Poisson PMFs"
  },
  {
    "objectID": "slides/slides06.html#joint-likelihood-function",
    "href": "slides/slides06.html#joint-likelihood-function",
    "title": "Gamma-Poisson model",
    "section": "Joint Likelihood Function",
    "text": "Joint Likelihood Function"
  },
  {
    "objectID": "slides/slides06.html#poisson-likelihood",
    "href": "slides/slides06.html#poisson-likelihood",
    "title": "Gamma-Poisson model",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\nIf we collect 4 hours of data and observe \\(y_i = 0, 2, 3, 2\\) babies born:"
  },
  {
    "objectID": "slides/slides06.html#section-1",
    "href": "slides/slides06.html#section-1",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "plot_poisson_likelihood(y = c(0, 2, 3, 2), lambda_upper_bound = 8)"
  },
  {
    "objectID": "slides/slides06.html#choosing-a-prior",
    "href": "slides/slides06.html#choosing-a-prior",
    "title": "Gamma-Poisson model",
    "section": "Choosing a prior",
    "text": "Choosing a prior\n\n\\(\\lambda\\) discrete or continuous?\nWhat is the support of \\(\\lambda\\)?\nBe strategic about creating a kernel"
  },
  {
    "objectID": "slides/slides06.html#gamma-model",
    "href": "slides/slides06.html#gamma-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma Model",
    "text": "Gamma Model\nLet \\(\\lambda\\) be a random variable which can take any value between 0 and \\(\\infty\\), ie. \\(\\lambda \\in [0,\\infty)\\). The Gamma model with shape parameter \\(s &gt; 0\\) and rate parameter \\(r &gt; 0\\) is:\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\]\n\\[f(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda} \\;\\; \\text{ for } \\lambda &gt; 0 \\]\nwhere constant \\(\\Gamma(s) = \\int_0^\\infty z^{s - 1} e^{-z}dz\\). When \\(s\\) is a positive integer, \\(s \\in \\{1,2,3,\\ldots\\}\\), this constant simplifies to \\(\\Gamma(s) = (s - 1)!\\)."
  },
  {
    "objectID": "slides/slides06.html#section-2",
    "href": "slides/slides06.html#section-2",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "Warning\n\n\nThe Gamma is sometimes parameterized by shape \\(s\\) and scale \\(\\theta = \\frac{1}{r}\\)\n\n\n\n\nIn econometrics, the (α, θ) parameterization is common for modeling waiting times … Bayesian statisticians prefer the (α,λ) parameterization, utilizing the gamma distribution as a conjugate prior for several inverse scale parameters, facilitating analytical tractability in posterior distribution computations.\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/slides06.html#section-3",
    "href": "slides/slides06.html#section-3",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "Exponential Model\n\\[\\lambda \\sim \\text{Exp}(r)\\]\nis a special case of the Gamma with shape parameter \\(s = 1\\), \\(\\text{Gamma}(1,r)\\)\n\\(\\chi^2\\) Model\n\\[\\lambda \\sim \\chi^2(k)\\]\nis a special case of the Gamma with shape \\(s = k/2\\) and rate \\(r=1/2\\)"
  },
  {
    "objectID": "slides/slides06.html#section-4",
    "href": "slides/slides06.html#section-4",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "\\(E(\\lambda) = \\frac{s}{r}, \\text{Mode}(\\lambda) = \\frac{s-1}{r} \\text{ for s } \\ge 1, \\text{Var}(\\lambda) = \\frac{s}{r^2}\\)"
  },
  {
    "objectID": "slides/slides06.html#posterior-model",
    "href": "slides/slides06.html#posterior-model",
    "title": "Gamma-Poisson model",
    "section": "Posterior Model",
    "text": "Posterior Model"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-bayesian-model",
    "href": "slides/slides06.html#gamma-poisson-bayesian-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Bayesian Model",
    "text": "Gamma-Poisson Bayesian Model\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\] \\[ Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\]\n\\[\\lambda | Y_1, Y_2, ..., Y_n \\sim \\text{Gamma}(s + \\sum Y_i, r + n)\\]"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-model",
    "href": "slides/slides06.html#gamma-poisson-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Model",
    "text": "Gamma-Poisson Model\n\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 7, n = 4)"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-model-1",
    "href": "slides/slides06.html#gamma-poisson-model-1",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Model",
    "text": "Gamma-Poisson Model\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 7, n = 4)\n\n      model shape rate     mean     mode       var        sd\n1     prior    10    2 5.000000 4.500000 2.5000000 1.5811388\n2 posterior    17    6 2.833333 2.666667 0.4722222 0.6871843\n\n\nInterpretation:"
  },
  {
    "objectID": "slides/slides06.html#example-tuning-a-gamma-prior",
    "href": "slides/slides06.html#example-tuning-a-gamma-prior",
    "title": "Gamma-Poisson model",
    "section": "Example: tuning a Gamma prior",
    "text": "Example: tuning a Gamma prior\nThe most likely value of \\(\\lambda\\) is 3.5 and the variance is 2"
  },
  {
    "objectID": "slides/slides06.html#example-gamma-poisson-model",
    "href": "slides/slides06.html#example-gamma-poisson-model",
    "title": "Gamma-Poisson model",
    "section": "Example: Gamma-Poisson model",
    "text": "Example: Gamma-Poisson model\nAssume a prior of \\(\\lambda \\sim \\text{Gamma}(24, 2)\\) and \\(Y | \\lambda \\sim \\text{Poisson}(\\lambda)\\). If you observe a sample of \\((12, 12, 12, 0)\\), what is the likelihood and posterior?"
  },
  {
    "objectID": "slides/slides06.html#section-5",
    "href": "slides/slides06.html#section-5",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "plot_gamma_poisson(shape = 24, rate = 2, sum_y = 36, n = 4)"
  },
  {
    "objectID": "slides/slides06.html#section-6",
    "href": "slides/slides06.html#section-6",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "summarize_gamma_poisson(shape = 24, rate = 2, sum_y = 36, n = 4)\n\n      model shape rate mean      mode      var       sd\n1     prior    24    2   12 11.500000 6.000000 2.449490\n2 posterior    60    6   10  9.833333 1.666667 1.290994\n\n\nInterpretation:"
  },
  {
    "objectID": "slides/slides05.html#key-ideas",
    "href": "slides/slides05.html#key-ideas",
    "title": "Balance and Sequentiality",
    "section": "Key ideas",
    "text": "Key ideas\n\nDifferent priors, different posteriors\nDifferent data, different posteriors\nSequential analyses\nData order is invariant\nDon’t be stubborn"
  },
  {
    "objectID": "slides/slides05.html#bechdel-test",
    "href": "slides/slides05.html#bechdel-test",
    "title": "Balance and Sequentiality",
    "section": "Bechdel test",
    "text": "Bechdel test\nAlison Bechdel’s 1985 comic Dykes to Watch Out For has a strip called The Rule where a person states that they only go to a movie if it satisfies the following three rules:\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThis test is used for assessing movies in terms of representation of women. Even though there are three criteria, a movie either fails or passes the Bechdel test."
  },
  {
    "objectID": "slides/slides05.html#priors",
    "href": "slides/slides05.html#priors",
    "title": "Balance and Sequentiality",
    "section": "Priors",
    "text": "Priors"
  },
  {
    "objectID": "slides/slides05.html#terminology",
    "href": "slides/slides05.html#terminology",
    "title": "Balance and Sequentiality",
    "section": "Terminology",
    "text": "Terminology\n\n\n\n\n\n\nInformative Prior\n\n\nAn informative prior reflects specific information about the unknown variable with high certainty (ie. low variability).\n\n\n\n\n\n\n\n\n\nDiffuse prior\n\n\nA vague or diffuse prior reflects little specific information about the unknown variable. A flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\n\n\n\n\n\n\n\n\n\nReference prior\n\n\nA formal type of diffuse prior\ncatalog of reference priors"
  },
  {
    "objectID": "slides/slides05.html#section",
    "href": "slides/slides05.html#section",
    "title": "Balance and Sequentiality",
    "section": "",
    "text": "“There is no objective, unique prior that represents ignorance. Instead, reference priors are chosen by public agreement, much like units of length and weight. In this interpretation, reference priors are akin to a default option in a computer package. We fall back to the default when there is insufficient information to otherwise define the prior.” Kass & Wasserman"
  },
  {
    "objectID": "slides/slides05.html#flat-prior",
    "href": "slides/slides05.html#flat-prior",
    "title": "Balance and Sequentiality",
    "section": "Flat prior",
    "text": "Flat prior\n\\[\\pi \\sim \\text{Unif}(0,1)\\]\n\n\nUnder re-parameterization (\\(\\theta = g(\\pi)\\)) \\(\\theta\\) might have a non-uniform distribution"
  },
  {
    "objectID": "slides/slides05.html#jeffreys-prior",
    "href": "slides/slides05.html#jeffreys-prior",
    "title": "Balance and Sequentiality",
    "section": "Jeffrey’s prior",
    "text": "Jeffrey’s prior\n\\(\\pi \\propto \\sqrt{I(\\pi)}\\)\nwhere \\(I(\\pi)\\) is the Fisher Information. For binomial data, Jeffrey’s prior gives \\(\\pi \\sim\\)"
  },
  {
    "objectID": "slides/slides05.html#bernardos-reference-prior",
    "href": "slides/slides05.html#bernardos-reference-prior",
    "title": "Balance and Sequentiality",
    "section": "Bernardo’s reference prior",
    "text": "Bernardo’s reference prior\n\nPrior should influence posterior as little as possible\nPosterior should be (on average) very different from the posterior\nWe can measure “difference” between distributions \\(P\\) and \\(Q\\) with the Kullback-Leibler divergence\n\n\\(KL(P,Q) = \\int_{-\\infty}^\\infty p(x) \\log \\frac{p(x)}{q(x)} dx\\)\n\nSo, choose reference prior by maximizing KL divergence between prior and posterior\nLucky for us, if we have univariate model, this is equivalent to Jeffrey’s prior"
  },
  {
    "objectID": "slides/slides05.html#priors-1",
    "href": "slides/slides05.html#priors-1",
    "title": "Balance and Sequentiality",
    "section": "Priors",
    "text": "Priors"
  },
  {
    "objectID": "slides/slides05.html#data",
    "href": "slides/slides05.html#data",
    "title": "Balance and Sequentiality",
    "section": "Data",
    "text": "Data\n\nlibrary(bayesrules) has bechdel data frame. Randomly select 20 movies from this dataset (seed = 84735)\nBased on observed data, update the posterior\nPlot the prior, likelihood, and the posterior\n\n\nset.seed(84735)\nbechdel_sample &lt;- sample_n(bechdel, 20)"
  },
  {
    "objectID": "slides/slides05.html#data-1",
    "href": "slides/slides05.html#data-1",
    "title": "Balance and Sequentiality",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\n\n\nyear\ntitle\nbinary\n\n\n\n\n2005\nKing Kong\nFAIL\n\n\n1983\nFlashdance\nPASS\n\n\n2013\nThe Purge\nFAIL\n\n\n2001\nAmerican Outlaws\nFAIL\n\n\n2010\nSex and the city 2\nPASS\n\n\n1997\nAir Bud\nFAIL\n\n\n2010\nRobin Hood\nFAIL\n\n\n2009\nCase 39\nPASS\n\n\n1998\nHope Floats\nPASS\n\n\n2007\nThe Golden Compass\nPASS\n\n\n2013\nParker\nPASS\n\n\n1971\nShaft\nFAIL\n\n\n2011\nFast Five\nPASS\n\n\n2013\nThe Croods\nFAIL\n\n\n2010\nEdge of Darkness\nFAIL\n\n\n1998\nMulan\nFAIL\n\n\n2010\nEasy A\nPASS\n\n\n2000\nMemento\nFAIL\n\n\n2002\nThe Tuxedo\nFAIL\n\n\n1982\nThe Best Little Whorehouse in Texas\nPASS\n\n\n\n\n\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45"
  },
  {
    "objectID": "slides/slides05.html#posteriors",
    "href": "slides/slides05.html#posteriors",
    "title": "Balance and Sequentiality",
    "section": "Posteriors",
    "text": "Posteriors"
  },
  {
    "objectID": "slides/slides05.html#different-data-different-posteriors",
    "href": "slides/slides05.html#different-data-different-posteriors",
    "title": "Balance and Sequentiality",
    "section": "Different data, different posteriors",
    "text": "Different data, different posteriors\nLet’s compare results for 3 analyses: 1 using 1991 movies, 1 using 2000 movies, and 1 using 2013 movies\n\nbechdel |&gt;\n  filter(year == 1991) |&gt;\n  tabyl(binary)\n\n binary n   percent\n   FAIL 7 0.5384615\n   PASS 6 0.4615385\n\nbechdel |&gt;\n  filter(year == 2000) |&gt;\n  tabyl(binary)\n\n binary  n   percent\n   FAIL 34 0.5396825\n   PASS 29 0.4603175\n\nbechdel |&gt;\n  filter(year == 2013) |&gt;\n  tabyl(binary)\n\n binary  n   percent\n   FAIL 53 0.5353535\n   PASS 46 0.4646465"
  },
  {
    "objectID": "slides/slides05.html#different-data-different-posteriors-1",
    "href": "slides/slides05.html#different-data-different-posteriors-1",
    "title": "Balance and Sequentiality",
    "section": "Different data, different posteriors",
    "text": "Different data, different posteriors"
  },
  {
    "objectID": "slides/slides05.html#prior-updatingsequential-data",
    "href": "slides/slides05.html#prior-updatingsequential-data",
    "title": "Balance and Sequentiality",
    "section": "Prior updating/sequential data",
    "text": "Prior updating/sequential data"
  },
  {
    "objectID": "slides/slides05.html#data-order-invariance",
    "href": "slides/slides05.html#data-order-invariance",
    "title": "Balance and Sequentiality",
    "section": "Data order invariance",
    "text": "Data order invariance\nUpdating the prior 20 times:\n      model alpha beta      mean      mode          var         sd\n1     prior   551  613 0.4733677 0.4733219 0.0002139835 0.01462817\n2 posterior   612  681 0.4733179 0.4732765 0.0001926492 0.01387981\n“Data dump”: treat years 1-20 as the data\n\nsummarize_beta_binomial(1, 1, 611, 1291)\n\n      model alpha beta      mean      mode          var         sd\n1     prior     1    1 0.5000000       NaN 0.0833333333 0.28867513\n2 posterior   612  681 0.4733179 0.4732765 0.0001926492 0.01387981\n\n\nFurther reading: How to think like an epidemiologist"
  },
  {
    "objectID": "slides/slides05.html#data-order-invariance-math",
    "href": "slides/slides05.html#data-order-invariance-math",
    "title": "Balance and Sequentiality",
    "section": "Data order invariance (math)",
    "text": "Data order invariance (math)"
  },
  {
    "objectID": "slides/slides05.html#dont-be-stubborn",
    "href": "slides/slides05.html#dont-be-stubborn",
    "title": "Balance and Sequentiality",
    "section": "Don’t be stubborn",
    "text": "Don’t be stubborn\n\n\n\n\n\n\nWarning\n\n\nThe posterior is defined on the same values as the prior. Do not assign \\(f(\\pi) = 0\\) on any reasonable value!"
  },
  {
    "objectID": "slides/slides01.html#plan-for-today",
    "href": "slides/slides01.html#plan-for-today",
    "title": "Welcome to Bayes!",
    "section": "Plan for today",
    "text": "Plan for today\n\nIntros\nIntro to Bayesian Thinking\nSyllabus"
  },
  {
    "objectID": "slides/slides01.html#about-me",
    "href": "slides/slides01.html#about-me",
    "title": "Welcome to Bayes!",
    "section": "About me",
    "text": "About me\n\n\n\nEllis (almost 1.5!)\nSecond year at Carleton\nTaught at Swarthmore for 5 years before moving to MN\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/slides01.html#my-crochet-journey-this-summer",
    "href": "slides/slides01.html#my-crochet-journey-this-summer",
    "title": "Welcome to Bayes!",
    "section": "My crochet journey this summer",
    "text": "My crochet journey this summer\n\n\n\n\n\nDecided I wanted to make a basket\nSkipped “practice these stitches” and just went for it\nDidn’t understand the pattern, watched a couple of youtube videos from random creators and went for it\nKept going even when I was very clearly doing something wrong"
  },
  {
    "objectID": "slides/slides01.html#my-crochet-journey-this-summer-1",
    "href": "slides/slides01.html#my-crochet-journey-this-summer-1",
    "title": "Welcome to Bayes!",
    "section": "My crochet journey this summer",
    "text": "My crochet journey this summer\n\n\n\n\n\nAsked an expert (my sister) for advice\nDid “practice problems” using easier material (yarn) and bigger hook\nFound a reliable internet source for videos and tutorials\nStarted being way more careful about checking my work and redoing things if I wasn’t sure"
  },
  {
    "objectID": "slides/slides01.html#intros",
    "href": "slides/slides01.html#intros",
    "title": "Welcome to Bayes!",
    "section": "Intros",
    "text": "Intros\n\nName\nSomething you did over the summer\nWhat makes you nervous about this class or term?\nSomething about yourself you are proud of\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides01.html#warm-up",
    "href": "slides/slides01.html#warm-up",
    "title": "Welcome to Bayes!",
    "section": "Warm Up",
    "text": "Warm Up\n\nWhat are the key takeaways from:\n\nStat250: Stat Inference\nStat230: Regression\n\nWhat do you already know about Bayesian statistics?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides01.html#statistical-models",
    "href": "slides/slides01.html#statistical-models",
    "title": "Welcome to Bayes!",
    "section": "Statistical models",
    "text": "Statistical models\nA statistical model consists of\n\na collection of random variables to describe observable data,\nthe possible joint distribution(s) of the random variables,\nand the parameters, \\(\\boldsymbol \\theta\\), that define those distributions\n\n\n\nMorris and DeGroot, 377"
  },
  {
    "objectID": "slides/slides01.html#r.a.-fisher",
    "href": "slides/slides01.html#r.a.-fisher",
    "title": "Welcome to Bayes!",
    "section": "R.A. Fisher",
    "text": "R.A. Fisher\n1890-1962\n\n\n\n\n\nVariance\nANOVA\nNull hypothesis\nMaximum likelihood estimation\np-value\nLots of contributions in genetics\nAlso a eugenecist"
  },
  {
    "objectID": "slides/slides01.html#neyman-pearson",
    "href": "slides/slides01.html#neyman-pearson",
    "title": "Welcome to Bayes!",
    "section": "Neyman & Pearson",
    "text": "Neyman & Pearson\n\n\n\n\n\nConfidence interval\nCorrelation\nRegression\nStandard deviation\nEffect size\n“Optimal” tests\n\\(\\alpha\\) and \\(\\beta\\)\nType I and II error"
  },
  {
    "objectID": "slides/slides01.html#frequentist-vs-bayesian",
    "href": "slides/slides01.html#frequentist-vs-bayesian",
    "title": "Welcome to Bayes!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian"
  },
  {
    "objectID": "slides/slides01.html#bayes",
    "href": "slides/slides01.html#bayes",
    "title": "Welcome to Bayes!",
    "section": "Bayes",
    "text": "Bayes\n\n\n\n\nAll this guy did was Bayes’ rule:\n\\(P(A|B) = \\frac{P(B|A) P(A)}{ P(B)}\\)"
  },
  {
    "objectID": "slides/slides01.html#frequentist-vs-bayesian-1",
    "href": "slides/slides01.html#frequentist-vs-bayesian-1",
    "title": "Welcome to Bayes!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian\n\n\nFrequentist\n\n“Classical” statistics\nProbability is a long-run frequency\nMake decisions based on a p-value\nType I and Type II errors\nConfidence intervals\n\n\nBayesian\n\nProbability is a subjective belief\nUpdate “prior” probabilities with data to obtain “posterior” probabilities\nMake decisions according to posterior probabilities\n“Credible” intervals"
  },
  {
    "objectID": "slides/slides01.html#q1",
    "href": "slides/slides01.html#q1",
    "title": "Welcome to Bayes!",
    "section": "Q1",
    "text": "Q1\nWhen flipping a fair coin, we say that “the probability of flipping Heads is 0.5.” How do you interpret this probability?\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a and b make sense."
  },
  {
    "objectID": "slides/slides01.html#q2",
    "href": "slides/slides01.html#q2",
    "title": "Welcome to Bayes!",
    "section": "Q2",
    "text": "Q2\nAn election is coming up and a pollster claims that candidate A has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/slides01.html#q3",
    "href": "slides/slides01.html#q3",
    "title": "Welcome to Bayes!",
    "section": "Q3",
    "text": "Q3\nConsider two claims.\n\nZuofu claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\nIn light of these experiments, what do you conclude?\n\nYou’re more confident in Kavya’s claim than Zuofu’s claim.\nThe evidence supporting Zuofu’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/slides01.html#q4",
    "href": "slides/slides01.html#q4",
    "title": "Welcome to Bayes!",
    "section": "Q4",
    "text": "Q4\nSuppose that during a recent doctor’s visit, you tested positive for a very rare disease. If you only get to ask the doctor one question, which would it be?\n\nWhat’s the chance that I actually have the disease?\nIf in fact I don’t have the disease, what’s the chance that I would’ve gotten this positive test result?"
  },
  {
    "objectID": "slides/slides01.html#results",
    "href": "slides/slides01.html#results",
    "title": "Welcome to Bayes!",
    "section": "Results",
    "text": "Results\nGive yourself:\n\n+1 for each A\n+3 for each B\n+2 for C on Q1\n-2 for C on Q2\n\nResults:\n\nScore &lt;5: your thinking is pretty frequentist\nScore &gt;9: your thinking is pretty Bayesian\n6-8: your leaning changes depending on the scenario"
  },
  {
    "objectID": "slides/slides01.html#tentative-schedule",
    "href": "slides/slides01.html#tentative-schedule",
    "title": "Welcome to Bayes!",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\nTopic\nChapters\nApprox. Duration\n\n\n\n\nIntro\n1-2\n1 week\n\n\nConjugate Analysis\n3-5\n2 weeks\n\n\nMCMC\n7\n1 weeks\n\n\nBayesian Regression\n9-14\n4 weeks\n\n\nHierarchical Models\n15\n2 weeks"
  },
  {
    "objectID": "slides/slides01.html#course-description",
    "href": "slides/slides01.html#course-description",
    "title": "Welcome to Bayes!",
    "section": "Course description",
    "text": "Course description\nThe Bayesian approach to statistics provides a powerful framework for incorporating prior knowledge into statistical analyses, updating this knowledge with data, and quantifying uncertainty in results. This course serves as a comprehensive introduction to Bayesian statistical inference and modeling, an alternative to the frequentist approach to statistics covered in previous classes. Topics include: Bayes’ Theorem; prior and posterior distributions; Bayesian regression; hierarchical models; and model adequacy and posterior predictive checks. Computational techniques will also be covered, including Markov Chain Monte Carlo methods, and modern Bayesian modeling packages in R."
  },
  {
    "objectID": "slides/slides01.html#course-objectives",
    "href": "slides/slides01.html#course-objectives",
    "title": "Welcome to Bayes!",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter completing this course, you should be be able to demonstrate your competency with:\n\nExplaining the differences between frequentist and Bayesian approaches to statistics and data analysis\nSelecting appropriate prior distributions and likelihood models for Bayesian analysis\nUsing modern software to fit Bayesian models and assess their performance\nInterpreting results from Bayesian analyses"
  },
  {
    "objectID": "slides/slides01.html#textbook",
    "href": "slides/slides01.html#textbook",
    "title": "Welcome to Bayes!",
    "section": "Textbook:",
    "text": "Textbook:\nBayes Rules! An Introduction to Applied Bayesian Modeling by Johnson, Ott, and Dogucu\nThe book is freely available at &lt;bayesrulesbook.com&gt;"
  },
  {
    "objectID": "slides/slides01.html#computing",
    "href": "slides/slides01.html#computing",
    "title": "Welcome to Bayes!",
    "section": "Computing:",
    "text": "Computing:\nWe’ll be using R and RStudio throughout the course. If you’ve downloaded R to your own computer from a different class, great! If not, there are instructions available on the course website"
  },
  {
    "objectID": "slides/slides01.html#what-will-you-do-in-this-course",
    "href": "slides/slides01.html#what-will-you-do-in-this-course",
    "title": "Welcome to Bayes!",
    "section": "What will you do in this course?",
    "text": "What will you do in this course?\nEach of the following components are important for your learning and therefore part of your final grade calculation:\n\n30% Homework due once per week\n\n10% Individual (graded on completion)\n10% Group (correctness and completion)\n10% Homework quizzes (correctness)\n\n10% Midterm project + presentation\n20% Final project + presentation\n20% Exam I\n20% Exam II"
  },
  {
    "objectID": "slides/slides01.html#what-will-a-typical-dayweek-look-like",
    "href": "slides/slides01.html#what-will-a-typical-dayweek-look-like",
    "title": "Welcome to Bayes!",
    "section": "What will a typical day/week look like?",
    "text": "What will a typical day/week look like?\n\n\nBefore class:\n\nRead a chapter\nCome with questions\nBe prepared to try what was covered\n\n\nIn class:\n\nLecture/demo\n\nSometimes review of chapter\nSometimes new\n\nHands-on work or coding in R\nTime for group assignments\n\n\nAfter class:\n\nFinish group assignments\nWork on individual assignments"
  },
  {
    "objectID": "slides/slides01.html#office-hours-tentative",
    "href": "slides/slides01.html#office-hours-tentative",
    "title": "Welcome to Bayes!",
    "section": "Office hours (tentative)",
    "text": "Office hours (tentative)\n\n\n\nDay\nTime\nType\nLocation\n\n\n\n\nMonday\n12:30-1:30\nDrop-in\nCMC 307\n\n\nTuesday\n2-3\nDrop-in\nCMC 307\n\n\nWednesday\n3-41\nDrop-in\nCMC 307\n\n\nFriday\n10-112\nDrop-in\nCMC 307\n\n\n\nI will also have at least 1 hour of individual appointments available on my calendar per week\nStat120 PriorityStat340 Priority"
  },
  {
    "objectID": "slides/slides01.html#communication",
    "href": "slides/slides01.html#communication",
    "title": "Welcome to Bayes!",
    "section": "Communication",
    "text": "Communication\n\n\nMoodle: assignments, slides, and grades\nSlack: homework questions, announcements, discussion\nEmail: personal matters, time-sensitive announcements\n\n\n\nSlack is the fastest way to reach me. I typically will respond to messages 3x per weekday. I try to respond to emails within 48 hours. I’m online sporadically on evenings and weekends to devote time to family and rest – I hope you also use this time to reset and recharge!"
  },
  {
    "objectID": "slides/slides01.html#advice-from-past-students",
    "href": "slides/slides01.html#advice-from-past-students",
    "title": "Welcome to Bayes!",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nSTART HOMEWORKS EARLY\nDo the reading! The textbook is really great and can help give you a motivation for everything that’s happening in class.\nStart the problem sets early and go to office hours!\nask questions! you will inevitably not understand everything in the course at first pass.\nGo to her office hours!!!!!\nRead the book, and ask questions!\nTry to start the homework problems on the day that they are assigned.\nThis course is very fast-paced and intensive, so it may not be the right fit for everyone. Students who are not prepared for a compact schedule, or who are looking for an easier workload, may find it challenging. A solid foundation in statistics and a strong level of commitment are essential to keep up with the material and succeed in the class.\nAsk questions in class, Amanda has no problem reviewing a previously discussed concept.\nWork with other people in the class, collaboration is key\nStart the homework early! Give yourself time to get things done, to understand, and to pause. Don’t feel afraid to ask questions; she’s so accessible.\nSpend some time reviewing handouts before doing homework."
  },
  {
    "objectID": "slides/slides01.html#the-genius-myth",
    "href": "slides/slides01.html#the-genius-myth",
    "title": "Welcome to Bayes!",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/slides01.html#academic-integrity",
    "href": "slides/slides01.html#academic-integrity",
    "title": "Welcome to Bayes!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to follow Carleton’s policies regarding academic integrity. I encourage you to discuss the homework problems with others and use the resources available to you to try to figure out tough problems. You should code and write up your solutions on your own. Exams must be done by yourself without communicating with others; all work must be your own. The use of textbook solution manuals (physical or online), course materials from other students, or materials from previous versions of this course are not allowed. Copying, paraphrasing, summarizing, or submitting work generated by anyone but yourself without proper attribution is considered academic dishonesty (this includes output from LLMs).\nPlease ask if you are unsure of whether or not your actions are complying with the assignment/exam/project instructions. Always default to acknowledging any help received. Cases of suspected academic dishonesty are handled by the Provost’s Office and I am obligated to report any suspected violations of this policy."
  },
  {
    "objectID": "slides/slides01.html#more-on-ai",
    "href": "slides/slides01.html#more-on-ai",
    "title": "Welcome to Bayes!",
    "section": "More on “AI”",
    "text": "More on “AI”\nLarge-language models (e.g. ChatGPT, Gemini, etc.) should only be used for help interpreting R’s error messages or suggestions for your own code once you have already attempted the problem. You should not copy and paste course material into or out of an AI text generator.\nI also have a few rules in place to protect my intellectual property. You may not record my lectures using tools such as Otter.ai or upload any video or audio recordings to generate transcripts or study notes. You may not upload my course materials (slides, assignment prompts, note sets, etc.) into AI tools or homework help sites (such as chegg).\n“AI” tools are new for all of us and it’s OK to have questions about what is and isn’t appropriate!"
  },
  {
    "objectID": "slides/slides01.html#diversity-inclusion",
    "href": "slides/slides01.html#diversity-inclusion",
    "title": "Welcome to Bayes!",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/slides01.html#accomodations",
    "href": "slides/slides01.html#accomodations",
    "title": "Welcome to Bayes!",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR."
  },
  {
    "objectID": "slides/slides01.html#title-ix",
    "href": "slides/slides01.html#title-ix",
    "title": "Welcome to Bayes!",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/slides01.html#take-care-of-yourself",
    "href": "slides/slides01.html#take-care-of-yourself",
    "title": "Welcome to Bayes!",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/slides02.html#warm-up-i-bayesian-personality-quiz",
    "href": "slides/slides02.html#warm-up-i-bayesian-personality-quiz",
    "title": "Bayes Rule",
    "section": "Warm up I: Bayesian Personality Quiz",
    "text": "Warm up I: Bayesian Personality Quiz\nQ2: An election is coming up and a pollster claims that candidate A has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/slides02.html#warm-up-i-bayesian-personality-quiz-1",
    "href": "slides/slides02.html#warm-up-i-bayesian-personality-quiz-1",
    "title": "Bayes Rule",
    "section": "Warm up I: Bayesian Personality Quiz",
    "text": "Warm up I: Bayesian Personality Quiz\nQ3: Consider two claims.\n\nZuofu claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\nIn light of these experiments, what do you conclude?\n\nYou’re more confident in Kavya’s claim than Zuofu’s claim.\nThe evidence supporting Zuofu’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/slides02.html#warm-up-ii",
    "href": "slides/slides02.html#warm-up-ii",
    "title": "Bayes Rule",
    "section": "Warm Up II",
    "text": "Warm Up II\nThe following are some survey results from a recent Pew Research report to the question “About how often to use the internet”?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\nCalculate the following probabilities and write them using event notation. Let \\(A\\): online almost constantly and \\(B\\): lives in US\n\nP(Online Almost Constantly)\nP(Online Almost Constantly and US)\nP(US given online almost constantly)"
  },
  {
    "objectID": "slides/slides02.html#marginal-probability",
    "href": "slides/slides02.html#marginal-probability",
    "title": "Bayes Rule",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nMarginal Probability"
  },
  {
    "objectID": "slides/slides02.html#joint-probability",
    "href": "slides/slides02.html#joint-probability",
    "title": "Bayes Rule",
    "section": "Joint Probability",
    "text": "Joint Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nJoint Probability"
  },
  {
    "objectID": "slides/slides02.html#conditional-probability",
    "href": "slides/slides02.html#conditional-probability",
    "title": "Bayes Rule",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nConditional Probability"
  },
  {
    "objectID": "slides/slides02.html#more-probability-notes",
    "href": "slides/slides02.html#more-probability-notes",
    "title": "Bayes Rule",
    "section": "More probability notes:",
    "text": "More probability notes:\n\n\n\n\n\n\nConditional Probability Facts\n\n\n\\(P(A|B) \\ne P(B|A)\\)\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\nComplement rule\n\n\n\\(P(A^c) = 1 - P(A)\\)"
  },
  {
    "objectID": "slides/slides02.html#section",
    "href": "slides/slides02.html#section",
    "title": "Bayes Rule",
    "section": "",
    "text": "I’m trying to determine whether a student response to a homework question is AI generated. I know that AI use among college students in general is high, but my prior is that Carleton students are pretty honest. I would guess only around 2% of submitted answers are AI generated."
  },
  {
    "objectID": "slides/slides02.html#section-1",
    "href": "slides/slides02.html#section-1",
    "title": "Bayes Rule",
    "section": "",
    "text": "LLMs are obsessed with em dashes:\n\nImagine you have an idea or belief about something–like whether it’s going to rain today. Then, you get some new information–like seeing dark clouds in the sky. Bayes’ Rule helps you recalculate how confident you should be in your belief after seeing that new information. Let me know if you want a real-world example (like medical tests or crime investigations)—those make it even clearer.\n\n\n–ChatGPT. Prompt: “Can you summarize Bayes Rule without any math?”"
  },
  {
    "objectID": "slides/slides02.html#section-2",
    "href": "slides/slides02.html#section-2",
    "title": "Bayes Rule",
    "section": "",
    "text": "This suggests a possible rule for determining if an answer is AI generated. I gather some data:\n\nSample 1: 100 AI-generated responses to a homework question\nSample 2: 100 student responses to the same question from pre-2021\n\nI notice that 78% of AI generated responses use an em-dash, but only 5% of student responses used an em-dash.\nIn probability notation:"
  },
  {
    "objectID": "slides/slides02.html#bayesian-knowledge-building-diagram",
    "href": "slides/slides02.html#bayesian-knowledge-building-diagram",
    "title": "Bayes Rule",
    "section": "Bayesian knowledge-building diagram",
    "text": "Bayesian knowledge-building diagram"
  },
  {
    "objectID": "slides/slides02.html#section-3",
    "href": "slides/slides02.html#section-3",
    "title": "Bayes Rule",
    "section": "",
    "text": "Which of the following best describes your posterior “gut check” of whether the answer was AI generated?\n\nThe chance this answer is AI generated drops from 2% to 1%. It’s rare that a Carleton student would submit AI-generated answers.\nThe chance this answer is AI generated jumps from 2% to about 25%. Though em dashes are more common in AI answers, let’s not forget that AI submitted answers are relatively rare for Carleton students\nThe chance this answer is AI generated jumps from 2% to roughly 75%. Given so few student answers use em dashes, this answer is almost certainly AI generated."
  },
  {
    "objectID": "slides/slides02.html#prior-model",
    "href": "slides/slides02.html#prior-model",
    "title": "Bayes Rule",
    "section": "Prior Model",
    "text": "Prior Model"
  },
  {
    "objectID": "slides/slides02.html#likelihood",
    "href": "slides/slides02.html#likelihood",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood\nLet A:\nLet B:\nLooking at the conditional probabilities:\n\\(P(A|B) =\\)\n\\(P(A|B^c) =\\)"
  },
  {
    "objectID": "slides/slides02.html#likelihood-function-notation",
    "href": "slides/slides02.html#likelihood-function-notation",
    "title": "Bayes Rule",
    "section": "Likelihood function notation",
    "text": "Likelihood function notation\n\\[L(\\cdot | A) = \\begin{cases} L(B|A) = P(A|B) \\\\ L(B^c|A) = P(A | B^c) \\end{cases}\\]\nWhen \\(B\\) is known, the conditional probability function \\(P(\\cdot | B)\\) allows us to compare the probabilities of an unknown event, \\(A\\) or \\(A^c\\), occurring with \\(B\\)\nWhen \\(A\\) is known, the likelihood function \\(L(\\cdot | A) = P(A | \\cdot)\\) allows us to evaluate the relative compatibility with data \\(A\\) with events \\(B\\) or \\(B^c\\)"
  },
  {
    "objectID": "slides/slides02.html#posterior-model",
    "href": "slides/slides02.html#posterior-model",
    "title": "Bayes Rule",
    "section": "Posterior Model",
    "text": "Posterior Model"
  },
  {
    "objectID": "slides/slides02.html#posterior-results",
    "href": "slides/slides02.html#posterior-results",
    "title": "Bayes Rule",
    "section": "Posterior Results",
    "text": "Posterior Results\n\\[P(B|A) = \\frac{P(B) L(B|A)}{P(A|B)P(B) + P(A|B^c)P(B^c)}\\] \\[P(B|A) = \\frac{.02 \\times .78}{.02\\times .78 + .98 \\times .05}\\] \\[P(B|A) = \\]"
  },
  {
    "objectID": "slides/slides02.html#the-posterior-distribution",
    "href": "slides/slides02.html#the-posterior-distribution",
    "title": "Bayes Rule",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\n\nPrior\n\n\n\n\n\nLikelihood\n\n\n\n\n\nPosterior"
  },
  {
    "objectID": "slides/slides02.html#notation",
    "href": "slides/slides02.html#notation",
    "title": "Bayes Rule",
    "section": "Notation",
    "text": "Notation\nIdea: what if the values we’re interested in can’t be simplified into binary events\n\nGreek letters (\\(\\pi, \\beta, \\mu\\) etc) denote our primary variables of interest (sometimes called parameters)\nCapital letters near the end of the alphabet (\\(X, Y, Z\\)) denote random variables related to our data\nWe denote an observed outcome of \\(Y\\) using lower case \\(y\\)"
  },
  {
    "objectID": "slides/slides02.html#example-phd-admissions",
    "href": "slides/slides02.html#example-phd-admissions",
    "title": "Bayes Rule",
    "section": "Example: PhD admissions",
    "text": "Example: PhD admissions\nLet Y represent a random variable that represents the number of applicants admitted to a PhD program which has received applications from 5 prospective students. That is \\(\\Omega_Y = \\{0, 1, 2, 3, 4, 5\\}\\). We are interested in the parameter \\(\\pi\\) which represents the probability of acceptance to this program. For demonstrative purposes, we will only consider three possible values of \\(\\pi\\) as 0.2, 0.4, and 0.8."
  },
  {
    "objectID": "slides/slides02.html#prior-model-for-pi",
    "href": "slides/slides02.html#prior-model-for-pi",
    "title": "Bayes Rule",
    "section": "Prior model for \\(\\pi\\)",
    "text": "Prior model for \\(\\pi\\)\nYou consult with an expert who knows the specific PhD program well and the following is the prior distribution the expert suggests you use in your analysis.\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n.7\n.2\n.1\n\n\n\n\nExplain what this prior distribution means"
  },
  {
    "objectID": "slides/slides02.html#from-prior-to-posterior",
    "href": "slides/slides02.html#from-prior-to-posterior",
    "title": "Bayes Rule",
    "section": "From prior to posterior",
    "text": "From prior to posterior\n\nFor the two scenarios below, use your intuition to “guesstimate” the posterior (fill out each row based on ~vibes~ )\n\nThe program accepted five of five applicants?\nThe program accepted none of the five applicants?\n\n\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n\n\n\n\n\n(1): \\(f(\\pi | y = 5)\\)\n\n\n\n\n\n(2): \\(f(\\pi | y = 0)\\)"
  },
  {
    "objectID": "slides/slides02.html#intuition-vs-reality",
    "href": "slides/slides02.html#intuition-vs-reality",
    "title": "Bayes Rule",
    "section": "Intuition vs Reality",
    "text": "Intuition vs Reality\nYour intuition may not be Bayesian if:\n\n\n\n\nBayesian statistics is a balancing act! We will take both the prior and the data to get to the posterior. Don’t worry if your intuition was wrong. As we practice more, you will learn to think like a Bayesian."
  },
  {
    "objectID": "slides/slides02.html#the-binomial-model",
    "href": "slides/slides02.html#the-binomial-model",
    "title": "Bayes Rule",
    "section": "The Binomial Model",
    "text": "The Binomial Model\nLet random variable \\(Y\\) be the number of successes in n trials. Assume that the number of trials is fixed, the trials are independent and the probability of success in each trial is \\(\\pi\\). Then, the dependence of \\(Y\\) on \\(\\pi\\) can be modeled by the Binomial model with parameters \\(n\\) and \\(\\pi\\):"
  },
  {
    "objectID": "slides/slides02.html#section-4",
    "href": "slides/slides02.html#section-4",
    "title": "Bayes Rule",
    "section": "",
    "text": "conditional pmf:\n\\[f(y | \\pi) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}\\]\nif \\(\\pi = .2\\) and \\(y = 3\\):"
  },
  {
    "objectID": "slides/slides02.html#section-5",
    "href": "slides/slides02.html#section-5",
    "title": "Bayes Rule",
    "section": "",
    "text": "(or we can use R)\n\ndbinom(3, size = 5, prob = .2)\n\n[1] 0.0512"
  },
  {
    "objectID": "slides/slides02.html#section-6",
    "href": "slides/slides02.html#section-6",
    "title": "Bayes Rule",
    "section": "",
    "text": "Rather than doing this one by one, we can let R consider all different possible values of \\(y\\):\n\ndbinom(0:5, size = 5, prob = .2)\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\n\n\n\n\\(y\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(f(y | \\pi = .2)\\)"
  },
  {
    "objectID": "slides/slides02.html#all-possibilities-for-pi",
    "href": "slides/slides02.html#all-possibilities-for-pi",
    "title": "Bayes Rule",
    "section": "All possibilities for \\(\\pi\\)",
    "text": "All possibilities for \\(\\pi\\)"
  },
  {
    "objectID": "slides/slides02.html#data",
    "href": "slides/slides02.html#data",
    "title": "Bayes Rule",
    "section": "Data",
    "text": "Data\nThe admissions committee has announced they have accepted 3/5 applicants"
  },
  {
    "objectID": "slides/slides02.html#likelihood-1",
    "href": "slides/slides02.html#likelihood-1",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "slides/slides02.html#likelihood-2",
    "href": "slides/slides02.html#likelihood-2",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood\n\ndbinom(x = 3, size = 5, prob = 0.2)\n\n[1] 0.0512\n\n\n\ndbinom(x = 3, size = 5, prob = 0.4)\n\n[1] 0.2304\n\n\n\ndbinom(x = 3, size = 5, prob = 0.8)\n\n[1] 0.2048\n\n\n\n\n\nπ\n\n\n0.2\n\n\n0.4\n\n\n0.8\n\n\n\n\nL(π | y = 3)\n\n\n0.0512\n\n\n0.2304\n\n\n0.2048\n\n\n\nThe likelihood function \\(L(\\pi|y=3)\\) is the same as the conditional probability mass function \\(f(y|\\pi)\\) at the observed value \\(y = 3\\)."
  },
  {
    "objectID": "slides/slides02.html#pmf-vs-likelihood",
    "href": "slides/slides02.html#pmf-vs-likelihood",
    "title": "Bayes Rule",
    "section": "pmf vs likelihood",
    "text": "pmf vs likelihood\nWhen \\(\\pi\\) is known, the conditional pmf \\(f(\\cdot | \\pi)\\) allows us to compare the probabilities of different possible values of data \\(Y\\) (eg: \\(y_1\\) or \\(y_2\\)) occurring with \\(\\pi\\):\n\\[f(y_1|\\pi) \\; \\text{ vs } \\; f(y_2|\\pi) \\; .\\]\nWhen \\(Y=y\\) is known, the likelihood function \\(L(\\cdot | y) = f(y | \\cdot)\\) allows us to compare the relative likelihoods of different possible values of \\(\\pi\\) (eg: \\(\\pi_1\\) or \\(\\pi_2\\)) given that we observed data \\(y\\):\n\\[L(\\pi_1|y) \\; \\text{ vs } \\; L(\\pi_2|y) \\; .\\]"
  },
  {
    "objectID": "slides/slides02.html#bayes-rule-from-events-to-random-variables",
    "href": "slides/slides02.html#bayes-rule-from-events-to-random-variables",
    "title": "Bayes Rule",
    "section": "Bayes Rule: from events to random variables",
    "text": "Bayes Rule: from events to random variables\n\\(\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{marginal probability of data}}\\)"
  },
  {
    "objectID": "slides/slides02.html#normalizing-constant",
    "href": "slides/slides02.html#normalizing-constant",
    "title": "Bayes Rule",
    "section": "Normalizing constant",
    "text": "Normalizing constant\nTherefore \\(f(y=3)=\\)\n\ndbinom(3, size = 5, prob = .2) * .7 + \n  dbinom(3, size = 5, prob = .4) * .2 + \n  dbinom(3, size = 5, prob = .8) * .1 \n\n[1] 0.1024"
  },
  {
    "objectID": "slides/slides02.html#posterior",
    "href": "slides/slides02.html#posterior",
    "title": "Bayes Rule",
    "section": "Posterior",
    "text": "Posterior\n\n\n\nπ\n\n\n0.2\n\n\n0.4\n\n\n0.8\n\n\n\n\nf(π)\n\n\n0.7\n\n\n0.2\n\n\n0.1\n\n\n\n\nL(π | y = 3)\n\n\n0.0512\n\n\n0.2304\n\n\n0.2048\n\n\n\n\nf(π | y = 3)"
  },
  {
    "objectID": "slides/slides02.html#why-is-it-a-normalizing-constant",
    "href": "slides/slides02.html#why-is-it-a-normalizing-constant",
    "title": "Bayes Rule",
    "section": "Why is it a “normalizing constant”?",
    "text": "Why is it a “normalizing constant”?"
  },
  {
    "objectID": "slides/slides02.html#summary",
    "href": "slides/slides02.html#summary",
    "title": "Bayes Rule",
    "section": "Summary",
    "text": "Summary\nThree steps to a Bayesian analysis:\n\nConstruct a prior model for the variable of interest (possible values and the relative plausibility of each)\nUpon observing data \\(Y = y\\), define the likelihoood \\(L(\\pi |y) = P(y | \\pi)\\)\nBuild the posterior model of the variable of interest using Bayes Rule, which balances the prior and likelihood: \\[\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\propto \\text{prior} \\cdot \\text{likelihood}\\]"
  },
  {
    "objectID": "slides/slides09.html#markov-chain-monte-carlo",
    "href": "slides/slides09.html#markov-chain-monte-carlo",
    "title": "Care and feeding of your MCMCs",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nA MCMC sample \\(\\{ \\theta^{(1)}, \\theta^{(1)}, ..., \\theta^{(N)}\\}\\) is a sequence of values drawn from conditional pdf:\n\\[f(\\theta^{(i+1)}|\\theta^{(i)}, y)\\] when done well, it converges approximately to the posterior \\(f(\\theta |y)\\)"
  },
  {
    "objectID": "slides/slides09.html#mcmc-plan",
    "href": "slides/slides09.html#mcmc-plan",
    "title": "Care and feeding of your MCMCs",
    "section": "MCMC plan",
    "text": "MCMC plan\n\nYesterday: how to find them with {rstan}\nToday: how to evaluate them\nNext week: how they work"
  },
  {
    "objectID": "slides/slides09.html#beta-binomial-example-in-rstan",
    "href": "slides/slides09.html#beta-binomial-example-in-rstan",
    "title": "Care and feeding of your MCMCs",
    "section": "Beta-Binomial Example in {rstan}",
    "text": "Beta-Binomial Example in {rstan}\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    real&lt;lower=0&gt; alpha;\n    real&lt;lower=0&gt; beta;\n    int&lt;lower=1&gt; n;\n    int&lt;lower=0, upper=n&gt; Y;\n  }\n\n  parameters {\n    real&lt;lower=0, upper=1&gt; pi;\n  }\n\n  model {\n    Y ~ binomial(n, pi);\n    pi ~ beta(alpha, beta);\n  }\n\"\n\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, \n               data = list(alpha = 2, beta = 2, Y = 9, n = 10), \n               chains = 4, iter = 1000*2, seed = 84735)"
  },
  {
    "objectID": "slides/slides09.html#section",
    "href": "slides/slides09.html#section",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "bb_sim\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n      mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff Rhat\npi    0.79    0.00 0.11   0.55  0.72  0.80  0.87  0.95  1304    1\nlp__ -7.85    0.02 0.80 -10.13 -8.05 -7.53 -7.33 -7.27  1419    1\n\nSamples were drawn using NUTS(diag_e) at Thu Oct  2 20:42:34 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/slides09.html#section-1",
    "href": "slides/slides09.html#section-1",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "mcmc_dens(bb_sim, pars = \"pi\") +\n  stat_function(fun = dbeta, args = list(11, 3), col = \"darkorange\", linewidth = 2)"
  },
  {
    "objectID": "slides/slides09.html#in-practice-we-dont-know-the-true-posterior",
    "href": "slides/slides09.html#in-practice-we-dont-know-the-true-posterior",
    "title": "Care and feeding of your MCMCs",
    "section": "In practice, we don’t know the true posterior",
    "text": "In practice, we don’t know the true posterior\n\nWhat does a good markov chain look like?\nHow can we tell if we are getting a reasonable approximation of the posterior?\nHow big should our Markov chain size be?\n\n\n\nSome examples from today are from Statistical Rethinking"
  },
  {
    "objectID": "slides/slides09.html#traceplots",
    "href": "slides/slides09.html#traceplots",
    "title": "Care and feeding of your MCMCs",
    "section": "Traceplots",
    "text": "Traceplots\n\n\n\nmcmc_trace(bb_sim, pars = \"pi\")\n\n\n\n\n\n\n\n\n\n\nIdeally, \\(\\theta^{(i+1)}\\) does not depend on \\(\\theta^{(i)}\\) too much\nShould look like a “fat hairy caterpillar”"
  },
  {
    "objectID": "slides/slides09.html#first-20-draws-burn-in-warmup",
    "href": "slides/slides09.html#first-20-draws-burn-in-warmup",
    "title": "Care and feeding of your MCMCs",
    "section": "First 20 draws (burn-in / warmup)",
    "text": "First 20 draws (burn-in / warmup)"
  },
  {
    "objectID": "slides/slides09.html#first-200-draws-burn-in-warmup",
    "href": "slides/slides09.html#first-200-draws-burn-in-warmup",
    "title": "Care and feeding of your MCMCs",
    "section": "First 200 draws (burn-in / warmup)",
    "text": "First 200 draws (burn-in / warmup)"
  },
  {
    "objectID": "slides/slides09.html#mixing-slowly",
    "href": "slides/slides09.html#mixing-slowly",
    "title": "Care and feeding of your MCMCs",
    "section": "“Mixing slowly”",
    "text": "“Mixing slowly”"
  },
  {
    "objectID": "slides/slides09.html#getting-stuck",
    "href": "slides/slides09.html#getting-stuck",
    "title": "Care and feeding of your MCMCs",
    "section": "“Getting Stuck”",
    "text": "“Getting Stuck”"
  },
  {
    "objectID": "slides/slides09.html#each-chain-should-converge-on-roughly-the-same-distribution",
    "href": "slides/slides09.html#each-chain-should-converge-on-roughly-the-same-distribution",
    "title": "Care and feeding of your MCMCs",
    "section": "Each chain should converge on roughly the same distribution",
    "text": "Each chain should converge on roughly the same distribution\n\nmcmc_dens_overlay(bb_sim, pars = \"pi\")"
  },
  {
    "objectID": "slides/slides09.html#how-many-chains-do-you-need",
    "href": "slides/slides09.html#how-many-chains-do-you-need",
    "title": "Care and feeding of your MCMCs",
    "section": "How many chains do you need?",
    "text": "How many chains do you need?\n\nWhen testing and debugging your model, use 1\nWhen deciding whether your chains are valid, need &gt;1\nWhen you do your final run that you will make inferences from, one long chain or multiple chains is fine\n\n\n\n\n\n\n\nRule of thumb:\n\n\nOne (short) chain to debug, four chains for verification and inference"
  },
  {
    "objectID": "slides/slides09.html#autocorrelation",
    "href": "slides/slides09.html#autocorrelation",
    "title": "Care and feeding of your MCMCs",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmcmc_acf(bb_sim, pars = \"pi\")"
  },
  {
    "objectID": "slides/slides09.html#slow-mixing-chains-often-have-high-autocorrelation",
    "href": "slides/slides09.html#slow-mixing-chains-often-have-high-autocorrelation",
    "title": "Care and feeding of your MCMCs",
    "section": "Slow mixing chains often have high autocorrelation",
    "text": "Slow mixing chains often have high autocorrelation"
  },
  {
    "objectID": "slides/slides09.html#effective-sample-size",
    "href": "slides/slides09.html#effective-sample-size",
    "title": "Care and feeding of your MCMCs",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\n\n\n\n\n\n\\(N_{eff}\\)\n\n\nEffective Sample Size (\\(N_{eff}\\)) quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation\n\n\n\n\n\n\n\n\n\n\\(N_{eff}/N\\)\n\n\nEffective sample size ratio\n\n\n\n\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3260274"
  },
  {
    "objectID": "slides/slides09.html#r-hat",
    "href": "slides/slides09.html#r-hat",
    "title": "Care and feeding of your MCMCs",
    "section": "R-hat",
    "text": "R-hat\nIdea: In a “good” Markov chain, the variability across all chains is similar to the variability within a given chain. In a “bad” Markov chain, the variability across all chains is bigger than the variability within a given chain"
  },
  {
    "objectID": "slides/slides09.html#r-hat-1",
    "href": "slides/slides09.html#r-hat-1",
    "title": "Care and feeding of your MCMCs",
    "section": "R-hat",
    "text": "R-hat\n\n\n\n\n\n\nGelman-Rubin convergence diagnostic\n\n\nR-hat \\(\\approx \\sqrt{\\frac{\\text{Var}_{\\text{combined}}}{\\text{Var}_{\\text{within}}}}\\)\nR-hat \\(\\approx\\) 1 is good; R-hat \\(&gt;1\\) indicates the chains have not converged\n\n\n\n\nrhat(bb_sim)\n\n       pi      lp__ \n0.9999668 1.0012277"
  },
  {
    "objectID": "slides/slides09.html#section-2",
    "href": "slides/slides09.html#section-2",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "fn_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 1000);\n    sigma ~ exponential(.0001);\n  }\n\""
  },
  {
    "objectID": "slides/slides09.html#section-3",
    "href": "slides/slides09.html#section-3",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "# STEP 2: SIMULATE the posterior\nfn_sim &lt;- stan(model_code = fn_model, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\nWarning :There were 521 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nWarning :Examine the pairs() plot to diagnose sampling problems\n\nWarning :Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\nWarning :Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess"
  },
  {
    "objectID": "slides/slides09.html#mu",
    "href": "slides/slides09.html#mu",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\mu\\)",
    "text": "\\(\\mu\\)\nmcmc_trace(fn_sim, pars = \"mu\")\nmcmc_dens_overlay(fn_sim, pars = \"mu\")"
  },
  {
    "objectID": "slides/slides09.html#sigma",
    "href": "slides/slides09.html#sigma",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\sigma\\)",
    "text": "\\(\\sigma\\)\nmcmc_trace(fn_sim, pars = \"sigma\")\nmcmc_dens_overlay(fn_sim, pars = \"sigma\")"
  },
  {
    "objectID": "slides/slides09.html#section-4",
    "href": "slides/slides09.html#section-4",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "fn_model2 &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 10);\n    sigma ~ exponential(1);\n  }\n\""
  },
  {
    "objectID": "slides/slides09.html#section-5",
    "href": "slides/slides09.html#section-5",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "# STEP 2: SIMULATE the posterior\nfn_sim2 &lt;- stan(model_code = fn_model2, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\nWarning :There were 3 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nWarning :Examine the pairs() plot to diagnose sampling problems"
  },
  {
    "objectID": "slides/slides09.html#mu-1",
    "href": "slides/slides09.html#mu-1",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\mu\\)",
    "text": "\\(\\mu\\)\nmcmc_trace(fn_sim2, pars = \"mu\")\nmcmc_dens_overlay(fn_sim2, pars = \"mu\")"
  },
  {
    "objectID": "slides/slides09.html#sigma-1",
    "href": "slides/slides09.html#sigma-1",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\sigma\\)",
    "text": "\\(\\sigma\\)\nmcmc_trace(fn_sim2, pars = \"sigma\")\nmcmc_dens_overlay(fn_sim2, pars = \"sigma\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 340: Bayesian Statistics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the term. Note that this schedule will be updated as the term progresses and the timeline of topics and assignments might be updated. Any major changes to due dates will be announced in class and indicated in bold here.\n\n\n\n\n\n\nWEEK\nDOW\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nM\nMon, Sep 15\n📝complete welcome survey  📚 Bayes Rules Ch1  💻 Join Slack workspace\n\nIntro to Bayesian Thinking\nSlides01\n\n\n\n1\nW\nWed, Sep 17\n📚 Bayes Rules Ch2  💻 Complete computing setup\n\nBayes Rule, R Setup\nSlides02\n\n\n\n1\nF\nFri, Sep 19\n\nProbability and Likelihood Review; Posterior Simulation\nSlides03\n\nHW 1 and GW 1\n\n\n\n2\nM\nMon, Sep 22\n📚 Bayes Rules Ch3 \n\nBeta-Binomial Model\nSlides04\n\n\n\n2\nW\nWed, Sep 24\n📚 Bayes Rules Ch4 \n\nBalance and Sequentiality\nSlides05\n\n\n\n2\nF\nFri, Sep 26\n📚 Bayes Rules Ch5.1-2 \n\nGamma-Gamma\nSlides06\n\nHW 2 and GW 2\n\n\n\n3\nM\nMon, Sep 29\n📚 Bayes Rules Ch5.3-5.6  📝 complete group eval \n\nNormal-Normal\nSlides07\n\n\n\n3\nW\nWed, Oct 1\n📚 Bayes Rules Ch6.1-2 \n\nGrid approximation and Markov Chains\nSlides08\n\n\n\n3\nF\nFri, Oct 3\n📚 Bayes Rules Ch6.3 \n\nDiagnostics\nSlides09\n\nHW 3 and GW 3\n\n\n\n4\nM\nMon, Oct 6\n📚 Bayes Rules Ch7.1-7.3  📝 complete group eval \n\nMCMC I\ntbd\nHand out computing project\n\n\n4\nW\nWed, Oct 8\n📚 Bayes Rules Ch7.4-7 \n\nMCMC II\ntbd\n\n\n\n4\nF\nFri, Oct 10\n\nExam I\ntbd\n\nHW 4 and GW 4\n\n\n\n5\nM\nMon, Oct 13\n\nPosterior Inference\ntbd\n\n\n\n5\nW\nWed, Oct 15\n\nPosterior Prediction\ntbd\n\n\n\n5\nF\nFri, Oct 17\n\nRegression\ntbd\nHW5\n\n\n6\nM\nMon, Oct 20\n\n❌ Midterm Break; no class\ntbd\n\n\n\n6\nW\nWed, Oct 22\n\nEvaluating I\ntbd\nComputing Project Due\n\n\n6\nF\nFri, Oct 24\n\nEvaluating II\ntbd\nHW6\n\n\n7\nM\nMon, Oct 27\n\nExtending Regression Model\ntbd\n\n\n\n7\nW\nWed, Oct 29\n\nPoisson Regression\ntbd\n\n\n\n7\nF\nFri, Oct 31\n\nNegative Binomial Regression\ntbd\nHW7\n\n\n8\nM\nMon, Nov 3\n\nLogistic Regression\ntbd\n\n\n\n8\nW\nWed, Nov 5\n\nNaive Bayes Classification\ntbd\n\n\n\n8\nF\nFri, Nov 7\n\nExam II\ntbd\nHW8\n\n\n9\nM\nMon, Nov 10\n\nHierarchical Models I\ntbd\n\n\n\n9\nW\nWed, Nov 12\n\nHierarchical Models II\ntbd\n\n\n\n9\nF\nFri, Nov 14\n\nHierarchical Models III\ntbd\n\n\n\n10\nM\nMon, Nov 17\n\nProject work day\ntbd\nHW 9\n\n\n10\nW\nWed, Nov 19\n\nLightning talks\ntbd\n\n\n\n\nM\nFri, Nov 21\n\nFinal Project Due @ noon\ntbd",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "slides/slides08.html#recap-conjugate-models",
    "href": "slides/slides08.html#recap-conjugate-models",
    "title": "Intro to Posterior Approximation",
    "section": "Recap: Conjugate models",
    "text": "Recap: Conjugate models"
  },
  {
    "objectID": "slides/slides08.html#motivation",
    "href": "slides/slides08.html#motivation",
    "title": "Intro to Posterior Approximation",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/slides08.html#section-1",
    "href": "slides/slides08.html#section-1",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Grid Approximation\n\n\nGrid approximation produces a sample of \\(N\\) independent \\(\\theta\\) values, \\(\\left\\lbrace \\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(N)} \\right\\rbrace\\), from a discretized approximation of posterior pdf \\(f(\\theta|y)\\). This algorithm evolves in four steps:\n\nDefine a discrete grid of possible \\(\\theta\\) values.\nEvaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\nObtain a discrete approximation of posterior pdf \\(f(\\theta |y)\\) by:\n\n\ncalculating the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value\nnormalizing the products so that they sum to 1 across all \\(\\theta\\).\n\n\nRandomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "slides/slides08.html#example-beta-binomial",
    "href": "slides/slides08.html#example-beta-binomial",
    "title": "Intro to Posterior Approximation",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\nDiscretize \\(\\pi\\):\n\\[\\pi \\in \\{0, .2, .4, .6, .8, 1.0\\}\\]"
  },
  {
    "objectID": "slides/slides08.html#define-a-grid-of-possible-theta-values",
    "href": "slides/slides08.html#define-a-grid-of-possible-theta-values",
    "title": "Intro to Posterior Approximation",
    "section": "1. Define a grid of possible \\(\\theta\\) values",
    "text": "1. Define a grid of possible \\(\\theta\\) values\n\\[\\pi \\in \\{0, .2, .4, .6, .8, 1.0\\}\\]\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))\n\ngrid_data\n\n# A tibble: 6 × 1\n  pi_grid\n    &lt;dbl&gt;\n1     0  \n2     0.2\n3     0.4\n4     0.6\n5     0.8\n6     1"
  },
  {
    "objectID": "slides/slides08.html#evaluate-the-prior-pdf-ftheta-and-likelihood-function-lthetay-at-each-theta-grid-value.",
    "href": "slides/slides08.html#evaluate-the-prior-pdf-ftheta-and-likelihood-function-lthetay-at-each-theta-grid-value.",
    "title": "Intro to Posterior Approximation",
    "section": "2. Evaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.",
    "text": "2. Evaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n\n\n\n# A tibble: 6 × 3\n  pi_grid prior likelihood\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1     0    0    0         \n2     0.2  0.96 0.00000410\n3     0.4  1.44 0.00157   \n4     0.6  1.44 0.0403    \n5     0.8  0.96 0.268     \n6     1    0    0"
  },
  {
    "objectID": "slides/slides08.html#a.-calculate-the-product-fthetalthetay-at-each-theta-grid-value",
    "href": "slides/slides08.html#a.-calculate-the-product-fthetalthetay-at-each-theta-grid-value",
    "title": "Intro to Posterior Approximation",
    "section": "3a. Calculate the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value",
    "text": "3a. Calculate the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(\n    unnorm_post = likelihood * prior\n  )\n\n\n\n# A tibble: 6 × 4\n  pi_grid prior likelihood unnorm_post\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     0    0    0           0         \n2     0.2  0.96 0.00000410  0.00000393\n3     0.4  1.44 0.00157     0.00226   \n4     0.6  1.44 0.0403      0.0580    \n5     0.8  0.96 0.268       0.258     \n6     1    0    0           0"
  },
  {
    "objectID": "slides/slides08.html#b.-normalize-this-product",
    "href": "slides/slides08.html#b.-normalize-this-product",
    "title": "Intro to Posterior Approximation",
    "section": "3b. Normalize this product",
    "text": "3b. Normalize this product\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(\n    posterior = unnorm_post / sum(unnorm_post)\n  )\n\n\n\n# A tibble: 6 × 5\n  pi_grid prior likelihood unnorm_post posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1     0    0    0           0          0        \n2     0.2  0.96 0.00000410  0.00000393 0.0000124\n3     0.4  1.44 0.00157     0.00226    0.00712  \n4     0.6  1.44 0.0403      0.0580     0.183    \n5     0.8  0.96 0.268       0.258      0.810    \n6     1    0    0           0          0"
  },
  {
    "objectID": "slides/slides08.html#pause-to-check-does-it-sum-to-1",
    "href": "slides/slides08.html#pause-to-check-does-it-sum-to-1",
    "title": "Intro to Posterior Approximation",
    "section": "Pause to check: does it sum to 1?",
    "text": "Pause to check: does it sum to 1?\n\n# Confirm that the posterior approximation sums to 1\ngrid_data %&gt;% \n  summarize(sum(unnorm_post), sum(posterior))\n\n# A tibble: 1 × 2\n  `sum(unnorm_post)` `sum(posterior)`\n               &lt;dbl&gt;            &lt;dbl&gt;\n1              0.318                1"
  },
  {
    "objectID": "slides/slides08.html#randomly-sample-n-theta-grid-values-with-respect-to-their-corresponding-normalized-posterior-probabilities.",
    "href": "slides/slides08.html#randomly-sample-n-theta-grid-values-with-respect-to-their-corresponding-normalized-posterior-probabilities.",
    "title": "Intro to Posterior Approximation",
    "section": "4. Randomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities.",
    "text": "4. Randomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities.\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\n\n\n# A tibble: 10,000 × 5\n   pi_grid prior likelihood unnorm_post posterior\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1     0.8  0.96     0.268       0.258      0.810\n 2     0.8  0.96     0.268       0.258      0.810\n 3     0.8  0.96     0.268       0.258      0.810\n 4     0.6  1.44     0.0403      0.0580     0.183\n 5     0.8  0.96     0.268       0.258      0.810\n 6     0.8  0.96     0.268       0.258      0.810\n 7     0.8  0.96     0.268       0.258      0.810\n 8     0.8  0.96     0.268       0.258      0.810\n 9     0.8  0.96     0.268       0.258      0.810\n10     0.8  0.96     0.268       0.258      0.810\n# ℹ 9,990 more rows"
  },
  {
    "objectID": "slides/slides08.html#section-2",
    "href": "slides/slides08.html#section-2",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "# Histogram of the grid simulation with posterior pdf\nggplot(post_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), color = \"white\") + \n  stat_function(fun = dbeta, args = list(11, 3)) + \n  lims(x = c(0, 1))"
  },
  {
    "objectID": "slides/slides08.html#section-4",
    "href": "slides/slides08.html#section-4",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "# Step 1: Define a grid of 101 pi values\ngrid_data  &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 101))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)"
  },
  {
    "objectID": "slides/slides08.html#what-does-grid-approximation-give-us-that-conjugacy-doesnt",
    "href": "slides/slides08.html#what-does-grid-approximation-give-us-that-conjugacy-doesnt",
    "title": "Intro to Posterior Approximation",
    "section": "What does grid approximation give us that conjugacy doesn’t?",
    "text": "What does grid approximation give us that conjugacy doesn’t?\nRemember our non-conjugate prior: \\(f(\\pi) = e - e^\\pi\\)?"
  },
  {
    "objectID": "slides/slides08.html#section-6",
    "href": "slides/slides08.html#section-6",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "grid_data  &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 101))\n\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = exp(1) - exp(pi_grid),\n         likelihood = dbinom(9, 10, pi_grid))\n\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\nset.seed(84735)\n\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)"
  },
  {
    "objectID": "slides/slides08.html#where-does-grid-approximation-start-to-fail",
    "href": "slides/slides08.html#where-does-grid-approximation-start-to-fail",
    "title": "Intro to Posterior Approximation",
    "section": "Where does grid approximation start to fail?",
    "text": "Where does grid approximation start to fail?"
  },
  {
    "objectID": "slides/slides08.html#plan",
    "href": "slides/slides08.html#plan",
    "title": "Intro to Posterior Approximation",
    "section": "Plan",
    "text": "Plan\n\nToday: how to find them with {rstan}\nFriday: how to evaluate them\nNext week: how they work"
  },
  {
    "objectID": "slides/slides08.html#section-8",
    "href": "slides/slides08.html#section-8",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Markov Chain Monte Carlo (MCMC)\n\n\nMCMC is a class of algorithms used to approximate a probability distribution.\nLet \\(\\{\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n)} \\}\\) be an N-length MCMC sample. Then, \\(\\theta^{(i+1)}\\) is drawn from a model with conditional pdf\n\\[f(\\theta^{(i+1)}|\\theta^{(i)}, y)\\]\n\n\n\nNote that:\n\nMarkov property\nNot equivalent to posterior pdf\nIf done well, approximates posterior pdf"
  },
  {
    "objectID": "slides/slides08.html#rstan",
    "href": "slides/slides08.html#rstan",
    "title": "Intro to Posterior Approximation",
    "section": "{rstan}",
    "text": "{rstan}\n\nlibrary(rstan)\n\nStan is a probabilistic programming language written in C++ (https://mc-stan.org/)\n{rstan} is the R interface to Stan\nThere are also interfaces for python, Julia, Mathematica, MATLAB, etc.\n\n\n\n\n\n\nNote\n\n\nSince {rstan} uses C++ under the hood, you might need to follow some additional steps to install the package. Follow the directions at https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started carefully!"
  },
  {
    "objectID": "slides/slides08.html#steps",
    "href": "slides/slides08.html#steps",
    "title": "Intro to Posterior Approximation",
    "section": "Steps",
    "text": "Steps\n\nWrite out the model in {rstan syntax}\n\ndata\nparameters\nmodel\n\nSample from the approximate posterior"
  },
  {
    "objectID": "slides/slides08.html#beta-binomial-example",
    "href": "slides/slides08.html#beta-binomial-example",
    "title": "Intro to Posterior Approximation",
    "section": "Beta-Binomial Example",
    "text": "Beta-Binomial Example\nDefine the model:\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\""
  },
  {
    "objectID": "slides/slides08.html#beta-binomial-example-1",
    "href": "slides/slides08.html#beta-binomial-example-1",
    "title": "Intro to Posterior Approximation",
    "section": "Beta-Binomial Example",
    "text": "Beta-Binomial Example\nSample from the posterior:\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 4:                0.081 seconds (Sampling)\nChain 4:                0.107 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "slides/slides08.html#section-9",
    "href": "slides/slides08.html#section-9",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "bb_sim\n\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi    0.78    0.00 0.11  0.55  0.72  0.80  0.86  0.95  6587    1\nlp__ -7.80    0.01 0.73 -9.87 -7.98 -7.52 -7.33 -7.27  6462    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct  1 08:13:19 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/slides08.html#section-10",
    "href": "slides/slides08.html#section-10",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Burn-in\n\n\nIf you’ve ever made a batch of pancakes or crêpes, you know that the first pancake is always the worst – the pan isn’t yet at the perfect temperature, you haven’t yet figured out how much batter to use, and you need more time to practice your flipping technique. MCMC chains are similar. Without direct knowledge of the posterior it’s trying to simulate, the Markov chain might start out sampling unreasonable values of our parameter of interest. Eventually though, it learns and starts producing values that mimic a random sample from the posterior. And just as we might need to toss out the first pancake, we might want to toss the Markov chain values produced during this learning period – keeping them in our sample might lead to a poor posterior approximation. As such, “burn-in” is the practice of discarding the first portion of Markov chain values."
  },
  {
    "objectID": "slides/slides08.html#section-11",
    "href": "slides/slides08.html#section-11",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "as.array(bb_sim, pars = \"pi\") |&gt;\n  head(5)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.9288874 0.8637566 0.6410152 0.8054958\n      [2,] 0.8710291 0.6737700 0.6861136 0.8727761\n      [3,] 0.8105749 0.7752253 0.6994077 0.8625163\n      [4,] 0.7567798 0.5318069 0.8200739 0.8065095\n      [5,] 0.7410937 0.7004429 0.8200739 0.8065095"
  },
  {
    "objectID": "slides/slides08.html#plotting-with-the-bayesplot-package",
    "href": "slides/slides08.html#plotting-with-the-bayesplot-package",
    "title": "Intro to Posterior Approximation",
    "section": "Plotting with the {bayesplot} package",
    "text": "Plotting with the {bayesplot} package\n\n\nlibrary(bayesplot)\n\n# Histogram of the Markov chain values\nmcmc_hist(bb_sim, \n          pars = \"pi\") + \n  yaxis_text(TRUE) + \n  ylab(\"count\")"
  },
  {
    "objectID": "slides/slides08.html#plotting-with-the-bayesplot-package-1",
    "href": "slides/slides08.html#plotting-with-the-bayesplot-package-1",
    "title": "Intro to Posterior Approximation",
    "section": "Plotting with the {bayesplot} package",
    "text": "Plotting with the {bayesplot} package\n\n# Density plot of the Markov chain values\nmcmc_dens(bb_sim, \n                pars = \"pi\") + \n  yaxis_text(TRUE) + \n  ylab(\"density\")"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#columns",
    "href": "slides/slides-cheat-sheet.html#columns",
    "title": "Slides Cheat Sheet",
    "section": "Columns",
    "text": "Columns\n\n\n\nFirst year at Carleton!\nTaught at Swarthmore for 5 years before moving here this fall\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#only-graph-from-code",
    "href": "slides/slides-cheat-sheet.html#only-graph-from-code",
    "title": "Slides Cheat Sheet",
    "section": "Only graph from code",
    "text": "Only graph from code"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#background-image-half",
    "href": "slides/slides-cheat-sheet.html#background-image-half",
    "title": "Slides Cheat Sheet",
    "section": "Background Image (half)",
    "text": "Background Image (half)\n\n\nWhat skills do you need?\n\nprogramming with data\nstatistical modeling\ndomain knowledge\ncommunication"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#full-size-image",
    "href": "slides/slides-cheat-sheet.html#full-size-image",
    "title": "Slides Cheat Sheet",
    "section": "Full size image",
    "text": "Full size image\n\n\n\nImage by Adam Loy  adapted from work of Joe Blitzstein, Hanspeter Pfister, and Hadley Wickham"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#background-image-full-size",
    "href": "slides/slides-cheat-sheet.html#background-image-full-size",
    "title": "Slides Cheat Sheet",
    "section": "Background image (full size)",
    "text": "Background image (full size)"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#countdown",
    "href": "slides/slides-cheat-sheet.html#countdown",
    "title": "Slides Cheat Sheet",
    "section": "Countdown",
    "text": "Countdown\n\nWith your neighbor(s):\nChoose two countries to compare to the U.S. voting record in the U.N. over the years.\nWhat did you learn?\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#table-column-widths",
    "href": "slides/slides-cheat-sheet.html#table-column-widths",
    "title": "Slides Cheat Sheet",
    "section": "table column widths",
    "text": "table column widths\n\n\n\n\n\n\n\n\nCollaboration Allowed\n\n\n\n\nHomework Problems\nYou are allowed and encouraged to collaborate on homework. You may also use outside resources, but your submitted work must be your own and reflect your own understanding .\n\n\nLab Quiz Problems\nNo collaboration is allowed at all . You may use your own notes for resubmissions, but should not use outside resources.\n\n\nPortfolio Projects\nYou are expected to collaborate with your group, but cannot rely on external sources other than to help motivate the questions or provide other background information. Getting answers on significant parts of solutions from outside resources is not allowed.\n\n\nFinal Project\nYou are expected to collaborate with your group, but cannot rely on external sources other than to help motivate the questions or provide other background information. Any outside resources should be properly cited."
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#use-of-generative-artificial-intelligence-ai",
    "href": "slides/slides-cheat-sheet.html#use-of-generative-artificial-intelligence-ai",
    "title": "Slides Cheat Sheet",
    "section": "Use of generative artificial intelligence (AI)",
    "text": "Use of generative artificial intelligence (AI)\n\nTreat generative AI, such as ChatGPT or Gemini, the same as other online resources.\nGuiding principles:\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. AI should facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n❌ AI tools for writing code: You may not use generative AI to take a “first pass” at a coding task. Do not type coursework prompts directly into AI tools.\n✅ AI tools for debugging code: You may make use of the technology to get help with error messages or trying to fix issues\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\n\n\nAdapted from Mine Çetinkaya-Rundel"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#linktask",
    "href": "slides/slides-cheat-sheet.html#linktask",
    "title": "Slides Cheat Sheet",
    "section": "Link/task",
    "text": "Link/task\n\nhttps://github.com/stat220-w25\n\n\nFill out the Welcome Survey for collection of your account names, later this week you will be invited to the course organization."
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#handwriting-font",
    "href": "slides/slides-cheat-sheet.html#handwriting-font",
    "title": "Slides Cheat Sheet",
    "section": "handwriting font",
    "text": "handwriting font\n\nin case you don’t yet have a GitHub account…"
  },
  {
    "objectID": "slides/slides03.html#is-this-the-same-likelihood-as-the-stat250-likelihood",
    "href": "slides/slides03.html#is-this-the-same-likelihood-as-the-stat250-likelihood",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Is this the same likelihood as the Stat250 likelihood?",
    "text": "Is this the same likelihood as the Stat250 likelihood?"
  },
  {
    "objectID": "slides/slides03.html#does-it-matter-if-y-is-a-discrete-or-continuous-rv",
    "href": "slides/slides03.html#does-it-matter-if-y-is-a-discrete-or-continuous-rv",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Does it matter if \\(Y\\) is a discrete or continuous RV?",
    "text": "Does it matter if \\(Y\\) is a discrete or continuous RV?"
  },
  {
    "objectID": "slides/slides03.html#example-from-last-time-phd-admissions",
    "href": "slides/slides03.html#example-from-last-time-phd-admissions",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Example from last time: PhD admissions",
    "text": "Example from last time: PhD admissions\n\n\n\nπ\n0.2\n0.4\n0.8\n\n\n\n\nf(π)\n0.7\n0.2\n0.1\n\n\nL(π | y = 3)\n0.0512\n0.2304\n0.2048\n\n\nf(π | y = 3)\n0.35\n0.45\n0.20"
  },
  {
    "objectID": "slides/slides03.html#first-define-and-simulate-from-prior",
    "href": "slides/slides03.html#first-define-and-simulate-from-prior",
    "title": "Likelihood review + Posterior Simulation",
    "section": "First, define and simulate from prior",
    "text": "First, define and simulate from prior\n\nadmissions &lt;- tibble(\n  pi = c(.2, .4, .8)\n)\n\nprior &lt;- c(0.7, 0.2, 0.1)\n\nset.seed(091925)\nadmissions_sim &lt;- sample_n(admissions, \n                           size = 10000, \n                           weight = prior, \n                           replace = TRUE)"
  },
  {
    "objectID": "slides/slides03.html#section",
    "href": "slides/slides03.html#section",
    "title": "Likelihood review + Posterior Simulation",
    "section": "",
    "text": "admissions_sim\n\n# A tibble: 10,000 × 1\n      pi\n   &lt;dbl&gt;\n 1   0.2\n 2   0.2\n 3   0.2\n 4   0.4\n 5   0.2\n 6   0.4\n 7   0.2\n 8   0.4\n 9   0.2\n10   0.8\n# ℹ 9,990 more rows"
  },
  {
    "objectID": "slides/slides03.html#simulated-pi-closely-approximates-prior-fpi",
    "href": "slides/slides03.html#simulated-pi-closely-approximates-prior-fpi",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulated pi closely approximates prior \\(f(\\pi)\\)",
    "text": "Simulated pi closely approximates prior \\(f(\\pi)\\)\n\nadmissions_sim |&gt;\n  tabyl(pi)\n\n  pi    n percent\n 0.2 7030  0.7030\n 0.4 2002  0.2002\n 0.8  968  0.0968"
  },
  {
    "objectID": "slides/slides03.html#simulate-outcomes",
    "href": "slides/slides03.html#simulate-outcomes",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulate outcomes",
    "text": "Simulate outcomes\n\nadmissions_sim &lt;- admissions_sim |&gt;\n  mutate(\n    y = rbinom(10000, size = 5, prob = pi)\n  )\n\nadmissions_sim |&gt; head(5)\n\n# A tibble: 5 × 2\n     pi     y\n  &lt;dbl&gt; &lt;int&gt;\n1   0.2     1\n2   0.2     2\n3   0.2     0\n4   0.4     0\n5   0.2     0"
  },
  {
    "objectID": "slides/slides03.html#simulated-y-outcomes-approximate-fypi",
    "href": "slides/slides03.html#simulated-y-outcomes-approximate-fypi",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulated y outcomes approximate \\(f(y|\\pi)\\)",
    "text": "Simulated y outcomes approximate \\(f(y|\\pi)\\)\n\nggplot(admissions_sim, aes(x = y)) +\n  geom_bar(aes(y = after_stat(prop))) + \n  facet_wrap(vars(pi))"
  },
  {
    "objectID": "slides/slides03.html#to-approximate-posterior-condition-on-observed-data-and-look-at-relative-frequency",
    "href": "slides/slides03.html#to-approximate-posterior-condition-on-observed-data-and-look-at-relative-frequency",
    "title": "Likelihood review + Posterior Simulation",
    "section": "To approximate posterior, condition on observed data and look at relative frequency",
    "text": "To approximate posterior, condition on observed data and look at relative frequency\n\nobserved &lt;- admissions_sim |&gt;\n  filter(y == 3)\n\nobserved |&gt;\n  tabyl(pi)\n\n  pi   n   percent\n 0.2 365 0.3628231\n 0.4 445 0.4423459\n 0.8 196 0.1948310"
  },
  {
    "objectID": "slides/slides04.html#back-to-graduate-school-applications",
    "href": "slides/slides04.html#back-to-graduate-school-applications",
    "title": "Beta-Binomial Model",
    "section": "Back to Graduate School Applications",
    "text": "Back to Graduate School Applications\nLast week we were trying to understand \\(\\pi\\) the acceptance rate of a graduate program in a specific department. We assumed a somewhat silly prior, and restricted \\(\\pi\\) to be 0.2, 0.4, or 0.8.\nThis time we will let \\(\\pi \\in [0,1]\\)."
  },
  {
    "objectID": "slides/slides04.html#section",
    "href": "slides/slides04.html#section",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "Continuous probability models\nLet \\(\\pi\\) be a continuous random variable with pdf \\(f(\\pi)\\). Then \\(f(\\pi)\\) has the following properties:\n\n\\(\\int_\\pi f(\\pi)d\\pi = 1\\), ie. the area under \\(f(\\pi)\\) is 1\n\\(f(\\pi) \\ge 0\\)\n\\(P(a &lt; \\pi &lt; b) = \\int_a^b f(\\pi) d\\pi\\) when \\(a \\le b\\)\n\nInterpreting \\(f(\\pi)\\):"
  },
  {
    "objectID": "slides/slides04.html#section-1",
    "href": "slides/slides04.html#section-1",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "For each of the student’s prior ideas for \\(\\pi\\) sketch the pdf of the prior. Your plot will not be exact since no exact values are given.\n\n\n\nMorteza thinks that it is extremely difficult to get into this program.\n\n\n\n\n\n\n\n\n\n\nErin does not have any strong opinions whether it is difficult or easy to get into this program."
  },
  {
    "objectID": "slides/slides04.html#section-2",
    "href": "slides/slides04.html#section-2",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "For each of the student’s prior ideas for \\(\\pi\\) sketch the pdf of the prior. Your plot will not be exact since no exact values are given.\n\n\n\nXuan thinks that it is either really easy or really hard to get into the program\n\n\n\n\n\n\n\n\n\n\nBeyoncé thinks that it is extremely easy to get into this program."
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model",
    "href": "slides/slides04.html#beta-prior-model",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\nLet \\(\\pi\\) be a random variable which can take any value between 0 and 1, ie. \\(\\pi \\in [0,1]\\). Then the variability in \\(\\pi\\) might be well modeled by a Beta model with shape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\):\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\]"
  },
  {
    "objectID": "slides/slides04.html#section-3",
    "href": "slides/slides04.html#section-3",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "The Beta model is specified by continuous pdf \\[\\begin{equation}\nf(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\;\\; \\text{ for } \\pi \\in [0,1]\n\\end{equation}\\]\nwhere \\(\\Gamma(z) = \\int_0^\\infty y^{z-1}e^{-y}dy\\) and \\(\\Gamma(z + 1) = z \\Gamma(z)\\).\nFun fact: when \\(z\\) is a positive integer, then \\(\\Gamma(z)\\) simplifies to \\(\\Gamma(z) = (z-1)!\\)."
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model-1",
    "href": "slides/slides04.html#beta-prior-model-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\pi \\sim \\text{Beta}(3, 8)\\)\n\\(f(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}\\)\n\\(f(0.5) =\\)"
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model-2",
    "href": "slides/slides04.html#beta-prior-model-2",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\n\\(\\pi \\sim \\text{Beta}(3, 8)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbeta(x = 0.5, \n      shape1 = 3, \n      shape2 = 8)\n\n[1] 0.703125"
  },
  {
    "objectID": "slides/slides04.html#plotting-beta-prior",
    "href": "slides/slides04.html#plotting-beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Plotting Beta prior",
    "text": "Plotting Beta prior"
  },
  {
    "objectID": "slides/slides04.html#plotting-beta-prior-with-bayesrules-package",
    "href": "slides/slides04.html#plotting-beta-prior-with-bayesrules-package",
    "title": "Beta-Binomial Model",
    "section": "Plotting Beta Prior with bayesrules package",
    "text": "Plotting Beta Prior with bayesrules package\n\nplot_beta(alpha = 3, beta = 8)"
  },
  {
    "objectID": "slides/slides04.html#beta-summaries",
    "href": "slides/slides04.html#beta-summaries",
    "title": "Beta-Binomial Model",
    "section": "Beta summaries",
    "text": "Beta summaries\n\\[E(\\pi) = \\frac{\\alpha}{\\alpha + \\beta}\\]\n\\[\\text{Mode}(\\pi) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}\\]\n\\[\\text{Var}(\\pi) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\]"
  },
  {
    "objectID": "slides/slides04.html#beta-summaries-with-bayesrules-package",
    "href": "slides/slides04.html#beta-summaries-with-bayesrules-package",
    "title": "Beta-Binomial Model",
    "section": "Beta summaries with bayesrules package",
    "text": "Beta summaries with bayesrules package\nUse the summarize_beta() function in the bayesrules package to find the mean, mode, and variance of various Beta distributions. Example:\n\nsummarize_beta(alpha = 5, beta = 7)\n\n       mean mode        var        sd\n1 0.4166667  0.4 0.01869658 0.1367354"
  },
  {
    "objectID": "slides/slides04.html#posterior-for-the-beta-binomial-model",
    "href": "slides/slides04.html#posterior-for-the-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Posterior for the Beta-Binomial model",
    "text": "Posterior for the Beta-Binomial model"
  },
  {
    "objectID": "slides/slides04.html#posterior-for-the-beta-binomial-model-1",
    "href": "slides/slides04.html#posterior-for-the-beta-binomial-model-1",
    "title": "Beta-Binomial Model",
    "section": "Posterior for the Beta-Binomial model",
    "text": "Posterior for the Beta-Binomial model"
  },
  {
    "objectID": "slides/slides04.html#conjugate-prior",
    "href": "slides/slides04.html#conjugate-prior",
    "title": "Beta-Binomial Model",
    "section": "Conjugate prior",
    "text": "Conjugate prior\nWe say that \\(f(\\pi)\\) is a conjugate prior for \\(L(\\pi|y)\\) if the posterior, \\(f(\\pi|y) \\propto f(\\pi)L(\\pi|y)\\), is from the same model family as the prior.\nThus, the Beta distribution is a conjugate prior for the Binomial likelihood model since the posterior also follows a Beta distribution."
  },
  {
    "objectID": "slides/slides04.html#admission-example",
    "href": "slides/slides04.html#admission-example",
    "title": "Beta-Binomial Model",
    "section": "Admission Example",
    "text": "Admission Example\n\nOriginal discrete prior:\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n.7\n.2\n.1\n\n\n\n\nBeta approximation:\n\nplot_beta(3, 7, mean = TRUE, mode = TRUE)"
  },
  {
    "objectID": "slides/slides04.html#data-posterior",
    "href": "slides/slides04.html#data-posterior",
    "title": "Beta-Binomial Model",
    "section": "Data + Posterior",
    "text": "Data + Posterior\n3/5 students were admitted\n\\[ \\pi \\sim \\text{Beta}(3, 7)\\] \\[ \\pi | Y \\sim \\text{Beta}(3 + y, 7 + n -y)\\] \\[ \\pi | Y \\sim \\text{Beta}(6, 9)\\]"
  },
  {
    "objectID": "slides/slides04.html#posterior-summary",
    "href": "slides/slides04.html#posterior-summary",
    "title": "Beta-Binomial Model",
    "section": "Posterior summary",
    "text": "Posterior summary\n\nsummarize_beta(6,9)\n\n  mean      mode   var        sd\n1  0.4 0.3846154 0.015 0.1224745\n\nplot_beta(6, 9, mean = TRUE, mode = TRUE)"
  },
  {
    "objectID": "slides/slides04.html#balancing-act",
    "href": "slides/slides04.html#balancing-act",
    "title": "Beta-Binomial Model",
    "section": "Balancing act",
    "text": "Balancing act\n\nplot_beta_binomial(alpha = 3, beta = 7,  y = 3, n = 5)"
  },
  {
    "objectID": "slides/slides04.html#more-data-more-certainty",
    "href": "slides/slides04.html#more-data-more-certainty",
    "title": "Beta-Binomial Model",
    "section": "More data, more certainty",
    "text": "More data, more certainty\nWhat if 30/50 applicants get in?\n\nplot_beta_binomial(alpha = 3, beta = 7, y = 30, n = 50)"
  },
  {
    "objectID": "slides/slides04.html#more-more-data-more-more-certainty",
    "href": "slides/slides04.html#more-more-data-more-more-certainty",
    "title": "Beta-Binomial Model",
    "section": "More more data, more more certainty",
    "text": "More more data, more more certainty\nWhat if 300/500 applicants get in?\n\nplot_beta_binomial(alpha = 3, beta = 7, y = 300, n = 500)"
  },
  {
    "objectID": "slides/slides04.html#data-context",
    "href": "slides/slides04.html#data-context",
    "title": "Beta-Binomial Model",
    "section": "Data Context",
    "text": "Data Context\nLet \\(\\pi\\) represent the proportion of students who are admitted to the program.\n\\(Y | \\pi \\sim \\text{Binom}(n, \\pi)\\)"
  },
  {
    "objectID": "slides/slides04.html#prior",
    "href": "slides/slides04.html#prior",
    "title": "Beta-Binomial Model",
    "section": "Prior",
    "text": "Prior\n\nplot_beta(3, 7)"
  },
  {
    "objectID": "slides/slides04.html#data-and-the-posterior",
    "href": "slides/slides04.html#data-and-the-posterior",
    "title": "Beta-Binomial Model",
    "section": "Data and the Posterior",
    "text": "Data and the Posterior\n30/50 students are admitted\n\nsummarize_beta_binomial(3, 7, y = 30, n = 50)\n\n      model alpha beta mean      mode         var         sd\n1     prior     3    7 0.30 0.2500000 0.019090909 0.13816986\n2 posterior    33   27 0.55 0.5517241 0.004057377 0.06369754"
  },
  {
    "objectID": "slides/slides04.html#section-4",
    "href": "slides/slides04.html#section-4",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "plot_beta_binomial(3, 7, y = 30, n = 50)\n\n\n\n\n\n\n\n\nCan we simulate the posterior without knowing it exactly?"
  },
  {
    "objectID": "slides/slides04.html#section-5",
    "href": "slides/slides04.html#section-5",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "set.seed(84735)\nadmission_sim &lt;- data.frame(pi = rbeta(10000, 3, 7)) \nhead(admission_sim)\n\n         pi\n1 0.4245463\n2 0.2796543\n3 0.4484403\n4 0.3412382\n5 0.4517916\n6 0.6361449\n\nadmission_sim &lt;- admission_sim |&gt;\n  mutate(y = rbinom(10000, size = 50, prob = pi))"
  },
  {
    "objectID": "slides/slides04.html#section-6",
    "href": "slides/slides04.html#section-6",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "pi\ny\n\n\n\n\n0.4245463\n24\n\n\n0.2796543\n12\n\n\n0.4484403\n24\n\n\n0.3412382\n19\n\n\n0.4517916\n29\n\n\n0.6361449\n29\n\n\n0.3087587\n19\n\n\n0.7005417\n33\n\n\n0.3560422\n20\n\n\n0.3904306\n22"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes",
    "href": "slides/slides04.html#simulation-outcomes",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes",
    "text": "Simulation outcomes"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes-that-match-the-data",
    "href": "slides/slides04.html#simulation-outcomes-that-match-the-data",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes that match the data",
    "text": "Simulation outcomes that match the data"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes-that-match-the-data-1",
    "href": "slides/slides04.html#simulation-outcomes-that-match-the-data-1",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes that match the data",
    "text": "Simulation outcomes that match the data\n\nadmission_posterior &lt;- admission_sim |&gt; \n  filter(y == 30)\n\n\nnrow(admission_posterior)\n\n[1] 100\n\n\n\\(f(\\pi|y =30)\\)"
  },
  {
    "objectID": "slides/slides04.html#simulated-posterior",
    "href": "slides/slides04.html#simulated-posterior",
    "title": "Beta-Binomial Model",
    "section": "Simulated Posterior",
    "text": "Simulated Posterior\n\nggplot(admission_posterior, aes(x = pi)) + \n  geom_histogram(color = \"white\", binwidth = 0.025)"
  },
  {
    "objectID": "slides/slides04.html#simulated-posterior-density-estimate",
    "href": "slides/slides04.html#simulated-posterior-density-estimate",
    "title": "Beta-Binomial Model",
    "section": "Simulated posterior density estimate",
    "text": "Simulated posterior density estimate\n\nggplot(admission_posterior, aes(x = pi)) + \n  geom_density() + xlim(0,1)"
  },
  {
    "objectID": "slides/slides07.html#recap",
    "href": "slides/slides07.html#recap",
    "title": "Normal-Normal model",
    "section": "Recap",
    "text": "Recap\n\nStart with a reasonable likelihood for the data\nA conjugate prior for a given likelihood is a prior model that results in a posterior with the same shape, but different parameters\nThe beta distribution is a conjugate prior for the binomial likelihood\nThe gamma distribution is a conjugate prior for the poisson likelihood"
  },
  {
    "objectID": "slides/slides07.html#section",
    "href": "slides/slides07.html#section",
    "title": "Normal-Normal model",
    "section": "",
    "text": "Beta-Binomial Bayesian Model\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\] \\[ Y | \\pi  \\sim \\text{Binomial}(n, \\pi)\\] \\[ \\pi |Y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nGamma-Poisson Bayesian Model\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\] \\[ Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\]\n\\[\\lambda | Y_1, Y_2, ..., Y_n \\sim \\text{Gamma}(s + \\sum Y_i, r + n)\\]"
  },
  {
    "objectID": "slides/slides07.html#data",
    "href": "slides/slides07.html#data",
    "title": "Normal-Normal model",
    "section": "Data",
    "text": "Data\nData on heights (in cm) of 352 adults"
  },
  {
    "objectID": "slides/slides07.html#normal-model",
    "href": "slides/slides07.html#normal-model",
    "title": "Normal-Normal model",
    "section": "Normal Model",
    "text": "Normal Model\nLet \\(Y\\) be a random variable which can take any value \\(Y \\in (-\\infty,\\infty)\\) and is unimodal and symmetric. Then \\(Y\\) might be well represented by a Normal model with mean parameter \\(\\mu \\in (-\\infty, \\infty)\\) and standard deviation parameter \\(\\sigma &gt; 0\\):\n\\[Y \\sim N(\\mu, \\sigma^2)\\]\nThe Normal model is specified by continuous pdf\n\\[\\begin{equation}\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg[{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\bigg] \\;\\; \\text{ for } y \\in (-\\infty,\\infty)\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/slides07.html#section-1",
    "href": "slides/slides07.html#section-1",
    "title": "Normal-Normal model",
    "section": "",
    "text": "\\(E(Y) = \\mu, \\text{Mode}(Y) = \\mu, \\text{Var}(Y) = \\sigma^2\\)"
  },
  {
    "objectID": "slides/slides07.html#terminology",
    "href": "slides/slides07.html#terminology",
    "title": "Normal-Normal model",
    "section": "Terminology",
    "text": "Terminology\n\nNormal: “usual” or “customary”\n\nStatisticians historically called it “normal” because it was the “usual” distribution for, e.g., the CLT\n\nGaussian: name the distribution for the person who developed it\n\nAlso not quite right, since DeMoivre first proved the CLT\n\nBoth terms describe the same mathematical function"
  },
  {
    "objectID": "slides/slides07.html#normal-likelihood",
    "href": "slides/slides07.html#normal-likelihood",
    "title": "Normal-Normal model",
    "section": "Normal Likelihood",
    "text": "Normal Likelihood"
  },
  {
    "objectID": "slides/slides07.html#normal-normal-bayesian-model",
    "href": "slides/slides07.html#normal-normal-bayesian-model",
    "title": "Normal-Normal model",
    "section": "Normal-Normal Bayesian Model",
    "text": "Normal-Normal Bayesian Model\nLet $$ be an unknown mean parameter and \\((Y_1,Y_2,\\ldots,Y_n)\\) be an independent \\(N(\\mu,\\sigma^2)\\) sample where \\(\\sigma\\) is assumed to be known.\n\\[\\mu \\sim N(\\theta, \\tau^2)\\] \\[Y_i | \\mu  \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\] \\[\\mu|\\vec{y} \\; \\sim \\;  N\\bigg(\\frac{\\theta\\sigma^2/n + \\bar{y}\\tau^2}{\\tau^2+\\sigma^2/n}, \\; \\frac{\\tau^2\\sigma^2/n}{\\tau^2+\\sigma^2/n}\\bigg)\\]\n\n\nYou’ll show this on homework this week!"
  },
  {
    "objectID": "slides/slides07.html#posterior-mean-as-weighted-average",
    "href": "slides/slides07.html#posterior-mean-as-weighted-average",
    "title": "Normal-Normal model",
    "section": "Posterior Mean as weighted average",
    "text": "Posterior Mean as weighted average"
  },
  {
    "objectID": "slides/slides07.html#example",
    "href": "slides/slides07.html#example",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\n\n\nPartial census data for the Dobe area !Kung San, a foraging population\nCompiled from Nancy Howell’s interviews\n\n\n\n# A tibble: 352 × 4\n   height weight   age  male\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   152.   47.8    63     1\n 2   140.   36.5    63     0\n 3   137.   31.9    65     0\n 4   157.   53.0    41     1\n 5   145.   41.3    51     0\n 6   164.   63.0    35     1\n 7   149.   38.2    32     0\n 8   169.   55.5    27     1\n 9   148.   34.9    19     0\n10   165.   54.5    54     1\n# ℹ 342 more rows"
  },
  {
    "objectID": "slides/slides07.html#example-1",
    "href": "slides/slides07.html#example-1",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\nLet’s say we’re interested in analyzing the average height of an adult\n\n\n\n\n\n\n\n\n\n\n\nAnthropologists would be interested in more complex relationships, but we have to start somewhere."
  },
  {
    "objectID": "slides/slides07.html#example-2",
    "href": "slides/slides07.html#example-2",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nplot_normal(170, 20)"
  },
  {
    "objectID": "slides/slides07.html#example-3",
    "href": "slides/slides07.html#example-3",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nadults |&gt;\n  summarize(\n    mean = mean(height),\n    sd = sd(height),\n    n = n()\n  )\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  155.  7.74   352"
  },
  {
    "objectID": "slides/slides07.html#example-4",
    "href": "slides/slides07.html#example-4",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nplot_normal_normal(mean = 170, \n                        sd = 20, \n                        sigma = sd(adults$height), \n                        y_bar = mean(adults$height),\n                        n = length(adults$height))"
  },
  {
    "objectID": "slides/slides07.html#example-5",
    "href": "slides/slides07.html#example-5",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nsummarize_normal_normal(mean = 170, \n                        sd = 20, \n                        sigma = sd(adults$height), \n                        y_bar = mean(adults$height),\n                        n = length(adults$height))\n\n      model     mean     mode         var         sd\n1     prior 170.0000 170.0000 400.0000000 20.0000000\n2 posterior 154.6036 154.6036   0.1702222  0.4125799"
  },
  {
    "objectID": "slides/slides07.html#what-if-we-only-had-a-sample-of-35",
    "href": "slides/slides07.html#what-if-we-only-had-a-sample-of-35",
    "title": "Normal-Normal model",
    "section": "What if we only had a sample of 35?",
    "text": "What if we only had a sample of 35?"
  },
  {
    "objectID": "slides/slides07.html#what-if-we-only-had-a-sample-of-3",
    "href": "slides/slides07.html#what-if-we-only-had-a-sample-of-3",
    "title": "Normal-Normal model",
    "section": "What if we only had a sample of 3?",
    "text": "What if we only had a sample of 3?"
  },
  {
    "objectID": "slides/slides07.html#multiparameter-model",
    "href": "slides/slides07.html#multiparameter-model",
    "title": "Normal-Normal model",
    "section": "Multiparameter model",
    "text": "Multiparameter model"
  },
  {
    "objectID": "slides/test.html#quarto",
    "href": "slides/test.html#quarto",
    "title": "Untitled",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "slides/test.html#bullets",
    "href": "slides/test.html#bullets",
    "title": "Untitled",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "slides/test.html#code",
    "href": "slides/test.html#code",
    "title": "Untitled",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "homework/hw04.html",
    "href": "homework/hw04.html",
    "title": "Individual HW04",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\nlibrary(rstan) # for MCMC\nlibrary(bayesplot) # for plotting\n\nQ1\nExplain the difference between \\(N_{eff}\\) and the actual number of MCMC samples\nQ2: mcmc_rank_hist and mcmc_rank_overlay\n\nTrace plots are a natural way to view a chain, but they become hard to read when we have many samples and/or many chains. An alternative way to view the chains is called a rank histogram or trace rank plot. What this means is to (a) take all of the samples for a parameter and rank them, (b) draw a histogram of the ranks for each individual chain\nThe code chunk below loads two pre-fit stan models.\n\nload(\"../data/hw04-q2.rda\")\n\n\nCreate traceplots, rank histograms (mcmc_rank_hist), and trace rank plots (mcmc_rank_overlay) for both MCMC samples.\nWhich sample is “healthy”? Which is “unhealthy”? Explain how you can tell from the new plots\nWhich type of plot do you find more useful?\nQ3: Revisiting BR Exercise 5.11\nProf. Abebe and Prof. Morales both recently finished their PhDs and are teaching their first statistics classes at Bayesian University. Their colleagues told them that the average final exam score across all students, \\(\\mu\\), varies Normally from year to year with a mean of 80 points and a standard deviation of 4. Further, individual students’ scores \\(Y\\) vary Normally around \\(\\mu\\) with an unknown standard deviation \\(\\sigma\\).\n\nSuggest three possible prior distributions for \\(\\sigma\\) (at least two should be different named distributions). Include density plots for each prior. (You can find available probability distributions in the Stan documentation: https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html)\nBelow is starter code for running a Stan model in this scenario. Delete the #| eval: false line and fill in the blanks\n\n\nfn_model2 &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(_____, ____);\n    mu ~ normal(_____, _____);\n    sigma ~ __________;\n  }\n\"\n\n\nRun your model on the data below from Prof. Abebe and Prof. Morales’ combined scores.\n\n\nscores&lt;- c(90.3, 81.3,  85.2, 85.6, 86.4, 83.8, 86.7, 90.1, 82.3, 88.4, 89.2, 90.6,81.2, 85.9, 88.8, 82.9,84.4, 86, 78.2, 87.3, 90.8, 87, 85, 86.8, 89.8, 91.2, 84.8, 83.8, 83.3, 86.1,  92.8, 85.9,  79,74.7, 78, 79.2,  83.9, 79.8, 81.1,  87.5, 81.8,  85.6,  77,  85.2, 85.9,  83.1,92.1,  72.6, 88.3,  78, 76.5,  79.4, 82.8,  82.5, 81.4, 78.9, 75.9, 73.2,84.2, 87.6,  77.6, 84.2,  78.7,  72)\n\n\nRun thorough diagnostics on your MCMC sample. This should include: traceplots of all relevant parameters, density plots for each chain for all relevant parameters, R-hat values, and N-eff. What do you conclude about the validity of your sample?\nWhat do you conclude about the posterior mean \\(\\mu | \\vec{y}\\)? How does your answer differ from HW3’s conjugate analysis?\nRewrite the stan model code to take a flexible normal prior for \\(\\mu\\). That is, we should be able to specify the prior mean \\(\\theta\\) and prior sd \\(\\tau\\) (or prior variance \\(\\tau^2\\), whichever you prefer), instead of hard-coding them as 80 and 4.\nQ4: TBA Mon\nQ5: TBA Mon\nQ6: TBA Mon\nQ7: TBA Wed\nQ8: TBA Wed",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW04"
    ]
  },
  {
    "objectID": "homework/hw03.html",
    "href": "homework/hw03.html",
    "title": "Individual HW03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 5.1 a, d, f\nFor each situation below, tune an appropriate Gamma(s, r) prior model for \\(\\lambda\\)\n\nThe most common value of \\(\\lambda\\) is 4, and the mean is 7.\nThe most common value of \\(\\lambda\\) is 14, and the variance is 6.\nThe mean of \\(\\lambda\\) is 22, and the variance is 3.\nBR Exercise 5.7\nLet \\(\\lambda\\) be the average number of goals scored in a Women’s World Cup game. We’ll analyze \\(\\lambda\\) by the following Gamma-Poisson model: \\[Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\] \\[\\lambda \\sim \\text{Gamma}(1, 0.25)\\]\n\nPlot and summarize our prior understanding of \\(\\lambda\\)\n\nWhy is the Poisson model a reasonable choice for \\(Y_i\\)?\nThe wwc_2019_matches data in the {fivethirtyeight} package includes the number of goals scored by the two teams in each 2019 Women’s World Cup match. Define, plot, and discuss the total number of goals scored per game\n\n\nlibrary(fivethirtyeight)\ndata(\"wwc_2019_matches\")\nwwc_2019_matches &lt;- wwc_2019_matches %&gt;% \n  mutate(total_goals = score1 + score2)\n\n\nidentify the posterior model of \\(\\lambda\\) and verify your answer using summarize_gamma_poisson()\n\nPlot the prior, likelihood, and posterior of \\(\\lambda\\). Describe the evolution in your understanding of \\(\\lambda\\) from the prior to posterior.\nBR Exercise 5.15\nBelow are kernels for Normal, Poisson, Gamma, Beta, and Binomial models. Identify the appropriate model with specific parameter values.\n\n\n\\(f(\\theta) \\propto 0.3^\\theta 0.7^{16 - \\theta}\\) for \\(\\theta \\in \\{0, 1, 2 ..., 16\\}\\)\n\n\n\\(f(\\theta) \\propto 1/\\theta!\\) for \\(\\theta \\in \\{0, 1, ..., \\infty\\}\\)\n\n\n\\(f(\\theta) \\propto \\theta^4(1-\\theta)^7\\) for \\(\\theta \\in [0,1]\\)\n\n\\(f(\\theta) \\propto e^{-\\theta^2}\\)\nNormal-normal conjugacy\nVerify that the normal distribution is a conjugate prior for a normal data model.\nNote: the sketch of this proof is given in 5.3.4 and you are welcome to use it. If you use other sources (e.g. for help remembering how to complete the square), please cite them.\nBR Exercise 5.11 (Normal-normal calculation)\nProf. Abebe and Prof. Morales both recently finished their PhDs and are teaching their first statistics classes at Bayesian University. Their colleagues told them that the average final exam score across all students, \\(\\mu\\), varies Normally from year to year with a mean of 80 points and a standard deviation of 4. Further, individual students’ scores \\(Y\\) vary Normally around \\(\\mu\\) with a known standard deviation of 3 points.\n\nProf. Abebe conducts the final exam and observes that his 32 students scored an average of 86 points. Calculate the posterior mean and variance of \\(\\mu\\) using the data from Prof. Abebe’s class.\nProf. Morales conducts the final exam and observes that her 32 students scored an average of 82 points. Calculate the posterior mean and variance of \\(\\mu\\) using the data from Prof. Morales’ class.\nNext, use Prof. Abebe and Prof. Morales’ combined exams to calculate the posterior mean and variance of \\(\\mu\\)\n\nBR Exercise 5.14: Normal-normal simulation\n\nYour friend Alex has read Chapter 4 of this book, but not Chapter 5. Explain to Alex why it’s difficult to simulate a Normal-Normal posterior using the simulation methods we have learned thus far.\nTo prove your point, try (and fail) to simulate the posterior of \\(\\mu\\) for the following model upon observing a single data point \\(Y_1 = 1.1\\):\n\n\\[Y | \\mu \\sim N(\\mu, 1^2)\\] \\[\\mu \\sim N(0,1)\\]\nBR Exercise 6.6: Gamma-Poisson grid approximation\nConsider the Gamma-Poisson model for \\(\\lambda\\) with \\(\\lambda \\sim \\text{Gamma}(20, 5)\\) and \\(Y_i | \\lambda \\sim \\text{Pois}(\\lambda)\\). Suppose you observe \\(n=3\\) data points \\((Y_1, Y_2, Y_3) = (0, 1, 0)\\)\n\nUtilize grid approximation with grid values \\(\\lambda = \\{0, 1, 2, ..., 8\\}\\) to approximate the posterior of \\(\\lambda\\)\n\nRepeat part a using a grid of 201 equally spaced values between 0 and 8.\nBR Exercise 6.15: Gamma-Poisson MCMC (skip traceplots for now)\n\nSimulate the posterior model of \\(\\lambda\\) with RStan using 4 chains and 10000 iterations per chain.\nProduce density plots for all four chains.\nFrom the density plots, what seems to be the most posterior plausible value of \\(\\lambda\\)?\nHearkening back to Chapter 5, specify the theoretical posterior model of \\(\\lambda\\). How does your MCMC approximation compare? (Try adding an overlaying line to your density plot from (c))\nBR Exercise 6.9: Comparing MCMC to grid approximation\n\nWhat drawback(s) do MCMC and grid approximation share?\nWhat advantage(s) do MCMC and grid approximation share?\nWhat is an advantage of grid approximation over MCMC?\nWhat is an advantage of MCMC over grid approximation?",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW03"
    ]
  },
  {
    "objectID": "homework/hw01.html",
    "href": "homework/hw01.html",
    "title": "Individual HW01",
    "section": "",
    "text": "BR Exercise 1.5\n\n\nBR Exercise 1.6\n\n\nBR Exercise 2.2\n\n\nBR Exercise 2.6\n\n\nBR Exercise 2.16\n\n\n\n\n\n\nImportant\n\n\n\nThis problem moved to HW2!",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW01"
    ]
  },
  {
    "objectID": "computing/computing-setup.html",
    "href": "computing/computing-setup.html",
    "title": "Computing Setup for Stat340",
    "section": "",
    "text": "For Stat340, I highly recommend using R on your local computer, instead of the maize server. Why?\n\nIt’s likely the 4th time (or more!) that you’ve seen R in a classroom setting\nYou’re graduating soon and won’t be able to access maize\nBayesian statistics is much more computationally expensive than traditional statistics, and your model fitting will run faster on a local installation\n\nFirst, download the most recent versions of R and RStudio for your operating system:\n\nR (https://www.r-project.org/)\nRStudio (https://rstudio.com/products/rstudio/download/)\nQuarto (https://quarto.org/docs/get-started/)\n\nNext, we need to install a version of LaTeX in order to render quarto files to PDF. I recommend using the tinytex installation that is included with quarto. To install it, click on “Terminal” within RStudio and type the following:\nquarto install tinytex\nAfter doing all of the following, you can check that it is working correctly by:\n\nOpen rstudio\nClick “File –&gt; New File –&gt; Quarto document” and choose the “PDF” output option\nMake a small change to the file within the editor and click “render”. If all goes well, you should end up with a PDF document!\n\nFor Stat340 coursework, it will be helpful to have some additional R packages installed. You can install these all at once with:\n\ninstall.packages(c(\"bayesrules\", \"tidyverse\", \"janitor\", \"rstanarm\",\n                   \"bayesplot\", \"tidybayes\", \"broom.mixed\", \"modelr\",\n                   \"e1071\", \"forcats\"), \n                 dependencies = TRUE)",
    "crumbs": [
      "Computing",
      "Setup"
    ]
  },
  {
    "objectID": "computing/computing-access.html",
    "href": "computing/computing-access.html",
    "title": "Computing Access",
    "section": "",
    "text": "I expect you to use RStudio to run R in this course. You have two options for using RStudio:\n\nWe have a Carleton server hosting Rstudio at https://maize.mathcs.carleton.edu/. Your files on this account will be accessible as long as you are a student at Carleton. Use your Carleton credentials to access your account and you need to be running the Carleton VPN (below) to access this server. Use this option if\n\nyour personal computer is old and/or slow\nyou prefer to use school computers (lab or library computers)\n\nYou can also run R/RStudio from your personal computer. If you use a local version of R/RStudio this term, make sure that you have recently updated both R and RStudio.\nTo check your version of R, run the command getRversion() and compare your version to the newest version posted on https://cran.r-project.org/. If you need an update, then install the newer version using the installation directions above.\nIn RStudio, check for updates with the menu option Help &gt; Check for updates. Follow directions if an update is needed.\nFor a fresh download:\n\nDownload the latest version of R for your operating system from https://cran.r-project.org/\nDownload the free RStudio desktop version from https://posit.co/download/rstudio-desktop/\n\nUse the default download and install options for each. For R, download the “precompiled binary” distribution rather than the source code."
  },
  {
    "objectID": "computing/computing-access.html#rrstudio",
    "href": "computing/computing-access.html#rrstudio",
    "title": "Computing Access",
    "section": "",
    "text": "I expect you to use RStudio to run R in this course. You have two options for using RStudio:\n\nWe have a Carleton server hosting Rstudio at https://maize.mathcs.carleton.edu/. Your files on this account will be accessible as long as you are a student at Carleton. Use your Carleton credentials to access your account and you need to be running the Carleton VPN (below) to access this server. Use this option if\n\nyour personal computer is old and/or slow\nyou prefer to use school computers (lab or library computers)\n\nYou can also run R/RStudio from your personal computer. If you use a local version of R/RStudio this term, make sure that you have recently updated both R and RStudio.\nTo check your version of R, run the command getRversion() and compare your version to the newest version posted on https://cran.r-project.org/. If you need an update, then install the newer version using the installation directions above.\nIn RStudio, check for updates with the menu option Help &gt; Check for updates. Follow directions if an update is needed.\nFor a fresh download:\n\nDownload the latest version of R for your operating system from https://cran.r-project.org/\nDownload the free RStudio desktop version from https://posit.co/download/rstudio-desktop/\n\nUse the default download and install options for each. For R, download the “precompiled binary” distribution rather than the source code."
  },
  {
    "objectID": "computing/computing-access.html#vpn",
    "href": "computing/computing-access.html#vpn",
    "title": "Computing Access",
    "section": "VPN",
    "text": "VPN\nIf you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS."
  },
  {
    "objectID": "computing/computing-access.html#git-and-github",
    "href": "computing/computing-access.html#git-and-github",
    "title": "Computing Access",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nGit is version control software that you install locally on your computer. Git is already installed on the maize RStudio server.\nGithub is a cloud-based service for hosting git projects. It allows multiple users to share and contribute to projects and it is how you will be submitting homework assignments and projects for this class. More information about Github for this class is found on Moodle.\nIf you are using a local install of R/RStudio, then you will need to install Git.\nInstalling Git\nDirections for both Windows & Mac here at http://happygitwithr.com/install-git.html.\n\nIf you are using maize, then there is nothing you need to install.\nWindows users should follow Option 1 in 6.2.\nMac users can follow Option 1 in 6.3 if comfortable, otherwise follow Option 2\nLinux users can follow 6.4."
  },
  {
    "objectID": "computing/computing-access.html#latex",
    "href": "computing/computing-access.html#latex",
    "title": "Computing Access",
    "section": "LaTeX",
    "text": "LaTeX\nYou need a LaTeX compiler to create a pdf document from a R Markdown file. If you use the maize server, you don’t need to install anything (the server already has a LaTeX compiler). If you are using a local RStudio, you should install a Latex compiler.\nInstalling LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\nMacTeX for Mac (3.2GB!)\nMiKTeX for Windows (190MB)\n\n\nAcknowledgements\nThis installation guide was written by Adam Loy and is based on the guide from stat545.com and is licensed under the CC BY-NC 3.0 Creative Commons License."
  },
  {
    "objectID": "computing/r-basics-refresher.html",
    "href": "computing/r-basics-refresher.html",
    "title": "R Basics",
    "section": "",
    "text": "In your previous statistics course at Carleton, you likely loaded at least one add-on R package. In this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#loading-r-packages",
    "href": "computing/r-basics-refresher.html#loading-r-packages",
    "title": "R Basics",
    "section": "",
    "text": "In your previous statistics course at Carleton, you likely loaded at least one add-on R package. In this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#creating-and-naming-objects",
    "href": "computing/r-basics-refresher.html#creating-and-naming-objects",
    "title": "R Basics",
    "section": "Creating and naming objects",
    "text": "Creating and naming objects\nAll R statements where you create objects have the form:\n\nobject_name &lt;- value\n\nAt first, we’ll be creating a lot of data objects. For example, we an load a data set containing the ratings for each episode of The Office using the code\n\noffice_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n\nIn this class you will be creating a lot of objects, so you’ll need to come up with names for those objects. Trying to think of informative/meaningful names for objects is hard, but necessary work! Below are the fundamental rules for naming objects in R:\n\nnames can’t start with a number\nnames are case-sensitive\nsome common letters are used internally by R and should be avoided as variable names (c, q, t, C, D, F, T, I)\nThere are reserved words that R won’t let you use for variable names (for, in, while, if, else, repeat, break, next)\nR will let you use the name of a predefined function—but don’t do it!\n\nYou can always check to see if you the name you want to use is already taken via exists():\nFor example lm exists\n\nexists(\"lm\")\n\n[1] TRUE\n\n\nbut carleton_college doesn’t.\n\nexists(\"carleton_college\")\n\n[1] FALSE\n\n\nThere are also a lot of naming styles out there, and if you have coded in another language, you may have already developed a preference. Below is an illustration by Allison Horst\n\n\n\n\n\n\n\n\nI generally following the tidyverse style guide, so you’ll see that I use only lowercase letters, numbers, and _ (snake case).",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#overviews-of-data-frames",
    "href": "computing/r-basics-refresher.html#overviews-of-data-frames",
    "title": "R Basics",
    "section": "Overviews of data frames",
    "text": "Overviews of data frames\nAbove, you loaded in a data set called office_ratings. Data sets are stored as a special data structure called a data frame. Data frames are the most-commonly used data structure for data analysis in R. For now, think of them like spreadsheets.\nOnce you have your data frame, you can get a quick overview of it using a few commands (below I use data_set as a generic placeholder for the data frame’s name):\n\n\n\n\n\n\nCommand\nDescription\n\n\n\nhead(data_set)\nprint the first 6 rows\n\n\ntail(data_set)\nprint the last 6 rows\n\n\nglimpse(data_set)\na quick overview where columns run down the screen and the data values run across. This allows you to see every column in the data frame.\n\n\nstr(data_set)\na quick overview like glimpse(), but without some of the formatting\n\n\nsummary(data_set)\nquick summary statistics for each column\n\n\ndim(data_set)\nthe number of rows and columns\n\n\nnrow(data_set)\nthe number of rows\n\n\nncol(data_set)\nthe number of columns",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#tibbles",
    "href": "computing/r-basics-refresher.html#tibbles",
    "title": "R Basics",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble, or a tbl_df is another version of a data frame which is used by default in a lot of the tidyverse packages that we’ll use.\n\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\n\n\n\n\n\n\n\nNone Check point\n\n\n\nRun the above commands on the office_ratings data set. Compare and contrast the information returned by each command.\n\n\n\n\n\n\n\n\nTipGetting a spreadsheet\n\n\n\nIn RStudio, you can run the command View(data_set) to pull up a spreadsheet representation of a data frame. You can also click on the name of the data frame in the Environment pane. This can be a great way help you think about the data, and even has some interactive functions (e.g., filtering and searching); however, never include View(data_set) in an .Rmd file!!\n\n\n\n\n\n\n\n\nNoteReview from intro stats\n\n\n\nIn intro stats we used the terms cases (or observations) and variables to describe the rows and columns of a data frame, respectively.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#extracting-pieces-of-data-frames",
    "href": "computing/r-basics-refresher.html#extracting-pieces-of-data-frames",
    "title": "R Basics",
    "section": "Extracting pieces of data frames",
    "text": "Extracting pieces of data frames\nSince data frames are the fundamental data structure for most analyses in R, it’s important to know how to work with them. You already know how to get an overview of a data frame, but that isn’t always very informative. Often, you want to extract pieces of a data frame, such as a specific column or row.\nExtracting rows\nData frames can be indexed by their row/column numbers. To extract elements of a data frame, the basic syntax is data_set[row.index, column.index]. So, to extract the 10th row of office_ratings we run\n\noffice_ratings[10, ]\n\n# A tibble: 1 × 6\n  season episode title    imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      2       4 The Fire         8.4        2713 2005-10-11\n\n\nNotice that to extract an entire row, we leave the column index position blank.\nWe can also extract multiple rows by creating a vector of row indices. For example, we can extract the first 5 rows via\n\noffice_ratings[1:5, ]\n\n# A tibble: 5 × 6\n  season episode title         imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                 7.6        3706 2005-03-24\n2      1       2 Diversity Day         8.3        3566 2005-03-29\n3      1       3 Health Care           7.9        2983 2005-04-05\n4      1       4 The Alliance          8.1        2886 2005-04-12\n5      1       5 Basketball            8.4        3179 2005-04-19\n\n\nHere, 1:5 create a sequence of integers from 1 to 5.\nWe could also specify arbitrary row index values by combing the values into a vector. For example, we could extract the 1st, 13th, 64th, and 128th rows via\n\noffice_ratings[c(1, 13, 64, 128), ]\n\n# A tibble: 4 × 6\n  season episode title            imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                    7.6        3706 2005-03-24\n2      2       7 The Client               8.6        2631 2005-11-08\n3      4      13 Job Fair                 7.9        1977 2008-05-08\n4      7      11 Classy Christmas         8.9        2138 2010-12-09\n\n\nExtracting columns\nSimilar to extracting rows, we can use a numeric index to extract the columns of a data frame. For example, to extract the 3rd column, we can run\n\noffice_ratings[,3]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nAlternatively, we can pass in the column name in quotes instead of the column number\n\noffice_ratings[,\"title\"]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nNotice that the extracted column is still formatted as a data frame (or tibble). If you want to extract the contents of the column and just have a vector of titles, you have a few options.\n\nYou could use double brackets with the column number:\n\n\noffice_ratings[[3]]\n\n\nYou could use double brackets with the column name in quotes:\n\n\noffice_ratings[[\"title\"]]\n\n\nYou could use the $ extractor with the column name (not in quotes):\n\n\noffice_ratings$title\n\n\n\n\n\n\n\nNone Check point\n\n\n\n\nExtract the 35th row of office_ratings.\nExtract rows 35, 36, 37, and 38 of office_ratings.\nExtract the imdb_rating column from office ratings using the column index number.\nExtract the imdb_rating column from office ratings using the column name.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#lists",
    "href": "computing/r-basics-refresher.html#lists",
    "title": "R Basics",
    "section": "Lists",
    "text": "Lists\nIt turns out that data frames are special cases of lists, a more general data structure. In a data frame, each column is an element of the data list and each column must be of the same length. In general, lists can be comprised of elements of vastly different lengths and data types.\nAs an example, let’s construct a list of the faculty in the MAST department and what is being taught this winter.\n\nstat_faculty &lt;- c(\"Kelling\", \"Loy\", \"Luby\", \"Poppick\", \"St. Clair\", \"Wadsworth\")\nstat_courses &lt;- c(120, 220, 230, 250, 285, 330)\nmath_faculty &lt;- c(\"Brooke\", \"Davis\", \"Egge\", \"Gomez-Gonzales\", \"Haunsperger\", \"Johnson\", \n                  \"Meyer\", \"Montee\", \"Shrestha\",\"Terry\", \"Thompson\", \"Turnage-Butterbaugh\")\nmath_courses &lt;- c(101, 106, 111, 120, 210, 211, 232, 236, 240, 241, 251, 321, 333, 395)\n\nmast &lt;- list(stat_faculty = stat_faculty, stat_courses = stat_courses, \n             math_faculty = math_faculty, math_courses = math_courses)\n\nOverview of a list\nYou can get an overview of a list a few ways:\n\n\nglimpse(list_name) and str(list_name) list the elements of the list and the first few entries of each element.\n\n\nglimpse(mast)\n\nList of 4\n $ stat_faculty: chr [1:6] \"Kelling\" \"Loy\" \"Luby\" \"Poppick\" ...\n $ stat_courses: num [1:6] 120 220 230 250 285 330\n $ math_faculty: chr [1:12] \"Brooke\" \"Davis\" \"Egge\" \"Gomez-Gonzales\" ...\n $ math_courses: num [1:14] 101 106 111 120 210 211 232 236 240 241 ...\n\n\n\n\nlength(list_name) will tell you how many elements are in the list\n\n\nlength(mast)\n\n[1] 4\n\n\nExtracting elements of a list\nSince data frames are lists, you’ve already seen how to extract elements of a list. For example, to extract the stat_faculty you could run\n\nmast[[1]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\nor\n\nmast[[\"stat_faculty\"]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you had only used a single bracket above, the returned object would still be a list, which is typically not what we would want.\n\nmast[1]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\n\n\nNone Check point\n\n\n\nExtract the statistics courses offered this term.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#vectors",
    "href": "computing/r-basics-refresher.html#vectors",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\nThe columns of the office_ratings data frame and the elements of the mast list were comprised of (atomic) vectors. Unlike lists, all elements within a vector share the same type. For example, all names in the stat_faculty vector were character strings and all ratings in the imdb_rating column were numeric. We’ll deal with a variety of types of vectors in this course, including:\n\nnumeric\ncharacter (text)\nlogical (TRUE/FALSE)\n\nExtracting elements of a vector\nJust like with lists (and therefore data frames), we use brackets to extract elements from a vector. As an example, let’s work with the title column from office_ratings.\n\ntitle &lt;- office_ratings$title # vector of titles\n\nTo extract the 111th title, we run\n\ntitle[111]\n\n[1] \"New Leads\"\n\n\nor two extract the 100th through 111th titles, we run\n\ntitle[100:111]\n\n [1] \"Double Date\"          \"Murder\"               \"Shareholder Meeting\" \n [4] \"Scott's Tots\"         \"Secret Santa\"         \"The Banker\"          \n [7] \"Sabre\"                \"Manager and Salesman\" \"The Delivery: Part 1\"\n[10] \"The Delivery: Part 2\" \"St. Patrick's Day\"    \"New Leads\"           \n\n\nNegative indices\nSometimes, we want to “kick out” elements of our vector. To do this, we can use a negative index value. For example,\n\ntitle[-1]\n\nreturns all but the first title—that is, it kicks out the first title. To kick out multiple elements, we need to negate a vector of indices. For example, below we kick out the first 10 titles\n\ntitle[-c(1:10)]\n\nAnd now we kick out the 5th, 50th, and 150th titles\n\ntitle[-c(5, 50, 150)]\n\nThis idea can be adapted to lists and data frames. For example, to kick out the first row of office_ratings, we run\n\noffice_ratings[-1,]\n\n# A tibble: 187 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       2 Diversity Day             8.3        3566 2005-03-29\n 2      1       3 Health Care               7.9        2983 2005-04-05\n 3      1       4 The Alliance              8.1        2886 2005-04-12\n 4      1       5 Basketball                8.4        3179 2005-04-19\n 5      1       6 Hot Girl                  7.8        2852 2005-04-26\n 6      2       1 The Dundies               8.7        3213 2005-09-20\n 7      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 8      2       3 Office Olympics           8.4        2742 2005-10-04\n 9      2       4 The Fire                  8.4        2713 2005-10-11\n10      2       5 Halloween                 8.2        2561 2005-10-18\n# ℹ 177 more rows\n\n\nor to kick out the math courses from the mast list we run\n\nmast[-4]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n$stat_courses\n[1] 120 220 230 250 285 330\n\n$math_faculty\n [1] \"Brooke\"              \"Davis\"               \"Egge\"               \n [4] \"Gomez-Gonzales\"      \"Haunsperger\"         \"Johnson\"            \n [7] \"Meyer\"               \"Montee\"              \"Shrestha\"           \n[10] \"Terry\"               \"Thompson\"            \"Turnage-Butterbaugh\"\n\n\nLogical indices\nIt’s great to be able to extract (or omit) elements using indices, but sometimes we don’t know what index value we should use. For example, if you wanted to extract all of the 300-level statistics courses from the stat_courses vector, you would need to manually determine that positions 2:5 meet that requirement. That’s a lot of work! A better alternative is to allow R to find the elements meeting that requirement using logical operators. Below is a table summarizing common logical operators in R.\n\n\nComparison\nMeaning\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nis equal to\n\n\n!=\nnot equal to\n\n\n\nIn order to extract the 300-level statistics courses, we’ll take two steps:\n\nWe’ll determine whether each course is numbered at least 300,\nthen we’ll use that sequence of TRUEs/FALSEs to extract the course.\n\nSo, first we use the logical operator &gt;= to compare stat_courses and 300. This returns TRUE if the element meets the specification and FALSE otherwise.\n\nstat_courses &gt;= 300\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nNow, we can use this vector as our index. Only the TRUE elements will be extracted:\n\nstat_courses[stat_courses &gt;= 300]\n\n[1] 330\n\n\nThe same idea can be used with data frames and lists, just remember how to format the brackets and indices!\n\n\n\n\n\n\nNone Check point\n\n\n\n\nExtract all statistics courses below 250 from stat_courses.\nExtract all math courses except for 240 (probability) from math_courses.\nExtract all rows from season 3 of The Office.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "groupwork/gw03.html",
    "href": "groupwork/gw03.html",
    "title": "Group Work 03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\n\n\n\n\n\n\nNote\n\n\n\nNew this week! Trying no group problems on Friday\n\n\nBR Exercise 5.8 (a) and (d): Normal Likelihood Functions\nIn each situation below, we observe the outcomes for a Normal random sample, \\(Y_i | \\mu \\sim N(\\mu, \\sigma^2)\\) with known \\(\\sigma\\). Specify and plot the corresponding likelihood function of \\(\\mu\\).\n\n\n\\((y_1, y_2, y_3) = (-4.3, 0.7, -19.4)\\) and \\(\\sigma = 10\\)\n\n\n\\((y_1, y_2, y_3, y_4, y_5) = (1.6, 0.09, 1.7, 1.1, 1.1)\\) and \\(\\sigma = 0.6\\)\n\nBR Exercise 5.13: Australia\nLet \\(\\mu\\) be the average 3 p.m. temperature in Perth, Australia. Not knowing much about Australian weather, your friend’s prior understanding is that the average temperature is likely around 30 degrees Celsius, though might be anywhere between 10 and 50 degrees Celsius. To learn about \\(\\mu\\), they plan to analyze 1000 days of temperature data. Letting \\(Y_i\\) denote the 3 p.m. temperature on day \\(i\\), they’ll assume that daily temperatures vary Normally around \\(\\mu\\) with a standard deviation of 5 degrees:\n\\[Y_i | \\mu \\sim N(mu, 5^2)\\]\n\nTune and plot a Normal prior for \\(\\mu\\) that reflects your friend’s understanding.\nThe weather_perth data in the {bayesrules} package includes 1000 daily observations of 3 p.m. temperatures in Perth (temp3pm). Plot this data and discuss whether it’s reasonable to assume a Normal model for the temperature data.\nIdentify the posterior model of \\(\\mu\\) and verify your answer using summarize_normal_normal().\nPlot the prior pdf, likelihood function, and posterior pdf of \\(\\mu\\). Describe the evolution in your understanding of \\(\\mu\\) from the prior to the posterior.\nBR Exercise 5.19\nConsider the Bayesian model:\n\\[Y|\\theta \\sim \\text{Geometric}(\\theta)\\] \\[\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\]\n\nDerive the posterior model for \\(\\theta\\) given observed data \\(Y = y\\). If possible, identify the name of the posterior distribution and its parameters\nIs the Beta model a conjugate prior for the Geometric data model?\nBR Exercise 6.7\nConsider the Normal-Normal model for \\(\\mu\\) with \\(Y|\\mu \\sim N(\\mu, 1.3^2)\\) and \\(\\mu \\sim N(10, 1.2^2)\\). Suppose that you observe data \\((Y_1, Y_2, Y_3, Y_4) = (7.1, 8.9, 8.4, 8.6)\\)\n\nUtilize grid approximation with grid values \\(\\mu \\in \\{5, 6, 7, ..., 15\\}\\) to approximate the posterior of \\(\\mu\\)\n\nRepeat part (a) using a grid of 201 equally spaced values between 5 and 15\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that the likelihood has to take all 4 data points into account!\n\n\nBR 6.17 a, c, d\n\nlibrary(rstan)\nlibrary(bayesplot)\n\nUsing the same normal-normal model:\n\nSimulate the posterior of \\(\\mu\\) with {rstan} using 4 chains and 10,000 iterations per chain\nSkip b for now\nProduce a density plot for all four chains\nWhat is the actual posterior distribution of \\(\\mu |Y\\)? How does the MCMC approximation compare? How does the grid approximation compare? (Try adding an overlaying line to your density plot from (c))",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 03"
    ]
  },
  {
    "objectID": "groupwork/gw04.html",
    "href": "groupwork/gw04.html",
    "title": "Group Work 03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\n\n\n\n\n\n\nNote\n\n\n\nNo group problems from Friday",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 03"
    ]
  }
]