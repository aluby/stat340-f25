[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Stat340: Bayesian Statistics",
    "section": "",
    "text": "MEETINGS:\nMWF 3a\n11:10-12:20 MW; 12-1 F\nCMC 306\n\n\nPROFESSOR:\nAmanda Luby\naluby@carleton\nCMC 223\n\n\nDROP-IN HOURS:\nMon 12:30-1:30\nTues 2-3\nWed 3-4 (120 priority)\nFri 10-11 (340 priority)\nCMC 307\n\n\nWEBSITE:\nmoodle.carleton.edu\naluby.github.io/stat340-s25\n\n\n\nTEXT:\nBayes Rules! An Introduction to Applied Bayesian Modeling\n\n\n\nSOFTWARE\nR: free download from r-project.org/\nRStudio: free download from rstudio.com/downloads\n{rstan}: R package for MCMC sampling",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#meetings",
    "href": "course-syllabus.html#meetings",
    "title": "Stat340: Bayesian Statistics",
    "section": "Meetings",
    "text": "Meetings\nThere will be three course meetings per week (Mondays, Wednesdays, and Fridays). Daily attendance and active participation is expected. Course meetings will combine demonstrations/lecture and in-class group exercises. Each course day has an associated reading from the textbook. I expect you to complete the reading before class, and returning to the textbook afterwards if you have any lingering confusion about the material from the day.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#individual-assignments",
    "href": "course-syllabus.html#individual-assignments",
    "title": "Stat340: Bayesian Statistics",
    "section": "Individual Assignments",
    "text": "Individual Assignments\nIndividual homework will be assigned once-ish per week, typically due on Fridays at the beginning of class. You will submit homework assignments via gradescope. You will use {quarto} for all assignments. Individual assignments will be graded on completion.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#group-assignments",
    "href": "course-syllabus.html#group-assignments",
    "title": "Stat340: Bayesian Statistics",
    "section": "Group Assignments",
    "text": "Group Assignments\nEach course day, you’ll be assigned a few problems as “group homework”. On the day homework is due (typically Fridays), you’ll submit solutions to these problems as a group via gradescope. Group assignments will be graded on correctness and completion.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#homework-quizzes",
    "href": "course-syllabus.html#homework-quizzes",
    "title": "Stat340: Bayesian Statistics",
    "section": "Homework quizzes",
    "text": "Homework quizzes\nOn days that homework is due, we will begin class with a short “homework quiz”. I will ask you 1-2 questions that are based entirely on the homework you just submitted. These will be graded on correctness, and your lowest score will be dropped.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#exams",
    "href": "course-syllabus.html#exams",
    "title": "Stat340: Bayesian Statistics",
    "section": "Exams",
    "text": "Exams\nThere will be two in-class exams, tentatively scheduled for Fridays of Week 4 and 8.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#projects",
    "href": "course-syllabus.html#projects",
    "title": "Stat340: Bayesian Statistics",
    "section": "Projects",
    "text": "Projects\nThere will be two projects (one midterm project and one final project). You will work in groups of 2-3 for each project.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#regrade-requests",
    "href": "course-syllabus.html#regrade-requests",
    "title": "Stat340: Bayesian Statistics",
    "section": "Regrade requests",
    "text": "Regrade requests\nGrading is often a tedious task, and the grading team will sometimes make mistakes. I am always happy to fix these mistakes, and gradescope makes it easy to do so. Regrade requests must be submitted on gradescope within two weekdays after an assignment or exam has been returned to you. Regrade requests are for administrative errors or obvious grading mistakes. I will not consider regrade requests for anything that applied to the entire class (e.g. “I think this mistake should only be worth 1 point instead of 2” or “I didn’t realize we had to do X”). If you submit two or more inappropriate regrade requests, I will not consider additional regrade requests from you for the remainder of the term. If you’re unsure whether you should file a regrade request or not, just ask! You are always welcome to discuss any grading questions with me in office hours.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#exam-revisions",
    "href": "course-syllabus.html#exam-revisions",
    "title": "Stat340: Bayesian Statistics",
    "section": "Exam revisions",
    "text": "Exam revisions\nI offer the opportunity to submit exam revisions for two reasons: (1) to correct any major misunderstandings and have a chance to revisit material that you struggled with, (2) to mitigate the impact that a bad exam day can have on your final course grade. You can submit corrections to your exam to earn back 50% of your missed points. Corrected exam grades are capped at 80%. Exam revisions are due 1 week after graded exams are returned to you, or the last day of class at 11:59pm, whichever is earlier.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#missing-homework-quizzes-and-exams",
    "href": "course-syllabus.html#missing-homework-quizzes-and-exams",
    "title": "Stat340: Bayesian Statistics",
    "section": "Missing homework quizzes and exams",
    "text": "Missing homework quizzes and exams\nIf you miss a class when there is an exam or homework quiz and don’t make arrangements with me at least a week in advance, then there are no makeups offered. If must miss an exam due to an illness or other last minute emergency, please let me know in advance to arrange an alternative. There are no makeup homework quizzes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbook",
    "href": "course-syllabus.html#textbook",
    "title": "Stat340: Bayesian Statistics",
    "section": "Textbook",
    "text": "Textbook\nOur textbook is freely available online:\n\nBayes Rules! An Introduction to Applied Bayesian Mdoeling https://www.bayesrulesbook.com/\n\nIf you prefer a hard copy, they are also available for purchase through the bookstore and/or publisher.\nI may also periodically assign or recommend readings from other sources. These will either be freely available online or I will provide a PDF. If you require readings in another format, please let me know during the first week of class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#software",
    "href": "course-syllabus.html#software",
    "title": "Stat340: Bayesian Statistics",
    "section": "Software",
    "text": "Software\nThe use of the R programming language, with the RStudio interface is an essential component of this course. Bayesian Statistics is more computationally intense than traditional frequentist methods, and so you will need to install local versions of the software on your own computer, or plan to complete assignments on a lab computer.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#communication",
    "href": "course-syllabus.html#communication",
    "title": "Stat340: Bayesian Statistics",
    "section": "Communication",
    "text": "Communication\nAssignments and slides will be shared publicly on our course website. Grades will be posted on gradescope and/or Moodle. Please use our slack workspace for any homework or course content questions; email me privately with any personal matters (grade discussions, illness, emergency, etc.). Any time-sensitive announcements will be sent via email. It is your responsibility to make sure that your notification settings allow time-sensitive announcements to reach you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#professor-availability",
    "href": "course-syllabus.html#professor-availability",
    "title": "Stat340: Bayesian Statistics",
    "section": "Professor availability",
    "text": "Professor availability\nPlease reach out to me if you have any questions! You can reach me on slack, email, drop-in hours, or make an appointment. Slack is typically the best venue for homework/content questions outside of drop-in hours. I’ll respond to slack messages at least 3 times each week day, and try to respond to emails within 48 hours You will receive responses faster on slack than email. I am online irregularly over the weekend to devote time to family and rest, and I hope that you also find time over the weekend to recharge and reset!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accomodations",
    "href": "course-syllabus.html#accomodations",
    "title": "Stat340: Bayesian Statistics",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#title-ix",
    "href": "course-syllabus.html#title-ix",
    "title": "Stat340: Bayesian Statistics",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#take-care-of-yourself",
    "href": "course-syllabus.html#take-care-of-yourself",
    "title": "Stat340: Bayesian Statistics",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this term by wearing a mask when you’re sick, eating a vegetable every now and then, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your physical and mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. For more information, see Student Health and Counseling (SHAC), the Office of Health Promotion, or the Office of the Chaplain. If you are experiencing physical or mental health symptoms as a result of coursework, please speak with me so we can address the problem together.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "groupwork/gw04.html",
    "href": "groupwork/gw04.html",
    "title": "Group Work 03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\n\n\n\n\n\n\nNote\n\n\n\nNo group problems from Friday\n\n\nBR Exercise 7.6: Proposing a new location\nIn each situation below, complete Step 1 of the Metropolis-Hastings algorithm. That is, starting from teh given current chain value \\(\\lambda^{(i)} = \\lambda\\) and with set.seed(84735), use the given proposal model to draw a \\(\\lambda'\\) proposal value for the next value in the chain \\(\\lambda^{(i+1)}\\)\n\n\\(\\lambda = 4.6, \\lambda' | \\lambda \\sim N(\\lambda, 2^2)\\)\n\\(\\lambda = 2.1, \\lambda' | \\lambda \\sim N(\\lambda, 7^2)\\)\n\\(\\lambda = 8.9, \\lambda' | \\lambda \\sim Unif(\\lambda - 2, \\lambda +2)\\)\n\\(\\lambda = 1.2, \\lambda' | \\lambda \\sim Unif(\\lambda - 0.5, \\lambda + 0.5)\\)\n\\(\\lambda = 7.7, \\lambda' | \\lambda \\sim Unif(\\lambda - 3, \\lambda +3)\\)\nDiscuss why these proposal models “make sense”\nBR Exercise 7.7: Calculate the acceptance probability\nSuppose that a Markov chain is currently at \\(\\lambda^{(i)}=2\\) and the proposal for \\(\\lambda^{(i+1)}\\) is \\(\\lambda' = 2.1\\). For each pair of unnnormalized pdf \\(f(\\lambda) L(\\lambda | y)\\) and proposal \\(q(\\lambda' | \\lambda)\\), calculate the acceptance probability \\(\\alpha\\) used in Step 2 of the Metropolis-Hastings algorithm.\n\n\n\\(f(\\lambda) L(\\lambda | y) = \\lambda^{-1}, \\lambda' | \\lambda \\sim N(\\lambda, 2^2)\\) with pdf \\(q(\\lambda'|\\lambda)\\)\n\n\n\\(f(\\lambda) L(\\lambda | y) = e^{3\\lambda}, \\lambda' | \\lambda \\sim N(\\lambda, .5^2)\\) with pdf \\(q(\\lambda'|\\lambda)\\)\n\n\n\\(f(\\lambda) L(\\lambda | y) = e^{-1.9\\lambda}, \\lambda' | \\lambda \\sim Unif(\\lambda - .3, \\lambda + .3)\\) with pdf \\(q(\\lambda'|\\lambda)\\)\n\n\n\\(f(\\lambda) L(\\lambda | y) =e^{-\\lambda^4}, \\lambda' | \\lambda \\sim Exp(\\lambda)\\) with pdf \\(q(\\lambda'|\\lambda)\\)\n\nFor which of these scenarios is there a 100% acceptance probability? Explain why we’d certainly want to accept \\(\\lambda\\) in these scenarios\nBR 7.14\nConsider a Gamma-Poisson model in which rate \\(\\lambda\\) has a Gamma(1,0.1) prior and you observe one Poisson data point, \\(Y=4\\). In this exercise, you will simulate the posterior model of \\(\\lambda\\) using an independence sampler.\n\nWhich of the following would make the most reasonable proposal model to use in your independence sampler for \\(\\lambda\\): Normal, Beta, or Exponential? Hint: Recall that \\(\\lambda &gt; 0\\).\nUsing the proposal model that you identified in part a, simulate a tour of 1000 \\(\\lambda\\) values. Tune your algorithm until you are satisfied and produce a trace plot of your final results.\nThis is a situation where we can derive the exact posterior model of \\(\\lambda\\). What is it?\nPlot a histogram of your tour and overlay the exact model. How good is your Markov chain approximation?\nIndependence Sampler for truncated normal prior\nMetropolis-Hastings and Independence Samplers are powerful when we don’t have a closed-form solution. Suppose we instead use a truncated normal model for \\(\\lambda\\). This pdf looks like a normal distribution, but is truncated at lower and/or upper bounds. The code chunk below loads the {truncnorm} package, which gives you access to dtruncnorm, rtruncnorm, etc, and plots a truncated normal with parameters \\(\\mu=0\\), \\(\\sigma=1\\) and \\(a=0\\).\n\nTest different values of a, mean, and sd to get a sense of what each parameter does. Provide explanations of each.\nTune the truncated normal prior to cover a similar range as the Gamma distribution above\nModify your independence sampler from the previous question to use a truncated normal prior distribution. Tune your algorithm until you are satisfied and produce a trace plot of your final results.\nCompare your posterior to the posterior from the last question.\n\n\nlibrary(truncnorm)\n\nggplot(data = data.frame(x = c(-4, 4)), aes(x = x)) +\n  stat_function(fun = dtruncnorm, \n                args = list(a = 0, mean = 0, sd = 1))",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 03"
    ]
  },
  {
    "objectID": "groupwork/gw03.html",
    "href": "groupwork/gw03.html",
    "title": "Group Work 03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\n\n\n\n\n\n\nNote\n\n\n\nNew this week! Trying no group problems on Friday\n\n\nBR Exercise 5.8 (a) and (d): Normal Likelihood Functions\nIn each situation below, we observe the outcomes for a Normal random sample, \\(Y_i | \\mu \\sim N(\\mu, \\sigma^2)\\) with known \\(\\sigma\\). Specify and plot the corresponding likelihood function of \\(\\mu\\).\n\n\n\\((y_1, y_2, y_3) = (-4.3, 0.7, -19.4)\\) and \\(\\sigma = 10\\)\n\n\n\\((y_1, y_2, y_3, y_4, y_5) = (1.6, 0.09, 1.7, 1.1, 1.1)\\) and \\(\\sigma = 0.6\\)\n\nBR Exercise 5.13: Australia\nLet \\(\\mu\\) be the average 3 p.m. temperature in Perth, Australia. Not knowing much about Australian weather, your friend’s prior understanding is that the average temperature is likely around 30 degrees Celsius, though might be anywhere between 10 and 50 degrees Celsius. To learn about \\(\\mu\\), they plan to analyze 1000 days of temperature data. Letting \\(Y_i\\) denote the 3 p.m. temperature on day \\(i\\), they’ll assume that daily temperatures vary Normally around \\(\\mu\\) with a standard deviation of 5 degrees:\n\\[Y_i | \\mu \\sim N(mu, 5^2)\\]\n\nTune and plot a Normal prior for \\(\\mu\\) that reflects your friend’s understanding.\nThe weather_perth data in the {bayesrules} package includes 1000 daily observations of 3 p.m. temperatures in Perth (temp3pm). Plot this data and discuss whether it’s reasonable to assume a Normal model for the temperature data.\nIdentify the posterior model of \\(\\mu\\) and verify your answer using summarize_normal_normal().\nPlot the prior pdf, likelihood function, and posterior pdf of \\(\\mu\\). Describe the evolution in your understanding of \\(\\mu\\) from the prior to the posterior.\nBR Exercise 5.19\nConsider the Bayesian model:\n\\[Y|\\theta \\sim \\text{Geometric}(\\theta)\\] \\[\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\]\n\nDerive the posterior model for \\(\\theta\\) given observed data \\(Y = y\\). If possible, identify the name of the posterior distribution and its parameters\nIs the Beta model a conjugate prior for the Geometric data model?\nBR Exercise 6.7\nConsider the Normal-Normal model for \\(\\mu\\) with \\(Y|\\mu \\sim N(\\mu, 1.3^2)\\) and \\(\\mu \\sim N(10, 1.2^2)\\). Suppose that you observe data \\((Y_1, Y_2, Y_3, Y_4) = (7.1, 8.9, 8.4, 8.6)\\)\n\nUtilize grid approximation with grid values \\(\\mu \\in \\{5, 6, 7, ..., 15\\}\\) to approximate the posterior of \\(\\mu\\)\n\nRepeat part (a) using a grid of 201 equally spaced values between 5 and 15\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that the likelihood has to take all 4 data points into account!\n\n\nBR 6.17 a, c, d\n\nlibrary(rstan)\nlibrary(bayesplot)\n\nUsing the same normal-normal model:\n\nSimulate the posterior of \\(\\mu\\) with {rstan} using 4 chains and 10,000 iterations per chain\nSkip b for now\nProduce a density plot for all four chains\nWhat is the actual posterior distribution of \\(\\mu |Y\\)? How does the MCMC approximation compare? How does the grid approximation compare? (Try adding an overlaying line to your density plot from (c))",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 03"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html",
    "href": "computing/r-basics-refresher.html",
    "title": "R Basics",
    "section": "",
    "text": "In your previous statistics course at Carleton, you likely loaded at least one add-on R package. In this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#loading-r-packages",
    "href": "computing/r-basics-refresher.html#loading-r-packages",
    "title": "R Basics",
    "section": "",
    "text": "In your previous statistics course at Carleton, you likely loaded at least one add-on R package. In this course, we’ll use a lot of tools found in the tidyverse of R packages. To load many of these packages at once, you can use the library(&lt;package_name&gt;) command. So to load the tidyverse we run:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we see a lot of extra info printed when we load the tidyverse. These messages are just telling you what packages are now available to you and warning you that a few functions (e.g., filter) has been replaced by the tidyverse version. We’ll see how to suppress these messages later.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#creating-and-naming-objects",
    "href": "computing/r-basics-refresher.html#creating-and-naming-objects",
    "title": "R Basics",
    "section": "Creating and naming objects",
    "text": "Creating and naming objects\nAll R statements where you create objects have the form:\n\nobject_name &lt;- value\n\nAt first, we’ll be creating a lot of data objects. For example, we an load a data set containing the ratings for each episode of The Office using the code\n\noffice_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n\nIn this class you will be creating a lot of objects, so you’ll need to come up with names for those objects. Trying to think of informative/meaningful names for objects is hard, but necessary work! Below are the fundamental rules for naming objects in R:\n\nnames can’t start with a number\nnames are case-sensitive\nsome common letters are used internally by R and should be avoided as variable names (c, q, t, C, D, F, T, I)\nThere are reserved words that R won’t let you use for variable names (for, in, while, if, else, repeat, break, next)\nR will let you use the name of a predefined function—but don’t do it!\n\nYou can always check to see if you the name you want to use is already taken via exists():\nFor example lm exists\n\nexists(\"lm\")\n\n[1] TRUE\n\n\nbut carleton_college doesn’t.\n\nexists(\"carleton_college\")\n\n[1] FALSE\n\n\nThere are also a lot of naming styles out there, and if you have coded in another language, you may have already developed a preference. Below is an illustration by Allison Horst\n\n\n\n\n\n\n\n\nI generally following the tidyverse style guide, so you’ll see that I use only lowercase letters, numbers, and _ (snake case).",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#overviews-of-data-frames",
    "href": "computing/r-basics-refresher.html#overviews-of-data-frames",
    "title": "R Basics",
    "section": "Overviews of data frames",
    "text": "Overviews of data frames\nAbove, you loaded in a data set called office_ratings. Data sets are stored as a special data structure called a data frame. Data frames are the most-commonly used data structure for data analysis in R. For now, think of them like spreadsheets.\nOnce you have your data frame, you can get a quick overview of it using a few commands (below I use data_set as a generic placeholder for the data frame’s name):\n\n\n\n\n\n\nCommand\nDescription\n\n\n\nhead(data_set)\nprint the first 6 rows\n\n\ntail(data_set)\nprint the last 6 rows\n\n\nglimpse(data_set)\na quick overview where columns run down the screen and the data values run across. This allows you to see every column in the data frame.\n\n\nstr(data_set)\na quick overview like glimpse(), but without some of the formatting\n\n\nsummary(data_set)\nquick summary statistics for each column\n\n\ndim(data_set)\nthe number of rows and columns\n\n\nnrow(data_set)\nthe number of rows\n\n\nncol(data_set)\nthe number of columns",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#tibbles",
    "href": "computing/r-basics-refresher.html#tibbles",
    "title": "R Basics",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble, or a tbl_df is another version of a data frame which is used by default in a lot of the tidyverse packages that we’ll use.\n\nTibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.\n\n\n\n\n\n\n\nNone Check point\n\n\n\nRun the above commands on the office_ratings data set. Compare and contrast the information returned by each command.\n\n\n\n\n\n\n\n\nTipGetting a spreadsheet\n\n\n\nIn RStudio, you can run the command View(data_set) to pull up a spreadsheet representation of a data frame. You can also click on the name of the data frame in the Environment pane. This can be a great way help you think about the data, and even has some interactive functions (e.g., filtering and searching); however, never include View(data_set) in an .Rmd file!!\n\n\n\n\n\n\n\n\nNoteReview from intro stats\n\n\n\nIn intro stats we used the terms cases (or observations) and variables to describe the rows and columns of a data frame, respectively.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#extracting-pieces-of-data-frames",
    "href": "computing/r-basics-refresher.html#extracting-pieces-of-data-frames",
    "title": "R Basics",
    "section": "Extracting pieces of data frames",
    "text": "Extracting pieces of data frames\nSince data frames are the fundamental data structure for most analyses in R, it’s important to know how to work with them. You already know how to get an overview of a data frame, but that isn’t always very informative. Often, you want to extract pieces of a data frame, such as a specific column or row.\nExtracting rows\nData frames can be indexed by their row/column numbers. To extract elements of a data frame, the basic syntax is data_set[row.index, column.index]. So, to extract the 10th row of office_ratings we run\n\noffice_ratings[10, ]\n\n# A tibble: 1 × 6\n  season episode title    imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      2       4 The Fire         8.4        2713 2005-10-11\n\n\nNotice that to extract an entire row, we leave the column index position blank.\nWe can also extract multiple rows by creating a vector of row indices. For example, we can extract the first 5 rows via\n\noffice_ratings[1:5, ]\n\n# A tibble: 5 × 6\n  season episode title         imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                 7.6        3706 2005-03-24\n2      1       2 Diversity Day         8.3        3566 2005-03-29\n3      1       3 Health Care           7.9        2983 2005-04-05\n4      1       4 The Alliance          8.1        2886 2005-04-12\n5      1       5 Basketball            8.4        3179 2005-04-19\n\n\nHere, 1:5 create a sequence of integers from 1 to 5.\nWe could also specify arbitrary row index values by combing the values into a vector. For example, we could extract the 1st, 13th, 64th, and 128th rows via\n\noffice_ratings[c(1, 13, 64, 128), ]\n\n# A tibble: 4 × 6\n  season episode title            imdb_rating total_votes air_date  \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n1      1       1 Pilot                    7.6        3706 2005-03-24\n2      2       7 The Client               8.6        2631 2005-11-08\n3      4      13 Job Fair                 7.9        1977 2008-05-08\n4      7      11 Classy Christmas         8.9        2138 2010-12-09\n\n\nExtracting columns\nSimilar to extracting rows, we can use a numeric index to extract the columns of a data frame. For example, to extract the 3rd column, we can run\n\noffice_ratings[,3]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nAlternatively, we can pass in the column name in quotes instead of the column number\n\noffice_ratings[,\"title\"]\n\n# A tibble: 188 × 1\n   title            \n   &lt;chr&gt;            \n 1 Pilot            \n 2 Diversity Day    \n 3 Health Care      \n 4 The Alliance     \n 5 Basketball       \n 6 Hot Girl         \n 7 The Dundies      \n 8 Sexual Harassment\n 9 Office Olympics  \n10 The Fire         \n# ℹ 178 more rows\n\n\nNotice that the extracted column is still formatted as a data frame (or tibble). If you want to extract the contents of the column and just have a vector of titles, you have a few options.\n\nYou could use double brackets with the column number:\n\n\noffice_ratings[[3]]\n\n\nYou could use double brackets with the column name in quotes:\n\n\noffice_ratings[[\"title\"]]\n\n\nYou could use the $ extractor with the column name (not in quotes):\n\n\noffice_ratings$title\n\n\n\n\n\n\n\nNone Check point\n\n\n\n\nExtract the 35th row of office_ratings.\nExtract rows 35, 36, 37, and 38 of office_ratings.\nExtract the imdb_rating column from office ratings using the column index number.\nExtract the imdb_rating column from office ratings using the column name.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#lists",
    "href": "computing/r-basics-refresher.html#lists",
    "title": "R Basics",
    "section": "Lists",
    "text": "Lists\nIt turns out that data frames are special cases of lists, a more general data structure. In a data frame, each column is an element of the data list and each column must be of the same length. In general, lists can be comprised of elements of vastly different lengths and data types.\nAs an example, let’s construct a list of the faculty in the MAST department and what is being taught this winter.\n\nstat_faculty &lt;- c(\"Kelling\", \"Loy\", \"Luby\", \"Poppick\", \"St. Clair\", \"Wadsworth\")\nstat_courses &lt;- c(120, 220, 230, 250, 285, 330)\nmath_faculty &lt;- c(\"Brooke\", \"Davis\", \"Egge\", \"Gomez-Gonzales\", \"Haunsperger\", \"Johnson\", \n                  \"Meyer\", \"Montee\", \"Shrestha\",\"Terry\", \"Thompson\", \"Turnage-Butterbaugh\")\nmath_courses &lt;- c(101, 106, 111, 120, 210, 211, 232, 236, 240, 241, 251, 321, 333, 395)\n\nmast &lt;- list(stat_faculty = stat_faculty, stat_courses = stat_courses, \n             math_faculty = math_faculty, math_courses = math_courses)\n\nOverview of a list\nYou can get an overview of a list a few ways:\n\n\nglimpse(list_name) and str(list_name) list the elements of the list and the first few entries of each element.\n\n\nglimpse(mast)\n\nList of 4\n $ stat_faculty: chr [1:6] \"Kelling\" \"Loy\" \"Luby\" \"Poppick\" ...\n $ stat_courses: num [1:6] 120 220 230 250 285 330\n $ math_faculty: chr [1:12] \"Brooke\" \"Davis\" \"Egge\" \"Gomez-Gonzales\" ...\n $ math_courses: num [1:14] 101 106 111 120 210 211 232 236 240 241 ...\n\n\n\n\nlength(list_name) will tell you how many elements are in the list\n\n\nlength(mast)\n\n[1] 4\n\n\nExtracting elements of a list\nSince data frames are lists, you’ve already seen how to extract elements of a list. For example, to extract the stat_faculty you could run\n\nmast[[1]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\nor\n\nmast[[\"stat_faculty\"]]\n\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you had only used a single bracket above, the returned object would still be a list, which is typically not what we would want.\n\nmast[1]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n\n\n\n\n\n\n\n\n\nNone Check point\n\n\n\nExtract the statistics courses offered this term.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/r-basics-refresher.html#vectors",
    "href": "computing/r-basics-refresher.html#vectors",
    "title": "R Basics",
    "section": "Vectors",
    "text": "Vectors\nThe columns of the office_ratings data frame and the elements of the mast list were comprised of (atomic) vectors. Unlike lists, all elements within a vector share the same type. For example, all names in the stat_faculty vector were character strings and all ratings in the imdb_rating column were numeric. We’ll deal with a variety of types of vectors in this course, including:\n\nnumeric\ncharacter (text)\nlogical (TRUE/FALSE)\n\nExtracting elements of a vector\nJust like with lists (and therefore data frames), we use brackets to extract elements from a vector. As an example, let’s work with the title column from office_ratings.\n\ntitle &lt;- office_ratings$title # vector of titles\n\nTo extract the 111th title, we run\n\ntitle[111]\n\n[1] \"New Leads\"\n\n\nor two extract the 100th through 111th titles, we run\n\ntitle[100:111]\n\n [1] \"Double Date\"          \"Murder\"               \"Shareholder Meeting\" \n [4] \"Scott's Tots\"         \"Secret Santa\"         \"The Banker\"          \n [7] \"Sabre\"                \"Manager and Salesman\" \"The Delivery: Part 1\"\n[10] \"The Delivery: Part 2\" \"St. Patrick's Day\"    \"New Leads\"           \n\n\nNegative indices\nSometimes, we want to “kick out” elements of our vector. To do this, we can use a negative index value. For example,\n\ntitle[-1]\n\nreturns all but the first title—that is, it kicks out the first title. To kick out multiple elements, we need to negate a vector of indices. For example, below we kick out the first 10 titles\n\ntitle[-c(1:10)]\n\nAnd now we kick out the 5th, 50th, and 150th titles\n\ntitle[-c(5, 50, 150)]\n\nThis idea can be adapted to lists and data frames. For example, to kick out the first row of office_ratings, we run\n\noffice_ratings[-1,]\n\n# A tibble: 187 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       2 Diversity Day             8.3        3566 2005-03-29\n 2      1       3 Health Care               7.9        2983 2005-04-05\n 3      1       4 The Alliance              8.1        2886 2005-04-12\n 4      1       5 Basketball                8.4        3179 2005-04-19\n 5      1       6 Hot Girl                  7.8        2852 2005-04-26\n 6      2       1 The Dundies               8.7        3213 2005-09-20\n 7      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 8      2       3 Office Olympics           8.4        2742 2005-10-04\n 9      2       4 The Fire                  8.4        2713 2005-10-11\n10      2       5 Halloween                 8.2        2561 2005-10-18\n# ℹ 177 more rows\n\n\nor to kick out the math courses from the mast list we run\n\nmast[-4]\n\n$stat_faculty\n[1] \"Kelling\"   \"Loy\"       \"Luby\"      \"Poppick\"   \"St. Clair\" \"Wadsworth\"\n\n$stat_courses\n[1] 120 220 230 250 285 330\n\n$math_faculty\n [1] \"Brooke\"              \"Davis\"               \"Egge\"               \n [4] \"Gomez-Gonzales\"      \"Haunsperger\"         \"Johnson\"            \n [7] \"Meyer\"               \"Montee\"              \"Shrestha\"           \n[10] \"Terry\"               \"Thompson\"            \"Turnage-Butterbaugh\"\n\n\nLogical indices\nIt’s great to be able to extract (or omit) elements using indices, but sometimes we don’t know what index value we should use. For example, if you wanted to extract all of the 300-level statistics courses from the stat_courses vector, you would need to manually determine that positions 2:5 meet that requirement. That’s a lot of work! A better alternative is to allow R to find the elements meeting that requirement using logical operators. Below is a table summarizing common logical operators in R.\n\n\nComparison\nMeaning\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nis equal to\n\n\n!=\nnot equal to\n\n\n\nIn order to extract the 300-level statistics courses, we’ll take two steps:\n\nWe’ll determine whether each course is numbered at least 300,\nthen we’ll use that sequence of TRUEs/FALSEs to extract the course.\n\nSo, first we use the logical operator &gt;= to compare stat_courses and 300. This returns TRUE if the element meets the specification and FALSE otherwise.\n\nstat_courses &gt;= 300\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nNow, we can use this vector as our index. Only the TRUE elements will be extracted:\n\nstat_courses[stat_courses &gt;= 300]\n\n[1] 330\n\n\nThe same idea can be used with data frames and lists, just remember how to format the brackets and indices!\n\n\n\n\n\n\nNone Check point\n\n\n\n\nExtract all statistics courses below 250 from stat_courses.\nExtract all math courses except for 240 (probability) from math_courses.\nExtract all rows from season 3 of The Office.",
    "crumbs": [
      "Computing",
      "R Refresher"
    ]
  },
  {
    "objectID": "computing/computing-access.html",
    "href": "computing/computing-access.html",
    "title": "Computing Access",
    "section": "",
    "text": "I expect you to use RStudio to run R in this course. You have two options for using RStudio:\n\nWe have a Carleton server hosting Rstudio at https://maize.mathcs.carleton.edu/. Your files on this account will be accessible as long as you are a student at Carleton. Use your Carleton credentials to access your account and you need to be running the Carleton VPN (below) to access this server. Use this option if\n\nyour personal computer is old and/or slow\nyou prefer to use school computers (lab or library computers)\n\nYou can also run R/RStudio from your personal computer. If you use a local version of R/RStudio this term, make sure that you have recently updated both R and RStudio.\nTo check your version of R, run the command getRversion() and compare your version to the newest version posted on https://cran.r-project.org/. If you need an update, then install the newer version using the installation directions above.\nIn RStudio, check for updates with the menu option Help &gt; Check for updates. Follow directions if an update is needed.\nFor a fresh download:\n\nDownload the latest version of R for your operating system from https://cran.r-project.org/\nDownload the free RStudio desktop version from https://posit.co/download/rstudio-desktop/\n\nUse the default download and install options for each. For R, download the “precompiled binary” distribution rather than the source code."
  },
  {
    "objectID": "computing/computing-access.html#rrstudio",
    "href": "computing/computing-access.html#rrstudio",
    "title": "Computing Access",
    "section": "",
    "text": "I expect you to use RStudio to run R in this course. You have two options for using RStudio:\n\nWe have a Carleton server hosting Rstudio at https://maize.mathcs.carleton.edu/. Your files on this account will be accessible as long as you are a student at Carleton. Use your Carleton credentials to access your account and you need to be running the Carleton VPN (below) to access this server. Use this option if\n\nyour personal computer is old and/or slow\nyou prefer to use school computers (lab or library computers)\n\nYou can also run R/RStudio from your personal computer. If you use a local version of R/RStudio this term, make sure that you have recently updated both R and RStudio.\nTo check your version of R, run the command getRversion() and compare your version to the newest version posted on https://cran.r-project.org/. If you need an update, then install the newer version using the installation directions above.\nIn RStudio, check for updates with the menu option Help &gt; Check for updates. Follow directions if an update is needed.\nFor a fresh download:\n\nDownload the latest version of R for your operating system from https://cran.r-project.org/\nDownload the free RStudio desktop version from https://posit.co/download/rstudio-desktop/\n\nUse the default download and install options for each. For R, download the “precompiled binary” distribution rather than the source code."
  },
  {
    "objectID": "computing/computing-access.html#vpn",
    "href": "computing/computing-access.html#vpn",
    "title": "Computing Access",
    "section": "VPN",
    "text": "VPN\nIf you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS."
  },
  {
    "objectID": "computing/computing-access.html#git-and-github",
    "href": "computing/computing-access.html#git-and-github",
    "title": "Computing Access",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nGit is version control software that you install locally on your computer. Git is already installed on the maize RStudio server.\nGithub is a cloud-based service for hosting git projects. It allows multiple users to share and contribute to projects and it is how you will be submitting homework assignments and projects for this class. More information about Github for this class is found on Moodle.\nIf you are using a local install of R/RStudio, then you will need to install Git.\nInstalling Git\nDirections for both Windows & Mac here at http://happygitwithr.com/install-git.html.\n\nIf you are using maize, then there is nothing you need to install.\nWindows users should follow Option 1 in 6.2.\nMac users can follow Option 1 in 6.3 if comfortable, otherwise follow Option 2\nLinux users can follow 6.4."
  },
  {
    "objectID": "computing/computing-access.html#latex",
    "href": "computing/computing-access.html#latex",
    "title": "Computing Access",
    "section": "LaTeX",
    "text": "LaTeX\nYou need a LaTeX compiler to create a pdf document from a R Markdown file. If you use the maize server, you don’t need to install anything (the server already has a LaTeX compiler). If you are using a local RStudio, you should install a Latex compiler.\nInstalling LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\nMacTeX for Mac (3.2GB!)\nMiKTeX for Windows (190MB)\n\n\nAcknowledgements\nThis installation guide was written by Adam Loy and is based on the guide from stat545.com and is licensed under the CC BY-NC 3.0 Creative Commons License."
  },
  {
    "objectID": "computing/computing-setup.html",
    "href": "computing/computing-setup.html",
    "title": "Computing Setup for Stat340",
    "section": "",
    "text": "For Stat340, I highly recommend using R on your local computer, instead of the maize server. Why?\n\nIt’s likely the 4th time (or more!) that you’ve seen R in a classroom setting\nYou’re graduating soon and won’t be able to access maize\nBayesian statistics is much more computationally expensive than traditional statistics, and your model fitting will run faster on a local installation\n\nFirst, download the most recent versions of R and RStudio for your operating system:\n\nR (https://www.r-project.org/)\nRStudio (https://rstudio.com/products/rstudio/download/)\nQuarto (https://quarto.org/docs/get-started/)\n\nNext, we need to install a version of LaTeX in order to render quarto files to PDF. I recommend using the tinytex installation that is included with quarto. To install it, click on “Terminal” within RStudio and type the following:\nquarto install tinytex\nAfter doing all of the following, you can check that it is working correctly by:\n\nOpen rstudio\nClick “File –&gt; New File –&gt; Quarto document” and choose the “PDF” output option\nMake a small change to the file within the editor and click “render”. If all goes well, you should end up with a PDF document!\n\nFor Stat340 coursework, it will be helpful to have some additional R packages installed. You can install these all at once with:\n\ninstall.packages(c(\"bayesrules\", \"tidyverse\", \"janitor\", \"rstanarm\",\n                   \"bayesplot\", \"tidybayes\", \"broom.mixed\", \"modelr\",\n                   \"e1071\", \"forcats\"), \n                 dependencies = TRUE)",
    "crumbs": [
      "Computing",
      "Setup"
    ]
  },
  {
    "objectID": "homework/hw01.html",
    "href": "homework/hw01.html",
    "title": "Individual HW01",
    "section": "",
    "text": "BR Exercise 1.5\n\n\nBR Exercise 1.6\n\n\nBR Exercise 2.2\n\n\nBR Exercise 2.6\n\n\nBR Exercise 2.16\n\n\n\n\n\n\nImportant\n\n\n\nThis problem moved to HW2!",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW01"
    ]
  },
  {
    "objectID": "homework/hw03.html",
    "href": "homework/hw03.html",
    "title": "Individual HW03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 5.1 a, d, f\nFor each situation below, tune an appropriate Gamma(s, r) prior model for \\(\\lambda\\)\n\nThe most common value of \\(\\lambda\\) is 4, and the mean is 7.\nThe most common value of \\(\\lambda\\) is 14, and the variance is 6.\nThe mean of \\(\\lambda\\) is 22, and the variance is 3.\nBR Exercise 5.7\nLet \\(\\lambda\\) be the average number of goals scored in a Women’s World Cup game. We’ll analyze \\(\\lambda\\) by the following Gamma-Poisson model: \\[Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\] \\[\\lambda \\sim \\text{Gamma}(1, 0.25)\\]\n\nPlot and summarize our prior understanding of \\(\\lambda\\)\n\nWhy is the Poisson model a reasonable choice for \\(Y_i\\)?\nThe wwc_2019_matches data in the {fivethirtyeight} package includes the number of goals scored by the two teams in each 2019 Women’s World Cup match. Define, plot, and discuss the total number of goals scored per game\n\n\nlibrary(fivethirtyeight)\ndata(\"wwc_2019_matches\")\nwwc_2019_matches &lt;- wwc_2019_matches %&gt;% \n  mutate(total_goals = score1 + score2)\n\n\nidentify the posterior model of \\(\\lambda\\) and verify your answer using summarize_gamma_poisson()\n\nPlot the prior, likelihood, and posterior of \\(\\lambda\\). Describe the evolution in your understanding of \\(\\lambda\\) from the prior to posterior.\nBR Exercise 5.15\nBelow are kernels for Normal, Poisson, Gamma, Beta, and Binomial models. Identify the appropriate model with specific parameter values.\n\n\n\\(f(\\theta) \\propto 0.3^\\theta 0.7^{16 - \\theta}\\) for \\(\\theta \\in \\{0, 1, 2 ..., 16\\}\\)\n\n\n\\(f(\\theta) \\propto 1/\\theta!\\) for \\(\\theta \\in \\{0, 1, ..., \\infty\\}\\)\n\n\n\\(f(\\theta) \\propto \\theta^4(1-\\theta)^7\\) for \\(\\theta \\in [0,1]\\)\n\n\\(f(\\theta) \\propto e^{-\\theta^2}\\)\nNormal-normal conjugacy\nVerify that the normal distribution is a conjugate prior for a normal data model.\nNote: the sketch of this proof is given in 5.3.4 and you are welcome to use it. If you use other sources (e.g. for help remembering how to complete the square), please cite them.\nBR Exercise 5.11 (Normal-normal calculation)\nProf. Abebe and Prof. Morales both recently finished their PhDs and are teaching their first statistics classes at Bayesian University. Their colleagues told them that the average final exam score across all students, \\(\\mu\\), varies Normally from year to year with a mean of 80 points and a standard deviation of 4. Further, individual students’ scores \\(Y\\) vary Normally around \\(\\mu\\) with a known standard deviation of 3 points.\n\nProf. Abebe conducts the final exam and observes that his 32 students scored an average of 86 points. Calculate the posterior mean and variance of \\(\\mu\\) using the data from Prof. Abebe’s class.\nProf. Morales conducts the final exam and observes that her 32 students scored an average of 82 points. Calculate the posterior mean and variance of \\(\\mu\\) using the data from Prof. Morales’ class.\nNext, use Prof. Abebe and Prof. Morales’ combined exams to calculate the posterior mean and variance of \\(\\mu\\)\n\nBR Exercise 5.14: Normal-normal simulation\n\nYour friend Alex has read Chapter 4 of this book, but not Chapter 5. Explain to Alex why it’s difficult to simulate a Normal-Normal posterior using the simulation methods we have learned thus far.\nTo prove your point, try (and fail) to simulate the posterior of \\(\\mu\\) for the following model upon observing a single data point \\(Y_1 = 1.1\\):\n\n\\[Y | \\mu \\sim N(\\mu, 1^2)\\] \\[\\mu \\sim N(0,1)\\]\nBR Exercise 6.6: Gamma-Poisson grid approximation\nConsider the Gamma-Poisson model for \\(\\lambda\\) with \\(\\lambda \\sim \\text{Gamma}(20, 5)\\) and \\(Y_i | \\lambda \\sim \\text{Pois}(\\lambda)\\). Suppose you observe \\(n=3\\) data points \\((Y_1, Y_2, Y_3) = (0, 1, 0)\\)\n\nUtilize grid approximation with grid values \\(\\lambda = \\{0, 1, 2, ..., 8\\}\\) to approximate the posterior of \\(\\lambda\\)\n\nRepeat part a using a grid of 201 equally spaced values between 0 and 8.\nBR Exercise 6.15: Gamma-Poisson MCMC (skip traceplots for now)\n\nSimulate the posterior model of \\(\\lambda\\) with RStan using 4 chains and 10000 iterations per chain.\nProduce density plots for all four chains.\nFrom the density plots, what seems to be the most posterior plausible value of \\(\\lambda\\)?\nHearkening back to Chapter 5, specify the theoretical posterior model of \\(\\lambda\\). How does your MCMC approximation compare? (Try adding an overlaying line to your density plot from (c))\nBR Exercise 6.9: Comparing MCMC to grid approximation\n\nWhat drawback(s) do MCMC and grid approximation share?\nWhat advantage(s) do MCMC and grid approximation share?\nWhat is an advantage of grid approximation over MCMC?\nWhat is an advantage of MCMC over grid approximation?",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW03"
    ]
  },
  {
    "objectID": "homework/hw04.html",
    "href": "homework/hw04.html",
    "title": "Individual HW04",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\nlibrary(rstan) # for MCMC\nlibrary(bayesplot) # for plotting\n\nQ1\nExplain the difference between \\(N_{eff}\\) and the actual number of MCMC samples\nQ2: mcmc_rank_hist and mcmc_rank_overlay\n\nTrace plots are a natural way to view a chain, but they become hard to read when we have many samples and/or many chains. An alternative way to view the chains is called a rank histogram or trace rank plot. What this means is to (a) take all of the samples for a parameter and rank them, (b) draw a histogram of the ranks for each individual chain\nThe code chunk below loads two pre-fit stan models.\n\nload(url(\"https://aluby.github.io/stat340-f25/data/hw04-q2.rda\"))\n\n\nCreate traceplots, rank histograms (mcmc_rank_hist), and trace rank plots (mcmc_rank_overlay) for both MCMC samples.\nWhich sample is “healthy”? Which is “unhealthy”? Explain how you can tell from the new plots\nWhich type of plot do you find more useful?\nQ3: Revisiting BR Exercise 5.11\nProf. Abebe and Prof. Morales both recently finished their PhDs and are teaching their first statistics classes at Bayesian University. Their colleagues told them that the average final exam score across all students, \\(\\mu\\), varies Normally from year to year with a mean of 80 points and a standard deviation of 4. Further, individual students’ scores \\(Y\\) vary Normally around \\(\\mu\\) with an unknown standard deviation \\(\\sigma\\).\n\nSuggest three possible prior distributions for \\(\\sigma\\) (at least two should be different named distributions). Include density plots for each prior. (You can find available probability distributions in the Stan documentation: https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html)\nBelow is starter code for running a Stan model in this scenario. Delete the #| eval: false line and fill in the blanks\n\n\nfn_model2 &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(_____, ____);\n    mu ~ normal(_____, _____);\n    sigma ~ __________;\n  }\n\"\n\n\nRun your model on the data below from Prof. Abebe and Prof. Morales’ combined scores.\n\n\nscores&lt;- c(90.3, 81.3,  85.2, 85.6, 86.4, 83.8, 86.7, 90.1, 82.3, 88.4, 89.2, 90.6,81.2, 85.9, 88.8, 82.9,84.4, 86, 78.2, 87.3, 90.8, 87, 85, 86.8, 89.8, 91.2, 84.8, 83.8, 83.3, 86.1,  92.8, 85.9,  79,74.7, 78, 79.2,  83.9, 79.8, 81.1,  87.5, 81.8,  85.6,  77,  85.2, 85.9,  83.1,92.1,  72.6, 88.3,  78, 76.5,  79.4, 82.8,  82.5, 81.4, 78.9, 75.9, 73.2,84.2, 87.6,  77.6, 84.2,  78.7,  72)\n\n\nRun thorough diagnostics on your MCMC sample. This should include: traceplots of all relevant parameters, density plots for each chain for all relevant parameters, R-hat values, and N-eff. What do you conclude about the validity of your sample?\nWhat do you conclude about the posterior mean \\(\\mu | \\vec{y}\\)? How does your answer differ from HW3’s conjugate analysis?\nRewrite the stan model code to take a flexible normal prior for \\(\\mu\\). That is, we should be able to specify the prior mean \\(\\theta\\) and prior sd \\(\\tau\\) (or prior variance \\(\\tau^2\\), whichever you prefer), instead of hard-coding them as 80 and 4.\nBR 7.4: Tuning the Metropolis-Hastings\nIn this exercise you will consider how to tune a Uniform proposal model with half-width \\(w\\) for a Metropolis-Hastings algorithm.\n\nDraw a trace plot for a tour where the Uniform proposal model uses a very small \\(w\\).\nWhy is it problematic if \\(w\\) is too small, and hence defines the neighborhood around the current chain value too narrowly?\nDraw a trace plot for a tour where the Uniform proposal model uses a very large \\(w\\)\n\nWhy is it problematic if \\(w\\) is too large, and hence defines the neighborhood too widely?\nDraw a trace plot for a tour where the Uniform proposal model uses a \\(w\\) that is neither too small or too large.\nDescribe how you would go about finding an appropriate half-width \\(w\\) for a Uniform proposal model.\nBR 7.5: One iteration with uniform proposal model\nThe function one_mh_iteration() from the text utilizes a Uniform proposal model, \\(\\mu' | \\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)\\), with half-width \\(w=1\\). Starting from a current value of \\(\\mu = 3\\) and using set.seed(1), run the code below and comment on the returned proposal, alpha, and next_stop values.\n\n\n\n\n\n\nNote\n\n\n\nYou can use one_mh_iteration directly from the text\n\n\n\none_mh_iteration(w = 0.01, current = 3)\none_mh_iteration(w = 0.5, current = 3)\none_mh_iteration(w = 1, current = 3)\none_mh_iteration(w = 3, current = 3)\nBR 7.11: Changing the proposal model\nFor this exercise, modify one_mh_iteration() to create a new function, one_mh_iteration_normal(), which utilizes a symmetric Normal proposal model, centered at the current chain value \\(\\mu\\) with standard deviation \\(s\\):\n\\[\\mu ' | \\mu \\sim N(\\mu, s^2)\\]\nSubsequently, starting from a current value of \\(\\mu = 3\\) and set.seed(1), run this function under each setting below. Comment on the returned proposal, alpha, and next_stop values.\n\none_mh_iteration_normal(s = 0.01, current = 3)\none_mh_iteration_normal(s = 0.5, current = 3)\none_mh_iteration_normal(s = 1, current = 3)\none_mh_iteration_normal(s = 3, current = 3)\nBR 7.12 (Metropolis-Hastings tour with normal proposals)\nUpon completing the previous exercise, modify mh_tour() to create a new function, mh_tour_normal(), which constructs a chain of \\(\\mu\\) values using a Normal proposal model with standard deviation \\(s\\). Subsequently, using set.seed(84735), run this function under each setting below and construct a trace plot of the chain.\n\n20 iterations, s=0.01\n20 iterations, s=10\n1000 iterations, s=0.01\n1000 iterations, s=10\nContrast the trace plots in a and b. Explain in simple terms why changing the standard deviation of the Normal proposal model causes these differences.\nReflecting on the above results, tune your Metropolis-Hastings algorithm. That is, identify a reasonable value for standard deviation \\(s\\) and provide a trace plot as proof.",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW04"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "The best place to come for help is Amanda’s office hours! I can help with anything related to the course. You can also check my calendar for one-on-one appointment slots.\n\n\n\nStudents needing help with their Statistics coursework and R/Rstudio questions can get help from Stats Lab Assistants in the Stats Lab in CMC 304. Stats Lab Assistants are primiarily for Stat 120 (Intro Stats), but they may also be able to assist with general R/Rstudio questions."
  },
  {
    "objectID": "course-faq.html#where-can-i-get-help",
    "href": "course-faq.html#where-can-i-get-help",
    "title": "FAQ",
    "section": "",
    "text": "The best place to come for help is Amanda’s office hours! I can help with anything related to the course. You can also check my calendar for one-on-one appointment slots.\n\n\n\nStudents needing help with their Statistics coursework and R/Rstudio questions can get help from Stats Lab Assistants in the Stats Lab in CMC 304. Stats Lab Assistants are primiarily for Stat 120 (Intro Stats), but they may also be able to assist with general R/Rstudio questions."
  },
  {
    "objectID": "slides/slides06.html#beta-binomial-review",
    "href": "slides/slides06.html#beta-binomial-review",
    "title": "Gamma-Poisson model",
    "section": "Beta-Binomial review",
    "text": "Beta-Binomial review\n\nPrior: \\(f(\\pi) \\sim \\text{Beta}(\\alpha, \\beta)\\)\nLikelihood: \\(L(\\pi | Y) \\sim \\text{Binomial}(n, \\pi)\\)\nPosterior: \\(f(\\pi |Y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\)\n\n \n\n\n\n\n\n\nConjugate Prior\n\n\nThe Beta distribution is a conjugate prior for the Binomial likelihood, since the posterior is also Beta"
  },
  {
    "objectID": "slides/slides06.html#non-conjugate-prior-for-binomial-likelihood",
    "href": "slides/slides06.html#non-conjugate-prior-for-binomial-likelihood",
    "title": "Gamma-Poisson model",
    "section": "Non-conjugate prior for Binomial likelihood",
    "text": "Non-conjugate prior for Binomial likelihood\n\\[f(\\pi) = e - e^\\pi\\]"
  },
  {
    "objectID": "slides/slides06.html#finding-the-posterior",
    "href": "slides/slides06.html#finding-the-posterior",
    "title": "Gamma-Poisson model",
    "section": "Finding the posterior",
    "text": "Finding the posterior\nAssume \\(Y=10\\) and \\(n=50\\)"
  },
  {
    "objectID": "slides/slides06.html#can-we-use-beta-binomial-model",
    "href": "slides/slides06.html#can-we-use-beta-binomial-model",
    "title": "Gamma-Poisson model",
    "section": "Can we use Beta-Binomial Model?",
    "text": "Can we use Beta-Binomial Model?\nAt a certain hospital, an average of 6 babies are born each hour. Let \\(Y\\) be the number of babies born between 9 a.m. and 10 a.m. tomorrow."
  },
  {
    "objectID": "slides/slides06.html#poisson-data-model",
    "href": "slides/slides06.html#poisson-data-model",
    "title": "Gamma-Poisson model",
    "section": "Poisson Data model",
    "text": "Poisson Data model\nLet \\(Y\\) be the number of independent events that occur in a fixed amount of time or space, where \\(\\lambda &gt; 0\\) is the rate at which these events occur. Then the dependence of \\(Y\\) on parameter \\(\\lambda\\) can be modeled by the Poisson. In mathematical notation:\n\\[Y | \\lambda \\sim \\text{Pois}(\\lambda) \\]\nand\n\\[ f(y|\\lambda) =  \\frac{\\lambda^y e^{-\\lambda}}{y!}\\;\\; \\text{ for } y \\in \\{0,1,2,\\ldots\\}\\]\n\\[E(Y | \\lambda) = V(Y | \\lambda) = \\lambda\\]"
  },
  {
    "objectID": "slides/slides06.html#poisson-pmfs",
    "href": "slides/slides06.html#poisson-pmfs",
    "title": "Gamma-Poisson model",
    "section": "Poisson PMFs",
    "text": "Poisson PMFs"
  },
  {
    "objectID": "slides/slides06.html#joint-likelihood-function",
    "href": "slides/slides06.html#joint-likelihood-function",
    "title": "Gamma-Poisson model",
    "section": "Joint Likelihood Function",
    "text": "Joint Likelihood Function"
  },
  {
    "objectID": "slides/slides06.html#poisson-likelihood",
    "href": "slides/slides06.html#poisson-likelihood",
    "title": "Gamma-Poisson model",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\nIf we collect 4 hours of data and observe \\(y_i = 0, 2, 3, 2\\) babies born:"
  },
  {
    "objectID": "slides/slides06.html#section-1",
    "href": "slides/slides06.html#section-1",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "plot_poisson_likelihood(y = c(0, 2, 3, 2), lambda_upper_bound = 8)"
  },
  {
    "objectID": "slides/slides06.html#choosing-a-prior",
    "href": "slides/slides06.html#choosing-a-prior",
    "title": "Gamma-Poisson model",
    "section": "Choosing a prior",
    "text": "Choosing a prior\n\n\\(\\lambda\\) discrete or continuous?\nWhat is the support of \\(\\lambda\\)?\nBe strategic about creating a kernel"
  },
  {
    "objectID": "slides/slides06.html#gamma-model",
    "href": "slides/slides06.html#gamma-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma Model",
    "text": "Gamma Model\nLet \\(\\lambda\\) be a random variable which can take any value between 0 and \\(\\infty\\), ie. \\(\\lambda \\in [0,\\infty)\\). The Gamma model with shape parameter \\(s &gt; 0\\) and rate parameter \\(r &gt; 0\\) is:\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\]\n\\[f(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda} \\;\\; \\text{ for } \\lambda &gt; 0 \\]\nwhere constant \\(\\Gamma(s) = \\int_0^\\infty z^{s - 1} e^{-z}dz\\). When \\(s\\) is a positive integer, \\(s \\in \\{1,2,3,\\ldots\\}\\), this constant simplifies to \\(\\Gamma(s) = (s - 1)!\\)."
  },
  {
    "objectID": "slides/slides06.html#section-2",
    "href": "slides/slides06.html#section-2",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "Warning\n\n\nThe Gamma is sometimes parameterized by shape \\(s\\) and scale \\(\\theta = \\frac{1}{r}\\)\n\n\n\n\nIn econometrics, the (α, θ) parameterization is common for modeling waiting times … Bayesian statisticians prefer the (α,λ) parameterization, utilizing the gamma distribution as a conjugate prior for several inverse scale parameters, facilitating analytical tractability in posterior distribution computations.\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/slides06.html#section-3",
    "href": "slides/slides06.html#section-3",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "Exponential Model\n\\[\\lambda \\sim \\text{Exp}(r)\\]\nis a special case of the Gamma with shape parameter \\(s = 1\\), \\(\\text{Gamma}(1,r)\\)\n\\(\\chi^2\\) Model\n\\[\\lambda \\sim \\chi^2(k)\\]\nis a special case of the Gamma with shape \\(s = k/2\\) and rate \\(r=1/2\\)"
  },
  {
    "objectID": "slides/slides06.html#section-4",
    "href": "slides/slides06.html#section-4",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "\\(E(\\lambda) = \\frac{s}{r}, \\text{Mode}(\\lambda) = \\frac{s-1}{r} \\text{ for s } \\ge 1, \\text{Var}(\\lambda) = \\frac{s}{r^2}\\)"
  },
  {
    "objectID": "slides/slides06.html#posterior-model",
    "href": "slides/slides06.html#posterior-model",
    "title": "Gamma-Poisson model",
    "section": "Posterior Model",
    "text": "Posterior Model"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-bayesian-model",
    "href": "slides/slides06.html#gamma-poisson-bayesian-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Bayesian Model",
    "text": "Gamma-Poisson Bayesian Model\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\] \\[ Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\]\n\\[\\lambda | Y_1, Y_2, ..., Y_n \\sim \\text{Gamma}(s + \\sum Y_i, r + n)\\]"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-model",
    "href": "slides/slides06.html#gamma-poisson-model",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Model",
    "text": "Gamma-Poisson Model\n\nplot_gamma_poisson(shape = 10, rate = 2, sum_y = 7, n = 4)"
  },
  {
    "objectID": "slides/slides06.html#gamma-poisson-model-1",
    "href": "slides/slides06.html#gamma-poisson-model-1",
    "title": "Gamma-Poisson model",
    "section": "Gamma-Poisson Model",
    "text": "Gamma-Poisson Model\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 7, n = 4)\n\n      model shape rate     mean     mode       var        sd\n1     prior    10    2 5.000000 4.500000 2.5000000 1.5811388\n2 posterior    17    6 2.833333 2.666667 0.4722222 0.6871843\n\n\nInterpretation:"
  },
  {
    "objectID": "slides/slides06.html#example-tuning-a-gamma-prior",
    "href": "slides/slides06.html#example-tuning-a-gamma-prior",
    "title": "Gamma-Poisson model",
    "section": "Example: tuning a Gamma prior",
    "text": "Example: tuning a Gamma prior\nThe most likely value of \\(\\lambda\\) is 3.5 and the variance is 2"
  },
  {
    "objectID": "slides/slides06.html#example-gamma-poisson-model",
    "href": "slides/slides06.html#example-gamma-poisson-model",
    "title": "Gamma-Poisson model",
    "section": "Example: Gamma-Poisson model",
    "text": "Example: Gamma-Poisson model\nAssume a prior of \\(\\lambda \\sim \\text{Gamma}(24, 2)\\) and \\(Y | \\lambda \\sim \\text{Poisson}(\\lambda)\\). If you observe a sample of \\((12, 12, 12, 0)\\), what is the likelihood and posterior?"
  },
  {
    "objectID": "slides/slides06.html#section-5",
    "href": "slides/slides06.html#section-5",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "plot_gamma_poisson(shape = 24, rate = 2, sum_y = 36, n = 4)"
  },
  {
    "objectID": "slides/slides06.html#section-6",
    "href": "slides/slides06.html#section-6",
    "title": "Gamma-Poisson model",
    "section": "",
    "text": "summarize_gamma_poisson(shape = 24, rate = 2, sum_y = 36, n = 4)\n\n      model shape rate mean      mode      var       sd\n1     prior    24    2   12 11.500000 6.000000 2.449490\n2 posterior    60    6   10  9.833333 1.666667 1.290994\n\n\nInterpretation:"
  },
  {
    "objectID": "slides/slides07.html#recap",
    "href": "slides/slides07.html#recap",
    "title": "Normal-Normal model",
    "section": "Recap",
    "text": "Recap\n\nStart with a reasonable likelihood for the data\nA conjugate prior for a given likelihood is a prior model that results in a posterior with the same shape, but different parameters\nThe beta distribution is a conjugate prior for the binomial likelihood\nThe gamma distribution is a conjugate prior for the poisson likelihood"
  },
  {
    "objectID": "slides/slides07.html#section",
    "href": "slides/slides07.html#section",
    "title": "Normal-Normal model",
    "section": "",
    "text": "Beta-Binomial Bayesian Model\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\] \\[ Y | \\pi  \\sim \\text{Binomial}(n, \\pi)\\] \\[ \\pi |Y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nGamma-Poisson Bayesian Model\n\\[\\lambda \\sim \\text{Gamma}(s, r)\\] \\[ Y_i | \\lambda \\sim \\text{Poisson}(\\lambda)\\]\n\\[\\lambda | Y_1, Y_2, ..., Y_n \\sim \\text{Gamma}(s + \\sum Y_i, r + n)\\]"
  },
  {
    "objectID": "slides/slides07.html#data",
    "href": "slides/slides07.html#data",
    "title": "Normal-Normal model",
    "section": "Data",
    "text": "Data\nData on heights (in cm) of 352 adults"
  },
  {
    "objectID": "slides/slides07.html#normal-model",
    "href": "slides/slides07.html#normal-model",
    "title": "Normal-Normal model",
    "section": "Normal Model",
    "text": "Normal Model\nLet \\(Y\\) be a random variable which can take any value \\(Y \\in (-\\infty,\\infty)\\) and is unimodal and symmetric. Then \\(Y\\) might be well represented by a Normal model with mean parameter \\(\\mu \\in (-\\infty, \\infty)\\) and standard deviation parameter \\(\\sigma &gt; 0\\):\n\\[Y \\sim N(\\mu, \\sigma^2)\\]\nThe Normal model is specified by continuous pdf\n\\[\\begin{equation}\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigg[{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\bigg] \\;\\; \\text{ for } y \\in (-\\infty,\\infty)\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/slides07.html#section-1",
    "href": "slides/slides07.html#section-1",
    "title": "Normal-Normal model",
    "section": "",
    "text": "\\(E(Y) = \\mu, \\text{Mode}(Y) = \\mu, \\text{Var}(Y) = \\sigma^2\\)"
  },
  {
    "objectID": "slides/slides07.html#terminology",
    "href": "slides/slides07.html#terminology",
    "title": "Normal-Normal model",
    "section": "Terminology",
    "text": "Terminology\n\nNormal: “usual” or “customary”\n\nStatisticians historically called it “normal” because it was the “usual” distribution for, e.g., the CLT\n\nGaussian: name the distribution for the person who developed it\n\nAlso not quite right, since DeMoivre first proved the CLT\n\nBoth terms describe the same mathematical function"
  },
  {
    "objectID": "slides/slides07.html#normal-likelihood",
    "href": "slides/slides07.html#normal-likelihood",
    "title": "Normal-Normal model",
    "section": "Normal Likelihood",
    "text": "Normal Likelihood"
  },
  {
    "objectID": "slides/slides07.html#normal-normal-bayesian-model",
    "href": "slides/slides07.html#normal-normal-bayesian-model",
    "title": "Normal-Normal model",
    "section": "Normal-Normal Bayesian Model",
    "text": "Normal-Normal Bayesian Model\nLet $$ be an unknown mean parameter and \\((Y_1,Y_2,\\ldots,Y_n)\\) be an independent \\(N(\\mu,\\sigma^2)\\) sample where \\(\\sigma\\) is assumed to be known.\n\\[\\mu \\sim N(\\theta, \\tau^2)\\] \\[Y_i | \\mu  \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\] \\[\\mu|\\vec{y} \\; \\sim \\;  N\\bigg(\\frac{\\theta\\sigma^2/n + \\bar{y}\\tau^2}{\\tau^2+\\sigma^2/n}, \\; \\frac{\\tau^2\\sigma^2/n}{\\tau^2+\\sigma^2/n}\\bigg)\\]\n\n\nYou’ll show this on homework this week!"
  },
  {
    "objectID": "slides/slides07.html#posterior-mean-as-weighted-average",
    "href": "slides/slides07.html#posterior-mean-as-weighted-average",
    "title": "Normal-Normal model",
    "section": "Posterior Mean as weighted average",
    "text": "Posterior Mean as weighted average"
  },
  {
    "objectID": "slides/slides07.html#example",
    "href": "slides/slides07.html#example",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\n\n\nPartial census data for the Dobe area !Kung San, a foraging population\nCompiled from Nancy Howell’s interviews\n\n\n\n# A tibble: 352 × 4\n   height weight   age  male\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   152.   47.8    63     1\n 2   140.   36.5    63     0\n 3   137.   31.9    65     0\n 4   157.   53.0    41     1\n 5   145.   41.3    51     0\n 6   164.   63.0    35     1\n 7   149.   38.2    32     0\n 8   169.   55.5    27     1\n 9   148.   34.9    19     0\n10   165.   54.5    54     1\n# ℹ 342 more rows"
  },
  {
    "objectID": "slides/slides07.html#example-1",
    "href": "slides/slides07.html#example-1",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\nLet’s say we’re interested in analyzing the average height of an adult\n\n\n\n\n\n\n\n\n\n\n\nAnthropologists would be interested in more complex relationships, but we have to start somewhere."
  },
  {
    "objectID": "slides/slides07.html#example-2",
    "href": "slides/slides07.html#example-2",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nplot_normal(170, 20)"
  },
  {
    "objectID": "slides/slides07.html#example-3",
    "href": "slides/slides07.html#example-3",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nadults |&gt;\n  summarize(\n    mean = mean(height),\n    sd = sd(height),\n    n = n()\n  )\n\n# A tibble: 1 × 3\n   mean    sd     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  155.  7.74   352"
  },
  {
    "objectID": "slides/slides07.html#example-4",
    "href": "slides/slides07.html#example-4",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nplot_normal_normal(mean = 170, \n                        sd = 20, \n                        sigma = sd(adults$height), \n                        y_bar = mean(adults$height),\n                        n = length(adults$height))"
  },
  {
    "objectID": "slides/slides07.html#example-5",
    "href": "slides/slides07.html#example-5",
    "title": "Normal-Normal model",
    "section": "Example",
    "text": "Example\n\nsummarize_normal_normal(mean = 170, \n                        sd = 20, \n                        sigma = sd(adults$height), \n                        y_bar = mean(adults$height),\n                        n = length(adults$height))\n\n      model     mean     mode         var         sd\n1     prior 170.0000 170.0000 400.0000000 20.0000000\n2 posterior 154.6036 154.6036   0.1702222  0.4125799"
  },
  {
    "objectID": "slides/slides07.html#what-if-we-only-had-a-sample-of-35",
    "href": "slides/slides07.html#what-if-we-only-had-a-sample-of-35",
    "title": "Normal-Normal model",
    "section": "What if we only had a sample of 35?",
    "text": "What if we only had a sample of 35?"
  },
  {
    "objectID": "slides/slides07.html#what-if-we-only-had-a-sample-of-3",
    "href": "slides/slides07.html#what-if-we-only-had-a-sample-of-3",
    "title": "Normal-Normal model",
    "section": "What if we only had a sample of 3?",
    "text": "What if we only had a sample of 3?"
  },
  {
    "objectID": "slides/slides07.html#multiparameter-model",
    "href": "slides/slides07.html#multiparameter-model",
    "title": "Normal-Normal model",
    "section": "Multiparameter model",
    "text": "Multiparameter model"
  },
  {
    "objectID": "slides/slides05.html#key-ideas",
    "href": "slides/slides05.html#key-ideas",
    "title": "Balance and Sequentiality",
    "section": "Key ideas",
    "text": "Key ideas\n\nDifferent priors, different posteriors\nDifferent data, different posteriors\nSequential analyses\nData order is invariant\nDon’t be stubborn"
  },
  {
    "objectID": "slides/slides05.html#bechdel-test",
    "href": "slides/slides05.html#bechdel-test",
    "title": "Balance and Sequentiality",
    "section": "Bechdel test",
    "text": "Bechdel test\nAlison Bechdel’s 1985 comic Dykes to Watch Out For has a strip called The Rule where a person states that they only go to a movie if it satisfies the following three rules:\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThis test is used for assessing movies in terms of representation of women. Even though there are three criteria, a movie either fails or passes the Bechdel test."
  },
  {
    "objectID": "slides/slides05.html#priors",
    "href": "slides/slides05.html#priors",
    "title": "Balance and Sequentiality",
    "section": "Priors",
    "text": "Priors"
  },
  {
    "objectID": "slides/slides05.html#terminology",
    "href": "slides/slides05.html#terminology",
    "title": "Balance and Sequentiality",
    "section": "Terminology",
    "text": "Terminology\n\n\n\n\n\n\nInformative Prior\n\n\nAn informative prior reflects specific information about the unknown variable with high certainty (ie. low variability).\n\n\n\n\n\n\n\n\n\nDiffuse prior\n\n\nA vague or diffuse prior reflects little specific information about the unknown variable. A flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\n\n\n\n\n\n\n\n\n\nReference prior\n\n\nA formal type of diffuse prior\ncatalog of reference priors"
  },
  {
    "objectID": "slides/slides05.html#section",
    "href": "slides/slides05.html#section",
    "title": "Balance and Sequentiality",
    "section": "",
    "text": "“There is no objective, unique prior that represents ignorance. Instead, reference priors are chosen by public agreement, much like units of length and weight. In this interpretation, reference priors are akin to a default option in a computer package. We fall back to the default when there is insufficient information to otherwise define the prior.” Kass & Wasserman"
  },
  {
    "objectID": "slides/slides05.html#flat-prior",
    "href": "slides/slides05.html#flat-prior",
    "title": "Balance and Sequentiality",
    "section": "Flat prior",
    "text": "Flat prior\n\\[\\pi \\sim \\text{Unif}(0,1)\\]\n\n\nUnder re-parameterization (\\(\\theta = g(\\pi)\\)) \\(\\theta\\) might have a non-uniform distribution"
  },
  {
    "objectID": "slides/slides05.html#jeffreys-prior",
    "href": "slides/slides05.html#jeffreys-prior",
    "title": "Balance and Sequentiality",
    "section": "Jeffrey’s prior",
    "text": "Jeffrey’s prior\n\\(\\pi \\propto \\sqrt{I(\\pi)}\\)\nwhere \\(I(\\pi)\\) is the Fisher Information. For binomial data, Jeffrey’s prior gives \\(\\pi \\sim\\)"
  },
  {
    "objectID": "slides/slides05.html#bernardos-reference-prior",
    "href": "slides/slides05.html#bernardos-reference-prior",
    "title": "Balance and Sequentiality",
    "section": "Bernardo’s reference prior",
    "text": "Bernardo’s reference prior\n\nPrior should influence posterior as little as possible\nPosterior should be (on average) very different from the posterior\nWe can measure “difference” between distributions \\(P\\) and \\(Q\\) with the Kullback-Leibler divergence\n\n\\(KL(P,Q) = \\int_{-\\infty}^\\infty p(x) \\log \\frac{p(x)}{q(x)} dx\\)\n\nSo, choose reference prior by maximizing KL divergence between prior and posterior\nLucky for us, if we have univariate model, this is equivalent to Jeffrey’s prior"
  },
  {
    "objectID": "slides/slides05.html#priors-1",
    "href": "slides/slides05.html#priors-1",
    "title": "Balance and Sequentiality",
    "section": "Priors",
    "text": "Priors"
  },
  {
    "objectID": "slides/slides05.html#data",
    "href": "slides/slides05.html#data",
    "title": "Balance and Sequentiality",
    "section": "Data",
    "text": "Data\n\nlibrary(bayesrules) has bechdel data frame. Randomly select 20 movies from this dataset (seed = 84735)\nBased on observed data, update the posterior\nPlot the prior, likelihood, and the posterior\n\n\nset.seed(84735)\nbechdel_sample &lt;- sample_n(bechdel, 20)"
  },
  {
    "objectID": "slides/slides05.html#data-1",
    "href": "slides/slides05.html#data-1",
    "title": "Balance and Sequentiality",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\n\n\nyear\ntitle\nbinary\n\n\n\n\n2005\nKing Kong\nFAIL\n\n\n1983\nFlashdance\nPASS\n\n\n2013\nThe Purge\nFAIL\n\n\n2001\nAmerican Outlaws\nFAIL\n\n\n2010\nSex and the city 2\nPASS\n\n\n1997\nAir Bud\nFAIL\n\n\n2010\nRobin Hood\nFAIL\n\n\n2009\nCase 39\nPASS\n\n\n1998\nHope Floats\nPASS\n\n\n2007\nThe Golden Compass\nPASS\n\n\n2013\nParker\nPASS\n\n\n1971\nShaft\nFAIL\n\n\n2011\nFast Five\nPASS\n\n\n2013\nThe Croods\nFAIL\n\n\n2010\nEdge of Darkness\nFAIL\n\n\n1998\nMulan\nFAIL\n\n\n2010\nEasy A\nPASS\n\n\n2000\nMemento\nFAIL\n\n\n2002\nThe Tuxedo\nFAIL\n\n\n1982\nThe Best Little Whorehouse in Texas\nPASS\n\n\n\n\n\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45"
  },
  {
    "objectID": "slides/slides05.html#posteriors",
    "href": "slides/slides05.html#posteriors",
    "title": "Balance and Sequentiality",
    "section": "Posteriors",
    "text": "Posteriors"
  },
  {
    "objectID": "slides/slides05.html#different-data-different-posteriors",
    "href": "slides/slides05.html#different-data-different-posteriors",
    "title": "Balance and Sequentiality",
    "section": "Different data, different posteriors",
    "text": "Different data, different posteriors\nLet’s compare results for 3 analyses: 1 using 1991 movies, 1 using 2000 movies, and 1 using 2013 movies\n\nbechdel |&gt;\n  filter(year == 1991) |&gt;\n  tabyl(binary)\n\n binary n   percent\n   FAIL 7 0.5384615\n   PASS 6 0.4615385\n\nbechdel |&gt;\n  filter(year == 2000) |&gt;\n  tabyl(binary)\n\n binary  n   percent\n   FAIL 34 0.5396825\n   PASS 29 0.4603175\n\nbechdel |&gt;\n  filter(year == 2013) |&gt;\n  tabyl(binary)\n\n binary  n   percent\n   FAIL 53 0.5353535\n   PASS 46 0.4646465"
  },
  {
    "objectID": "slides/slides05.html#different-data-different-posteriors-1",
    "href": "slides/slides05.html#different-data-different-posteriors-1",
    "title": "Balance and Sequentiality",
    "section": "Different data, different posteriors",
    "text": "Different data, different posteriors"
  },
  {
    "objectID": "slides/slides05.html#prior-updatingsequential-data",
    "href": "slides/slides05.html#prior-updatingsequential-data",
    "title": "Balance and Sequentiality",
    "section": "Prior updating/sequential data",
    "text": "Prior updating/sequential data"
  },
  {
    "objectID": "slides/slides05.html#data-order-invariance",
    "href": "slides/slides05.html#data-order-invariance",
    "title": "Balance and Sequentiality",
    "section": "Data order invariance",
    "text": "Data order invariance\nUpdating the prior 20 times:\n      model alpha beta      mean      mode          var         sd\n1     prior   551  613 0.4733677 0.4733219 0.0002139835 0.01462817\n2 posterior   612  681 0.4733179 0.4732765 0.0001926492 0.01387981\n“Data dump”: treat years 1-20 as the data\n\nsummarize_beta_binomial(1, 1, 611, 1291)\n\n      model alpha beta      mean      mode          var         sd\n1     prior     1    1 0.5000000       NaN 0.0833333333 0.28867513\n2 posterior   612  681 0.4733179 0.4732765 0.0001926492 0.01387981\n\n\nFurther reading: How to think like an epidemiologist"
  },
  {
    "objectID": "slides/slides05.html#data-order-invariance-math",
    "href": "slides/slides05.html#data-order-invariance-math",
    "title": "Balance and Sequentiality",
    "section": "Data order invariance (math)",
    "text": "Data order invariance (math)"
  },
  {
    "objectID": "slides/slides05.html#dont-be-stubborn",
    "href": "slides/slides05.html#dont-be-stubborn",
    "title": "Balance and Sequentiality",
    "section": "Don’t be stubborn",
    "text": "Don’t be stubborn\n\n\n\n\n\n\nWarning\n\n\nThe posterior is defined on the same values as the prior. Do not assign \\(f(\\pi) = 0\\) on any reasonable value!"
  },
  {
    "objectID": "slides/slides10.html#recap-bayesian-analysis",
    "href": "slides/slides10.html#recap-bayesian-analysis",
    "title": "Under the MCMC hood",
    "section": "Recap (Bayesian Analysis)",
    "text": "Recap (Bayesian Analysis)"
  },
  {
    "objectID": "slides/slides10.html#computing-the-posterior",
    "href": "slides/slides10.html#computing-the-posterior",
    "title": "Under the MCMC hood",
    "section": "Computing the posterior",
    "text": "Computing the posterior\n1. Analytical Approach\n2. Grid Approximation\n3. Markov Chain Monte Carlo"
  },
  {
    "objectID": "slides/slides10.html#mcmc",
    "href": "slides/slides10.html#mcmc",
    "title": "Under the MCMC hood",
    "section": "MCMC",
    "text": "MCMC\n\nHow to fit them in {rstan}\nHow to diagnose convergence and fit\nHow they work\n\ntoday: conceptual\nWed: implementation in R"
  },
  {
    "objectID": "slides/slides10.html#section",
    "href": "slides/slides10.html#section",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Source: Richard McElreath’s Statistical Rethinking course, Lecture 08"
  },
  {
    "objectID": "slides/slides10.html#section-1",
    "href": "slides/slides10.html#section-1",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "King Markov must visit each island in proportion to its population size"
  },
  {
    "objectID": "slides/slides10.html#section-2",
    "href": "slides/slides10.html#section-2",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Flip a coin to choose island on left or right (proposal)"
  },
  {
    "objectID": "slides/slides10.html#section-3",
    "href": "slides/slides10.html#section-3",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Flip a coin to choose island on left or right (proposal)\nFind population of proposal island"
  },
  {
    "objectID": "slides/slides10.html#section-4",
    "href": "slides/slides10.html#section-4",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Flip a coin to choose island on left or right (proposal)\nFind population of proposal island\nFind population of current island"
  },
  {
    "objectID": "slides/slides10.html#section-5",
    "href": "slides/slides10.html#section-5",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Flip a coin to choose island on left or right (proposal)\nFind population of proposal island\nFind population of current island\nMove to proposal with probability \\(p_{new}/p_{current}\\)"
  },
  {
    "objectID": "slides/slides10.html#section-6",
    "href": "slides/slides10.html#section-6",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Flip a coin to choose island on left or right (proposal)\nFind population of proposal island\nFind population of current island\nMove to proposal with probability \\(p_{new}/p_{current}\\)\nRepeat"
  },
  {
    "objectID": "slides/slides10.html#this-procedure-ensures-visiting-each-island-in-proportion-to-its-population-in-the-long-run",
    "href": "slides/slides10.html#this-procedure-ensures-visiting-each-island-in-proportion-to-its-population-in-the-long-run",
    "title": "Under the MCMC hood",
    "section": "This procedure ensures visiting each island in proportion to its population in the long run",
    "text": "This procedure ensures visiting each island in proportion to its population in the long run\n\n\n\n\n\n\nMetropolis Algorithm\n\n\n\nFlip a coin to choose island on left or right (proposal)\nFind population of proposal island\nFind population of current island\nMove to proposal with probability \\(p_{new}/p_{current}\\)\nRepeat"
  },
  {
    "objectID": "slides/slides10.html#animation",
    "href": "slides/slides10.html#animation",
    "title": "Under the MCMC hood",
    "section": "Animation",
    "text": "Animation"
  },
  {
    "objectID": "slides/slides10.html#markov-chain-monte-carlo-1",
    "href": "slides/slides10.html#markov-chain-monte-carlo-1",
    "title": "Under the MCMC hood",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\n“Islands”: parameter values\n“Population size”: posterior probability\nVisit each parameter value in proportion to its posterior probability\nAny number of parameters\nLots of “flavors” of algorithms\n\nHow do you propose a next step?\nHow do you calculate acceptance probability?"
  },
  {
    "objectID": "slides/slides10.html#some-variations",
    "href": "slides/slides10.html#some-variations",
    "title": "Under the MCMC hood",
    "section": "Some variations",
    "text": "Some variations\nhttps://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH"
  },
  {
    "objectID": "slides/slides10.html#section-8",
    "href": "slides/slides10.html#section-8",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Metropolis-Hastings Algorithm\n\n\nLet parameter \\(\\mu\\) have posterior \\(f(\\mu|y) \\propto f(\\mu) L(\\mu | y)\\). A Metropolis-Hastings Markov Chain \\(\\{\\mu^{(1)}, \\mu^{(2)}, ..., \\mu^{(n)}\\}\\) evolves as follows. Let \\(\\mu^{(i)} = \\mu\\) be the chains location at \\(i \\in \\{1, 2, ...., N-1\\}\\) and identify \\(\\mu^{(i+1)}\\) through a two step process:\n\nPropose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu'\\) from a proposal model with pdf \\(q(\\mu'|\\mu)\\)\nDecide whether to go there. Calculate the acceptance probability \\[\\alpha = \\min\\{1, \\frac{f(\\mu') L(\\mu'|y) q(\\mu|\\mu')}{f(\\mu) L(\\mu|y) q(\\mu'|\\mu)} \\}\\] Then,\n\n\\[\\mu^{(i+1)} = \\begin{cases} \\mu' & \\text{with probability } \\alpha \\\\ \\mu & \\text{with probability } 1-\\alpha \\end{cases}\\]"
  },
  {
    "objectID": "slides/slides10.html#section-9",
    "href": "slides/slides10.html#section-9",
    "title": "Under the MCMC hood",
    "section": "",
    "text": "Metropolis Algorithm\n\n\nIf the proposal model is symmetric, then \\(q(\\mu | \\mu') = q(\\mu' | \\mu)\\) and the acceptance probability simplifies to:\n\\[\\alpha = \\min\\{1, \\frac{f(\\mu') L(\\mu'|y)}{f(\\mu) L(\\mu|y)} \\}\\]"
  },
  {
    "objectID": "slides/slides03.html#is-this-the-same-likelihood-as-the-stat250-likelihood",
    "href": "slides/slides03.html#is-this-the-same-likelihood-as-the-stat250-likelihood",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Is this the same likelihood as the Stat250 likelihood?",
    "text": "Is this the same likelihood as the Stat250 likelihood?"
  },
  {
    "objectID": "slides/slides03.html#does-it-matter-if-y-is-a-discrete-or-continuous-rv",
    "href": "slides/slides03.html#does-it-matter-if-y-is-a-discrete-or-continuous-rv",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Does it matter if \\(Y\\) is a discrete or continuous RV?",
    "text": "Does it matter if \\(Y\\) is a discrete or continuous RV?"
  },
  {
    "objectID": "slides/slides03.html#example-from-last-time-phd-admissions",
    "href": "slides/slides03.html#example-from-last-time-phd-admissions",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Example from last time: PhD admissions",
    "text": "Example from last time: PhD admissions\n\n\n\nπ\n0.2\n0.4\n0.8\n\n\n\n\nf(π)\n0.7\n0.2\n0.1\n\n\nL(π | y = 3)\n0.0512\n0.2304\n0.2048\n\n\nf(π | y = 3)\n0.35\n0.45\n0.20"
  },
  {
    "objectID": "slides/slides03.html#first-define-and-simulate-from-prior",
    "href": "slides/slides03.html#first-define-and-simulate-from-prior",
    "title": "Likelihood review + Posterior Simulation",
    "section": "First, define and simulate from prior",
    "text": "First, define and simulate from prior\n\nadmissions &lt;- tibble(\n  pi = c(.2, .4, .8)\n)\n\nprior &lt;- c(0.7, 0.2, 0.1)\n\nset.seed(091925)\nadmissions_sim &lt;- sample_n(admissions, \n                           size = 10000, \n                           weight = prior, \n                           replace = TRUE)"
  },
  {
    "objectID": "slides/slides03.html#section",
    "href": "slides/slides03.html#section",
    "title": "Likelihood review + Posterior Simulation",
    "section": "",
    "text": "admissions_sim\n\n# A tibble: 10,000 × 1\n      pi\n   &lt;dbl&gt;\n 1   0.2\n 2   0.2\n 3   0.2\n 4   0.4\n 5   0.2\n 6   0.4\n 7   0.2\n 8   0.4\n 9   0.2\n10   0.8\n# ℹ 9,990 more rows"
  },
  {
    "objectID": "slides/slides03.html#simulated-pi-closely-approximates-prior-fpi",
    "href": "slides/slides03.html#simulated-pi-closely-approximates-prior-fpi",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulated pi closely approximates prior \\(f(\\pi)\\)",
    "text": "Simulated pi closely approximates prior \\(f(\\pi)\\)\n\nadmissions_sim |&gt;\n  tabyl(pi)\n\n  pi    n percent\n 0.2 7030  0.7030\n 0.4 2002  0.2002\n 0.8  968  0.0968"
  },
  {
    "objectID": "slides/slides03.html#simulate-outcomes",
    "href": "slides/slides03.html#simulate-outcomes",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulate outcomes",
    "text": "Simulate outcomes\n\nadmissions_sim &lt;- admissions_sim |&gt;\n  mutate(\n    y = rbinom(10000, size = 5, prob = pi)\n  )\n\nadmissions_sim |&gt; head(5)\n\n# A tibble: 5 × 2\n     pi     y\n  &lt;dbl&gt; &lt;int&gt;\n1   0.2     1\n2   0.2     2\n3   0.2     0\n4   0.4     0\n5   0.2     0"
  },
  {
    "objectID": "slides/slides03.html#simulated-y-outcomes-approximate-fypi",
    "href": "slides/slides03.html#simulated-y-outcomes-approximate-fypi",
    "title": "Likelihood review + Posterior Simulation",
    "section": "Simulated y outcomes approximate \\(f(y|\\pi)\\)",
    "text": "Simulated y outcomes approximate \\(f(y|\\pi)\\)\n\nggplot(admissions_sim, aes(x = y)) +\n  geom_bar(aes(y = after_stat(prop))) + \n  facet_wrap(vars(pi))"
  },
  {
    "objectID": "slides/slides03.html#to-approximate-posterior-condition-on-observed-data-and-look-at-relative-frequency",
    "href": "slides/slides03.html#to-approximate-posterior-condition-on-observed-data-and-look-at-relative-frequency",
    "title": "Likelihood review + Posterior Simulation",
    "section": "To approximate posterior, condition on observed data and look at relative frequency",
    "text": "To approximate posterior, condition on observed data and look at relative frequency\n\nobserved &lt;- admissions_sim |&gt;\n  filter(y == 3)\n\nobserved |&gt;\n  tabyl(pi)\n\n  pi   n   percent\n 0.2 365 0.3628231\n 0.4 445 0.4423459\n 0.8 196 0.1948310"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#columns",
    "href": "slides/slides-cheat-sheet.html#columns",
    "title": "Slides Cheat Sheet",
    "section": "Columns",
    "text": "Columns\n\n\n\nFirst year at Carleton!\nTaught at Swarthmore for 5 years before moving here this fall\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#only-graph-from-code",
    "href": "slides/slides-cheat-sheet.html#only-graph-from-code",
    "title": "Slides Cheat Sheet",
    "section": "Only graph from code",
    "text": "Only graph from code"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#background-image-half",
    "href": "slides/slides-cheat-sheet.html#background-image-half",
    "title": "Slides Cheat Sheet",
    "section": "Background Image (half)",
    "text": "Background Image (half)\n\n\nWhat skills do you need?\n\nprogramming with data\nstatistical modeling\ndomain knowledge\ncommunication"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#full-size-image",
    "href": "slides/slides-cheat-sheet.html#full-size-image",
    "title": "Slides Cheat Sheet",
    "section": "Full size image",
    "text": "Full size image\n\n\n\nImage by Adam Loy  adapted from work of Joe Blitzstein, Hanspeter Pfister, and Hadley Wickham"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#background-image-full-size",
    "href": "slides/slides-cheat-sheet.html#background-image-full-size",
    "title": "Slides Cheat Sheet",
    "section": "Background image (full size)",
    "text": "Background image (full size)"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#countdown",
    "href": "slides/slides-cheat-sheet.html#countdown",
    "title": "Slides Cheat Sheet",
    "section": "Countdown",
    "text": "Countdown\n\nWith your neighbor(s):\nChoose two countries to compare to the U.S. voting record in the U.N. over the years.\nWhat did you learn?\n\n\n\n\n−+\n04:00"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#table-column-widths",
    "href": "slides/slides-cheat-sheet.html#table-column-widths",
    "title": "Slides Cheat Sheet",
    "section": "table column widths",
    "text": "table column widths\n\n\n\n\n\n\n\n\nCollaboration Allowed\n\n\n\n\nHomework Problems\nYou are allowed and encouraged to collaborate on homework. You may also use outside resources, but your submitted work must be your own and reflect your own understanding .\n\n\nLab Quiz Problems\nNo collaboration is allowed at all . You may use your own notes for resubmissions, but should not use outside resources.\n\n\nPortfolio Projects\nYou are expected to collaborate with your group, but cannot rely on external sources other than to help motivate the questions or provide other background information. Getting answers on significant parts of solutions from outside resources is not allowed.\n\n\nFinal Project\nYou are expected to collaborate with your group, but cannot rely on external sources other than to help motivate the questions or provide other background information. Any outside resources should be properly cited."
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#use-of-generative-artificial-intelligence-ai",
    "href": "slides/slides-cheat-sheet.html#use-of-generative-artificial-intelligence-ai",
    "title": "Slides Cheat Sheet",
    "section": "Use of generative artificial intelligence (AI)",
    "text": "Use of generative artificial intelligence (AI)\n\nTreat generative AI, such as ChatGPT or Gemini, the same as other online resources.\nGuiding principles:\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. AI should facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n❌ AI tools for writing code: You may not use generative AI to take a “first pass” at a coding task. Do not type coursework prompts directly into AI tools.\n✅ AI tools for debugging code: You may make use of the technology to get help with error messages or trying to fix issues\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\n\n\nAdapted from Mine Çetinkaya-Rundel"
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#linktask",
    "href": "slides/slides-cheat-sheet.html#linktask",
    "title": "Slides Cheat Sheet",
    "section": "Link/task",
    "text": "Link/task\n\nhttps://github.com/stat220-w25\n\n\nFill out the Welcome Survey for collection of your account names, later this week you will be invited to the course organization."
  },
  {
    "objectID": "slides/slides-cheat-sheet.html#handwriting-font",
    "href": "slides/slides-cheat-sheet.html#handwriting-font",
    "title": "Slides Cheat Sheet",
    "section": "handwriting font",
    "text": "handwriting font\n\nin case you don’t yet have a GitHub account…"
  },
  {
    "objectID": "slides/slides08.html#recap-conjugate-models",
    "href": "slides/slides08.html#recap-conjugate-models",
    "title": "Intro to Posterior Approximation",
    "section": "Recap: Conjugate models",
    "text": "Recap: Conjugate models"
  },
  {
    "objectID": "slides/slides08.html#motivation",
    "href": "slides/slides08.html#motivation",
    "title": "Intro to Posterior Approximation",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/slides08.html#section-1",
    "href": "slides/slides08.html#section-1",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Grid Approximation\n\n\nGrid approximation produces a sample of \\(N\\) independent \\(\\theta\\) values, \\(\\left\\lbrace \\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(N)} \\right\\rbrace\\), from a discretized approximation of posterior pdf \\(f(\\theta|y)\\). This algorithm evolves in four steps:\n\nDefine a discrete grid of possible \\(\\theta\\) values.\nEvaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\nObtain a discrete approximation of posterior pdf \\(f(\\theta |y)\\) by:\n\n\ncalculating the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value\nnormalizing the products so that they sum to 1 across all \\(\\theta\\).\n\n\nRandomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "slides/slides08.html#example-beta-binomial",
    "href": "slides/slides08.html#example-beta-binomial",
    "title": "Intro to Posterior Approximation",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\nDiscretize \\(\\pi\\):\n\\[\\pi \\in \\{0, .2, .4, .6, .8, 1.0\\}\\]"
  },
  {
    "objectID": "slides/slides08.html#define-a-grid-of-possible-theta-values",
    "href": "slides/slides08.html#define-a-grid-of-possible-theta-values",
    "title": "Intro to Posterior Approximation",
    "section": "1. Define a grid of possible \\(\\theta\\) values",
    "text": "1. Define a grid of possible \\(\\theta\\) values\n\\[\\pi \\in \\{0, .2, .4, .6, .8, 1.0\\}\\]\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))\n\ngrid_data\n\n# A tibble: 6 × 1\n  pi_grid\n    &lt;dbl&gt;\n1     0  \n2     0.2\n3     0.4\n4     0.6\n5     0.8\n6     1"
  },
  {
    "objectID": "slides/slides08.html#evaluate-the-prior-pdf-ftheta-and-likelihood-function-lthetay-at-each-theta-grid-value.",
    "href": "slides/slides08.html#evaluate-the-prior-pdf-ftheta-and-likelihood-function-lthetay-at-each-theta-grid-value.",
    "title": "Intro to Posterior Approximation",
    "section": "2. Evaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.",
    "text": "2. Evaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n\n\n\n# A tibble: 6 × 3\n  pi_grid prior likelihood\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1     0    0    0         \n2     0.2  0.96 0.00000410\n3     0.4  1.44 0.00157   \n4     0.6  1.44 0.0403    \n5     0.8  0.96 0.268     \n6     1    0    0"
  },
  {
    "objectID": "slides/slides08.html#a.-calculate-the-product-fthetalthetay-at-each-theta-grid-value",
    "href": "slides/slides08.html#a.-calculate-the-product-fthetalthetay-at-each-theta-grid-value",
    "title": "Intro to Posterior Approximation",
    "section": "3a. Calculate the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value",
    "text": "3a. Calculate the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(\n    unnorm_post = likelihood * prior\n  )\n\n\n\n# A tibble: 6 × 4\n  pi_grid prior likelihood unnorm_post\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     0    0    0           0         \n2     0.2  0.96 0.00000410  0.00000393\n3     0.4  1.44 0.00157     0.00226   \n4     0.6  1.44 0.0403      0.0580    \n5     0.8  0.96 0.268       0.258     \n6     1    0    0           0"
  },
  {
    "objectID": "slides/slides08.html#b.-normalize-this-product",
    "href": "slides/slides08.html#b.-normalize-this-product",
    "title": "Intro to Posterior Approximation",
    "section": "3b. Normalize this product",
    "text": "3b. Normalize this product\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(\n    posterior = unnorm_post / sum(unnorm_post)\n  )\n\n\n\n# A tibble: 6 × 5\n  pi_grid prior likelihood unnorm_post posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1     0    0    0           0          0        \n2     0.2  0.96 0.00000410  0.00000393 0.0000124\n3     0.4  1.44 0.00157     0.00226    0.00712  \n4     0.6  1.44 0.0403      0.0580     0.183    \n5     0.8  0.96 0.268       0.258      0.810    \n6     1    0    0           0          0"
  },
  {
    "objectID": "slides/slides08.html#pause-to-check-does-it-sum-to-1",
    "href": "slides/slides08.html#pause-to-check-does-it-sum-to-1",
    "title": "Intro to Posterior Approximation",
    "section": "Pause to check: does it sum to 1?",
    "text": "Pause to check: does it sum to 1?\n\n# Confirm that the posterior approximation sums to 1\ngrid_data %&gt;% \n  summarize(sum(unnorm_post), sum(posterior))\n\n# A tibble: 1 × 2\n  `sum(unnorm_post)` `sum(posterior)`\n               &lt;dbl&gt;            &lt;dbl&gt;\n1              0.318                1"
  },
  {
    "objectID": "slides/slides08.html#randomly-sample-n-theta-grid-values-with-respect-to-their-corresponding-normalized-posterior-probabilities.",
    "href": "slides/slides08.html#randomly-sample-n-theta-grid-values-with-respect-to-their-corresponding-normalized-posterior-probabilities.",
    "title": "Intro to Posterior Approximation",
    "section": "4. Randomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities.",
    "text": "4. Randomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities.\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\n\n\n# A tibble: 10,000 × 5\n   pi_grid prior likelihood unnorm_post posterior\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1     0.8  0.96     0.268       0.258      0.810\n 2     0.8  0.96     0.268       0.258      0.810\n 3     0.8  0.96     0.268       0.258      0.810\n 4     0.6  1.44     0.0403      0.0580     0.183\n 5     0.8  0.96     0.268       0.258      0.810\n 6     0.8  0.96     0.268       0.258      0.810\n 7     0.8  0.96     0.268       0.258      0.810\n 8     0.8  0.96     0.268       0.258      0.810\n 9     0.8  0.96     0.268       0.258      0.810\n10     0.8  0.96     0.268       0.258      0.810\n# ℹ 9,990 more rows"
  },
  {
    "objectID": "slides/slides08.html#section-2",
    "href": "slides/slides08.html#section-2",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "# Histogram of the grid simulation with posterior pdf\nggplot(post_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), color = \"white\") + \n  stat_function(fun = dbeta, args = list(11, 3)) + \n  lims(x = c(0, 1))"
  },
  {
    "objectID": "slides/slides08.html#section-4",
    "href": "slides/slides08.html#section-4",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "# Step 1: Define a grid of 101 pi values\ngrid_data  &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 101))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)"
  },
  {
    "objectID": "slides/slides08.html#what-does-grid-approximation-give-us-that-conjugacy-doesnt",
    "href": "slides/slides08.html#what-does-grid-approximation-give-us-that-conjugacy-doesnt",
    "title": "Intro to Posterior Approximation",
    "section": "What does grid approximation give us that conjugacy doesn’t?",
    "text": "What does grid approximation give us that conjugacy doesn’t?\nRemember our non-conjugate prior: \\(f(\\pi) = e - e^\\pi\\)?"
  },
  {
    "objectID": "slides/slides08.html#section-6",
    "href": "slides/slides08.html#section-6",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "grid_data  &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 101))\n\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = exp(1) - exp(pi_grid),\n         likelihood = dbinom(9, 10, pi_grid))\n\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\nset.seed(84735)\n\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)"
  },
  {
    "objectID": "slides/slides08.html#where-does-grid-approximation-start-to-fail",
    "href": "slides/slides08.html#where-does-grid-approximation-start-to-fail",
    "title": "Intro to Posterior Approximation",
    "section": "Where does grid approximation start to fail?",
    "text": "Where does grid approximation start to fail?"
  },
  {
    "objectID": "slides/slides08.html#plan",
    "href": "slides/slides08.html#plan",
    "title": "Intro to Posterior Approximation",
    "section": "Plan",
    "text": "Plan\n\nToday: how to find them with {rstan}\nFriday: how to evaluate them\nNext week: how they work"
  },
  {
    "objectID": "slides/slides08.html#section-8",
    "href": "slides/slides08.html#section-8",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Markov Chain Monte Carlo (MCMC)\n\n\nMCMC is a class of algorithms used to approximate a probability distribution.\nLet \\(\\{\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n)} \\}\\) be an N-length MCMC sample. Then, \\(\\theta^{(i+1)}\\) is drawn from a model with conditional pdf\n\\[f(\\theta^{(i+1)}|\\theta^{(i)}, y)\\]\n\n\n\nNote that:\n\nMarkov property\nNot equivalent to posterior pdf\nIf done well, approximates posterior pdf"
  },
  {
    "objectID": "slides/slides08.html#rstan",
    "href": "slides/slides08.html#rstan",
    "title": "Intro to Posterior Approximation",
    "section": "{rstan}",
    "text": "{rstan}\n\nlibrary(rstan)\n\nStan is a probabilistic programming language written in C++ (https://mc-stan.org/)\n{rstan} is the R interface to Stan\nThere are also interfaces for python, Julia, Mathematica, MATLAB, etc.\n\n\n\n\n\n\nNote\n\n\nSince {rstan} uses C++ under the hood, you might need to follow some additional steps to install the package. Follow the directions at https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started carefully!"
  },
  {
    "objectID": "slides/slides08.html#steps",
    "href": "slides/slides08.html#steps",
    "title": "Intro to Posterior Approximation",
    "section": "Steps",
    "text": "Steps\n\nWrite out the model in {rstan syntax}\n\ndata\nparameters\nmodel\n\nSample from the approximate posterior"
  },
  {
    "objectID": "slides/slides08.html#beta-binomial-example",
    "href": "slides/slides08.html#beta-binomial-example",
    "title": "Intro to Posterior Approximation",
    "section": "Beta-Binomial Example",
    "text": "Beta-Binomial Example\nDefine the model:\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\""
  },
  {
    "objectID": "slides/slides08.html#beta-binomial-example-1",
    "href": "slides/slides08.html#beta-binomial-example-1",
    "title": "Intro to Posterior Approximation",
    "section": "Beta-Binomial Example",
    "text": "Beta-Binomial Example\nSample from the posterior:\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 4:                0.081 seconds (Sampling)\nChain 4:                0.107 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "slides/slides08.html#section-9",
    "href": "slides/slides08.html#section-9",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "bb_sim\n\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi    0.78    0.00 0.11  0.55  0.72  0.80  0.86  0.95  6587    1\nlp__ -7.80    0.01 0.73 -9.87 -7.98 -7.52 -7.33 -7.27  6462    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct  1 08:13:19 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/slides08.html#section-10",
    "href": "slides/slides08.html#section-10",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "Burn-in\n\n\nIf you’ve ever made a batch of pancakes or crêpes, you know that the first pancake is always the worst – the pan isn’t yet at the perfect temperature, you haven’t yet figured out how much batter to use, and you need more time to practice your flipping technique. MCMC chains are similar. Without direct knowledge of the posterior it’s trying to simulate, the Markov chain might start out sampling unreasonable values of our parameter of interest. Eventually though, it learns and starts producing values that mimic a random sample from the posterior. And just as we might need to toss out the first pancake, we might want to toss the Markov chain values produced during this learning period – keeping them in our sample might lead to a poor posterior approximation. As such, “burn-in” is the practice of discarding the first portion of Markov chain values."
  },
  {
    "objectID": "slides/slides08.html#section-11",
    "href": "slides/slides08.html#section-11",
    "title": "Intro to Posterior Approximation",
    "section": "",
    "text": "as.array(bb_sim, pars = \"pi\") |&gt;\n  head(5)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.9288874 0.8637566 0.6410152 0.8054958\n      [2,] 0.8710291 0.6737700 0.6861136 0.8727761\n      [3,] 0.8105749 0.7752253 0.6994077 0.8625163\n      [4,] 0.7567798 0.5318069 0.8200739 0.8065095\n      [5,] 0.7410937 0.7004429 0.8200739 0.8065095"
  },
  {
    "objectID": "slides/slides08.html#plotting-with-the-bayesplot-package",
    "href": "slides/slides08.html#plotting-with-the-bayesplot-package",
    "title": "Intro to Posterior Approximation",
    "section": "Plotting with the {bayesplot} package",
    "text": "Plotting with the {bayesplot} package\n\n\nlibrary(bayesplot)\n\n# Histogram of the Markov chain values\nmcmc_hist(bb_sim, \n          pars = \"pi\") + \n  yaxis_text(TRUE) + \n  ylab(\"count\")"
  },
  {
    "objectID": "slides/slides08.html#plotting-with-the-bayesplot-package-1",
    "href": "slides/slides08.html#plotting-with-the-bayesplot-package-1",
    "title": "Intro to Posterior Approximation",
    "section": "Plotting with the {bayesplot} package",
    "text": "Plotting with the {bayesplot} package\n\n# Density plot of the Markov chain values\nmcmc_dens(bb_sim, \n                pars = \"pi\") + \n  yaxis_text(TRUE) + \n  ylab(\"density\")"
  },
  {
    "objectID": "projects/proj1.html",
    "href": "projects/proj1.html",
    "title": "Project 1",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\nlibrary(rstan) # for MCMC\nlibrary(bayesplot) # for plotting\n\nThe data in fish_poisson.csv are records of visits to a National Park and comes from the {rethinking} R package. The variable fish_caught gives the number of fish that was caught by a park visitor, and the hours variables gives the total number of hours that they spent in the park. Your task is to estimate the hourly average of fish caught for an individual while they are fishing. You can assume that the hourly number of fish caught is a Poisson distribution.\nHowever, the problem is that not every party tried to fish. So there are two ways that a zero can arise in the data: (1) the individual tried to fish but did not catch any, or (2) the individual did not try to fish at all. If we do not account for this, we’ll end up with an underestimate for fish extraction from the park.\nLet \\(\\theta\\) be the probability that a visitor does not fish. Let \\(\\lambda\\) be the mean number of fish caught, when a visitor is fishing. Then \\(Y|\\lambda \\sim \\text{Pois}(\\lambda)\\).\nThen, the probability of observing zero fish caught is:\n\\[\\theta + (1-\\theta) P(Y = 0 | \\lambda)\\]\nand the probability of observing a nonzero number of fish caught is:\n\\[(1-\\theta) P(Y=y | \\lambda)\\]\nThis is an example of a zero-inflated poisson distribution, and you will implement it for this case study project.\n(Note: Your model should also account for the differing amount of time individuals spend in the park.)\n\nfish_poisson &lt;- read_csv(\"../data/fish_poisson.csv\")\n\nFinding the posterior\nOption 1: Find the posterior distribution of \\(\\lambda\\) analytically\nOption 2: Implement your own MCMC sampler or grid approximation to estimate the posterior distribution of \\(\\lambda\\)\nOption 3: Use the stan starter code below. If you choose this option, you should also answer Question (1) below.\ndata {\n  int&lt;lower=1&gt; N;\n  array[N] int&lt;lower=0&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta; // Pr(y = 0)\n  real&lt;lower=0&gt; lambda; // Poisson rate parameter (if y &gt; 0)\n}\nmodel {\n  lambda ~ _____________ ;\n  theta ~ ______________ ;\n\n  for (n in 1 : N) {\n    if (y[n] == 0) {\n      target += log_sum_exp(bernoulli_lpmf(1 | theta),\n                            bernoulli_lpmf(0 | theta)\n                              + poisson_lpmf(y[n] | lambda)); // log(Pr(y = 0))\n    } else {\n      target += bernoulli_lpmf(0 | theta)\n                  + poisson_lpmf(y[n] | lambda); // log(Pr(y &gt; 0))\n      y[n] ~ __________ ; // truncated poisson\n    }\n  }\n}\nQuestions to Consider\n\nThe target += lines in the stan code update the log posterior density for each run through the for loop. We need to include them when we do not use an explicit modeling statement (eg y ~ normal(0,1)). Explain or justify these lines of code. (You may have to dig into the stan reference manual)\nPerform an appropriate EDA to justify the use of a zero-inflated poisson analysis\nHow did you choose your prior distributions? Confirm they are reasonable through simulation.\n(If estimated with MCMC): Are you confident that your MCMC has converged? Provide justification\nWhat do you learn from \\(\\lambda\\)? What do you learn from \\(\\theta\\)? (these should be in context of the data)\nHow is your answer different than a “regular” Bayesian posterior analysis of \\(\\lambda\\)? (ie non zero-inflated)\nWhat would your model predict for a new visitor at the park?\nWhat are the benefits and drawbacks of using a fully specified Bayesian model in this scenario? What do we gain instead of, e.g., splitting up the data into zero fish caught and nonzero fish caught?\nReport\nYour submission should be a &lt;4 page report written in quarto/rmarkdown. Everything should be typed/proofread/labeled/captioned etc.\nEvaluation\nThere are two elements that make this a “project” rather than a “homework problem”:\n\nIt is more open-ended.\n\nIf you think an alternative approach is more appropriate for this data, go for it! There is no “right answer” that I am looking for. However, there are lots of ways to be wrong. You should think through your choices and provide justification for them.\nOn homework, you often have an example from class or the textbook to use as a “model”. On this project, I’m asking you to do something new!\n\n\nPart of your grade will be based on the presentation of your work. All portions of the report should be polished and proofread carefully.",
    "crumbs": [
      "Projects",
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 340: Bayesian Statistics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the term. Note that this schedule will be updated as the term progresses and the timeline of topics and assignments might be updated. Any major changes to due dates will be announced in class and indicated in bold here.\n\n\n\n\n\n\nWEEK\nDOW\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nM\nMon, Sep 15\n📝complete welcome survey  📚 Bayes Rules Ch1  💻 Join Slack workspace\n\nIntro to Bayesian Thinking\nSlides01\n\n\n\n1\nW\nWed, Sep 17\n📚 Bayes Rules Ch2  💻 Complete computing setup\n\nBayes Rule, R Setup\nSlides02\n\n\n\n1\nF\nFri, Sep 19\n\nProbability and Likelihood Review; Posterior Simulation\nSlides03\n\nHW 1 and GW 1\n\n\n\n2\nM\nMon, Sep 22\n📚 Bayes Rules Ch3 \n\nBeta-Binomial Model\nSlides04\n\n\n\n2\nW\nWed, Sep 24\n📚 Bayes Rules Ch4 \n\nBalance and Sequentiality\nSlides05\n\n\n\n2\nF\nFri, Sep 26\n📚 Bayes Rules Ch5.1-2 \n\nGamma-Gamma\nSlides06\n\nHW 2 and GW 2\n\n\n\n3\nM\nMon, Sep 29\n📚 Bayes Rules Ch5.3-5.6  📝 complete group eval \n\nNormal-Normal\nSlides07\n\n\n\n3\nW\nWed, Oct 1\n📚 Bayes Rules Ch6.1-2 \n\nGrid approximation and Markov Chains\nSlides08\n\n\n\n3\nF\nFri, Oct 3\n📚 Bayes Rules Ch6.3 \n\nDiagnostics\nSlides09\n\nHW 3 and GW 3\n\n\n\n4\nM\nMon, Oct 6\n📚 Bayes Rules Ch7.1-7.3  📝 complete group eval \n\nMCMC I\nSlides10\nHand out computing project\n\n\n4\nW\nWed, Oct 8\n📚 Bayes Rules Ch7.4-7 \n\nMCMC II\nSlides11\n\n\n\n4\nF\nFri, Oct 10\n\nExam I\n\n\nHW 4 and GW 4\n\n\n\n5\nM\nMon, Oct 13\n📚 Bayes Rules Ch8.1-8.2  📝 complete group eval (includes project partner question) \n\nPosterior Inference\n\n\n\n\n5\nW\nWed, Oct 15\n📚 Bayes Rules Ch8.3-8.5  📝 complete anonymous midterm check-in\n\nPosterior Prediction\ntbd\n\n\n\n5\nF\nFri, Oct 17\n📚 Bayes Rules Ch9 \n\nRegression\ntbd\nHW5\n\n\n6\nM\nMon, Oct 20\n\n❌ Midterm Break; no class\ntbd\n\n\n\n6\nW\nWed, Oct 22\n\nEvaluating I\ntbd\nComputing Project Due\n\n\n6\nF\nFri, Oct 24\n\nEvaluating II\ntbd\nHW6\n\n\n7\nM\nMon, Oct 27\n\nExtending Regression Model\ntbd\n\n\n\n7\nW\nWed, Oct 29\n\nPoisson Regression\ntbd\n\n\n\n7\nF\nFri, Oct 31\n\nNegative Binomial Regression\ntbd\nHW7\n\n\n8\nM\nMon, Nov 3\n\nLogistic Regression\ntbd\n\n\n\n8\nW\nWed, Nov 5\n\nNaive Bayes Classification\ntbd\n\n\n\n8\nF\nFri, Nov 7\n\nExam II\ntbd\nHW8\n\n\n9\nM\nMon, Nov 10\n\nHierarchical Models I\ntbd\n\n\n\n9\nW\nWed, Nov 12\n\nHierarchical Models II\ntbd\n\n\n\n9\nF\nFri, Nov 14\n\nHierarchical Models III\ntbd\n\n\n\n10\nM\nMon, Nov 17\n\nProject work day\ntbd\nHW 9\n\n\n10\nW\nWed, Nov 19\n\nLightning talks\ntbd\n\n\n\n\nM\nFri, Nov 21\n\nFinal Project Due @ noon\ntbd",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "slides/slides09.html#markov-chain-monte-carlo",
    "href": "slides/slides09.html#markov-chain-monte-carlo",
    "title": "Care and feeding of your MCMCs",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nA MCMC sample \\(\\{ \\theta^{(1)}, \\theta^{(1)}, ..., \\theta^{(N)}\\}\\) is a sequence of values drawn from conditional pdf:\n\\[f(\\theta^{(i+1)}|\\theta^{(i)}, y)\\] when done well, it converges approximately to the posterior \\(f(\\theta |y)\\)"
  },
  {
    "objectID": "slides/slides09.html#mcmc-plan",
    "href": "slides/slides09.html#mcmc-plan",
    "title": "Care and feeding of your MCMCs",
    "section": "MCMC plan",
    "text": "MCMC plan\n\nYesterday: how to find them with {rstan}\nToday: how to evaluate them\nNext week: how they work"
  },
  {
    "objectID": "slides/slides09.html#beta-binomial-example-in-rstan",
    "href": "slides/slides09.html#beta-binomial-example-in-rstan",
    "title": "Care and feeding of your MCMCs",
    "section": "Beta-Binomial Example in {rstan}",
    "text": "Beta-Binomial Example in {rstan}\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    real&lt;lower=0&gt; alpha;\n    real&lt;lower=0&gt; beta;\n    int&lt;lower=1&gt; n;\n    int&lt;lower=0, upper=n&gt; Y;\n  }\n\n  parameters {\n    real&lt;lower=0, upper=1&gt; pi;\n  }\n\n  model {\n    Y ~ binomial(n, pi);\n    pi ~ beta(alpha, beta);\n  }\n\"\n\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, \n               data = list(alpha = 2, beta = 2, Y = 9, n = 10), \n               chains = 4, iter = 1000*2, seed = 84735)"
  },
  {
    "objectID": "slides/slides09.html#section",
    "href": "slides/slides09.html#section",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "bb_sim\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n      mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff Rhat\npi    0.79    0.00 0.11   0.55  0.72  0.80  0.87  0.95  1304    1\nlp__ -7.85    0.02 0.80 -10.13 -8.05 -7.53 -7.33 -7.27  1419    1\n\nSamples were drawn using NUTS(diag_e) at Thu Oct  2 20:42:34 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "slides/slides09.html#section-1",
    "href": "slides/slides09.html#section-1",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "mcmc_dens(bb_sim, pars = \"pi\") +\n  stat_function(fun = dbeta, args = list(11, 3), col = \"darkorange\", linewidth = 2)"
  },
  {
    "objectID": "slides/slides09.html#in-practice-we-dont-know-the-true-posterior",
    "href": "slides/slides09.html#in-practice-we-dont-know-the-true-posterior",
    "title": "Care and feeding of your MCMCs",
    "section": "In practice, we don’t know the true posterior",
    "text": "In practice, we don’t know the true posterior\n\nWhat does a good markov chain look like?\nHow can we tell if we are getting a reasonable approximation of the posterior?\nHow big should our Markov chain size be?\n\n\n\nSome examples from today are from Statistical Rethinking"
  },
  {
    "objectID": "slides/slides09.html#traceplots",
    "href": "slides/slides09.html#traceplots",
    "title": "Care and feeding of your MCMCs",
    "section": "Traceplots",
    "text": "Traceplots\n\n\n\nmcmc_trace(bb_sim, pars = \"pi\")\n\n\n\n\n\n\n\n\n\n\nIdeally, \\(\\theta^{(i+1)}\\) does not depend on \\(\\theta^{(i)}\\) too much\nShould look like a “fat hairy caterpillar”"
  },
  {
    "objectID": "slides/slides09.html#first-20-draws-burn-in-warmup",
    "href": "slides/slides09.html#first-20-draws-burn-in-warmup",
    "title": "Care and feeding of your MCMCs",
    "section": "First 20 draws (burn-in / warmup)",
    "text": "First 20 draws (burn-in / warmup)"
  },
  {
    "objectID": "slides/slides09.html#first-200-draws-burn-in-warmup",
    "href": "slides/slides09.html#first-200-draws-burn-in-warmup",
    "title": "Care and feeding of your MCMCs",
    "section": "First 200 draws (burn-in / warmup)",
    "text": "First 200 draws (burn-in / warmup)"
  },
  {
    "objectID": "slides/slides09.html#mixing-slowly",
    "href": "slides/slides09.html#mixing-slowly",
    "title": "Care and feeding of your MCMCs",
    "section": "“Mixing slowly”",
    "text": "“Mixing slowly”"
  },
  {
    "objectID": "slides/slides09.html#getting-stuck",
    "href": "slides/slides09.html#getting-stuck",
    "title": "Care and feeding of your MCMCs",
    "section": "“Getting Stuck”",
    "text": "“Getting Stuck”"
  },
  {
    "objectID": "slides/slides09.html#each-chain-should-converge-on-roughly-the-same-distribution",
    "href": "slides/slides09.html#each-chain-should-converge-on-roughly-the-same-distribution",
    "title": "Care and feeding of your MCMCs",
    "section": "Each chain should converge on roughly the same distribution",
    "text": "Each chain should converge on roughly the same distribution\n\nmcmc_dens_overlay(bb_sim, pars = \"pi\")"
  },
  {
    "objectID": "slides/slides09.html#how-many-chains-do-you-need",
    "href": "slides/slides09.html#how-many-chains-do-you-need",
    "title": "Care and feeding of your MCMCs",
    "section": "How many chains do you need?",
    "text": "How many chains do you need?\n\nWhen testing and debugging your model, use 1\nWhen deciding whether your chains are valid, need &gt;1\nWhen you do your final run that you will make inferences from, one long chain or multiple chains is fine\n\n\n\n\n\n\n\nRule of thumb:\n\n\nOne (short) chain to debug, four chains for verification and inference"
  },
  {
    "objectID": "slides/slides09.html#autocorrelation",
    "href": "slides/slides09.html#autocorrelation",
    "title": "Care and feeding of your MCMCs",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmcmc_acf(bb_sim, pars = \"pi\")"
  },
  {
    "objectID": "slides/slides09.html#slow-mixing-chains-often-have-high-autocorrelation",
    "href": "slides/slides09.html#slow-mixing-chains-often-have-high-autocorrelation",
    "title": "Care and feeding of your MCMCs",
    "section": "Slow mixing chains often have high autocorrelation",
    "text": "Slow mixing chains often have high autocorrelation"
  },
  {
    "objectID": "slides/slides09.html#effective-sample-size",
    "href": "slides/slides09.html#effective-sample-size",
    "title": "Care and feeding of your MCMCs",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\n\n\n\n\n\n\\(N_{eff}\\)\n\n\nEffective Sample Size (\\(N_{eff}\\)) quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation\n\n\n\n\n\n\n\n\n\n\\(N_{eff}/N\\)\n\n\nEffective sample size ratio\n\n\n\n\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3260274"
  },
  {
    "objectID": "slides/slides09.html#r-hat",
    "href": "slides/slides09.html#r-hat",
    "title": "Care and feeding of your MCMCs",
    "section": "R-hat",
    "text": "R-hat\nIdea: In a “good” Markov chain, the variability across all chains is similar to the variability within a given chain. In a “bad” Markov chain, the variability across all chains is bigger than the variability within a given chain"
  },
  {
    "objectID": "slides/slides09.html#r-hat-1",
    "href": "slides/slides09.html#r-hat-1",
    "title": "Care and feeding of your MCMCs",
    "section": "R-hat",
    "text": "R-hat\n\n\n\n\n\n\nGelman-Rubin convergence diagnostic\n\n\nR-hat \\(\\approx \\sqrt{\\frac{\\text{Var}_{\\text{combined}}}{\\text{Var}_{\\text{within}}}}\\)\nR-hat \\(\\approx\\) 1 is good; R-hat \\(&gt;1\\) indicates the chains have not converged\n\n\n\n\nrhat(bb_sim)\n\n       pi      lp__ \n0.9999668 1.0012277"
  },
  {
    "objectID": "slides/slides09.html#section-2",
    "href": "slides/slides09.html#section-2",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "fn_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 1000);\n    sigma ~ exponential(.0001);\n  }\n\""
  },
  {
    "objectID": "slides/slides09.html#section-3",
    "href": "slides/slides09.html#section-3",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "# STEP 2: SIMULATE the posterior\nfn_sim &lt;- stan(model_code = fn_model, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\nWarning :There were 521 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nWarning :Examine the pairs() plot to diagnose sampling problems\n\nWarning :Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\nWarning :Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess"
  },
  {
    "objectID": "slides/slides09.html#mu",
    "href": "slides/slides09.html#mu",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\mu\\)",
    "text": "\\(\\mu\\)\nmcmc_trace(fn_sim, pars = \"mu\")\nmcmc_dens_overlay(fn_sim, pars = \"mu\")"
  },
  {
    "objectID": "slides/slides09.html#sigma",
    "href": "slides/slides09.html#sigma",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\sigma\\)",
    "text": "\\(\\sigma\\)\nmcmc_trace(fn_sim, pars = \"sigma\")\nmcmc_dens_overlay(fn_sim, pars = \"sigma\")"
  },
  {
    "objectID": "slides/slides09.html#section-4",
    "href": "slides/slides09.html#section-4",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "fn_model2 &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 10);\n    sigma ~ exponential(1);\n  }\n\""
  },
  {
    "objectID": "slides/slides09.html#section-5",
    "href": "slides/slides09.html#section-5",
    "title": "Care and feeding of your MCMCs",
    "section": "",
    "text": "# STEP 2: SIMULATE the posterior\nfn_sim2 &lt;- stan(model_code = fn_model2, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\nWarning :There were 3 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nWarning :Examine the pairs() plot to diagnose sampling problems"
  },
  {
    "objectID": "slides/slides09.html#mu-1",
    "href": "slides/slides09.html#mu-1",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\mu\\)",
    "text": "\\(\\mu\\)\nmcmc_trace(fn_sim2, pars = \"mu\")\nmcmc_dens_overlay(fn_sim2, pars = \"mu\")"
  },
  {
    "objectID": "slides/slides09.html#sigma-1",
    "href": "slides/slides09.html#sigma-1",
    "title": "Care and feeding of your MCMCs",
    "section": "\\(\\sigma\\)",
    "text": "\\(\\sigma\\)\nmcmc_trace(fn_sim2, pars = \"sigma\")\nmcmc_dens_overlay(fn_sim2, pars = \"sigma\")"
  },
  {
    "objectID": "slides/slides02.html#warm-up-i-bayesian-personality-quiz",
    "href": "slides/slides02.html#warm-up-i-bayesian-personality-quiz",
    "title": "Bayes Rule",
    "section": "Warm up I: Bayesian Personality Quiz",
    "text": "Warm up I: Bayesian Personality Quiz\nQ2: An election is coming up and a pollster claims that candidate A has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/slides02.html#warm-up-i-bayesian-personality-quiz-1",
    "href": "slides/slides02.html#warm-up-i-bayesian-personality-quiz-1",
    "title": "Bayes Rule",
    "section": "Warm up I: Bayesian Personality Quiz",
    "text": "Warm up I: Bayesian Personality Quiz\nQ3: Consider two claims.\n\nZuofu claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\nIn light of these experiments, what do you conclude?\n\nYou’re more confident in Kavya’s claim than Zuofu’s claim.\nThe evidence supporting Zuofu’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/slides02.html#warm-up-ii",
    "href": "slides/slides02.html#warm-up-ii",
    "title": "Bayes Rule",
    "section": "Warm Up II",
    "text": "Warm Up II\nThe following are some survey results from a recent Pew Research report to the question “About how often to use the internet”?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\nCalculate the following probabilities and write them using event notation. Let \\(A\\): online almost constantly and \\(B\\): lives in US\n\nP(Online Almost Constantly)\nP(Online Almost Constantly and US)\nP(US given online almost constantly)"
  },
  {
    "objectID": "slides/slides02.html#marginal-probability",
    "href": "slides/slides02.html#marginal-probability",
    "title": "Bayes Rule",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nMarginal Probability"
  },
  {
    "objectID": "slides/slides02.html#joint-probability",
    "href": "slides/slides02.html#joint-probability",
    "title": "Bayes Rule",
    "section": "Joint Probability",
    "text": "Joint Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nJoint Probability"
  },
  {
    "objectID": "slides/slides02.html#conditional-probability",
    "href": "slides/slides02.html#conditional-probability",
    "title": "Bayes Rule",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlmost constantly\nSeveral Times a day\nAbout once a day\nSeveral times a week\nLess often\nDo not use\nTOTAL\n\n\n\n\nUS\n2059\n2159\n301\n150\n100\n201\n4970\n\n\nCanada\n328\n553\n61\n31\n10\n41\n1024\n\n\nTOTAL\n2387\n2712\n362\n181\n110\n242\n5994\n\n\n\n\n\n\n\n\n\nConditional Probability"
  },
  {
    "objectID": "slides/slides02.html#more-probability-notes",
    "href": "slides/slides02.html#more-probability-notes",
    "title": "Bayes Rule",
    "section": "More probability notes:",
    "text": "More probability notes:\n\n\n\n\n\n\nConditional Probability Facts\n\n\n\\(P(A|B) \\ne P(B|A)\\)\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\nComplement rule\n\n\n\\(P(A^c) = 1 - P(A)\\)"
  },
  {
    "objectID": "slides/slides02.html#section",
    "href": "slides/slides02.html#section",
    "title": "Bayes Rule",
    "section": "",
    "text": "I’m trying to determine whether a student response to a homework question is AI generated. I know that AI use among college students in general is high, but my prior is that Carleton students are pretty honest. I would guess only around 2% of submitted answers are AI generated."
  },
  {
    "objectID": "slides/slides02.html#section-1",
    "href": "slides/slides02.html#section-1",
    "title": "Bayes Rule",
    "section": "",
    "text": "LLMs are obsessed with em dashes:\n\nImagine you have an idea or belief about something–like whether it’s going to rain today. Then, you get some new information–like seeing dark clouds in the sky. Bayes’ Rule helps you recalculate how confident you should be in your belief after seeing that new information. Let me know if you want a real-world example (like medical tests or crime investigations)—those make it even clearer.\n\n\n–ChatGPT. Prompt: “Can you summarize Bayes Rule without any math?”"
  },
  {
    "objectID": "slides/slides02.html#section-2",
    "href": "slides/slides02.html#section-2",
    "title": "Bayes Rule",
    "section": "",
    "text": "This suggests a possible rule for determining if an answer is AI generated. I gather some data:\n\nSample 1: 100 AI-generated responses to a homework question\nSample 2: 100 student responses to the same question from pre-2021\n\nI notice that 78% of AI generated responses use an em-dash, but only 5% of student responses used an em-dash.\nIn probability notation:"
  },
  {
    "objectID": "slides/slides02.html#bayesian-knowledge-building-diagram",
    "href": "slides/slides02.html#bayesian-knowledge-building-diagram",
    "title": "Bayes Rule",
    "section": "Bayesian knowledge-building diagram",
    "text": "Bayesian knowledge-building diagram"
  },
  {
    "objectID": "slides/slides02.html#section-3",
    "href": "slides/slides02.html#section-3",
    "title": "Bayes Rule",
    "section": "",
    "text": "Which of the following best describes your posterior “gut check” of whether the answer was AI generated?\n\nThe chance this answer is AI generated drops from 2% to 1%. It’s rare that a Carleton student would submit AI-generated answers.\nThe chance this answer is AI generated jumps from 2% to about 25%. Though em dashes are more common in AI answers, let’s not forget that AI submitted answers are relatively rare for Carleton students\nThe chance this answer is AI generated jumps from 2% to roughly 75%. Given so few student answers use em dashes, this answer is almost certainly AI generated."
  },
  {
    "objectID": "slides/slides02.html#prior-model",
    "href": "slides/slides02.html#prior-model",
    "title": "Bayes Rule",
    "section": "Prior Model",
    "text": "Prior Model"
  },
  {
    "objectID": "slides/slides02.html#likelihood",
    "href": "slides/slides02.html#likelihood",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood\nLet A:\nLet B:\nLooking at the conditional probabilities:\n\\(P(A|B) =\\)\n\\(P(A|B^c) =\\)"
  },
  {
    "objectID": "slides/slides02.html#likelihood-function-notation",
    "href": "slides/slides02.html#likelihood-function-notation",
    "title": "Bayes Rule",
    "section": "Likelihood function notation",
    "text": "Likelihood function notation\n\\[L(\\cdot | A) = \\begin{cases} L(B|A) = P(A|B) \\\\ L(B^c|A) = P(A | B^c) \\end{cases}\\]\nWhen \\(B\\) is known, the conditional probability function \\(P(\\cdot | B)\\) allows us to compare the probabilities of an unknown event, \\(A\\) or \\(A^c\\), occurring with \\(B\\)\nWhen \\(A\\) is known, the likelihood function \\(L(\\cdot | A) = P(A | \\cdot)\\) allows us to evaluate the relative compatibility with data \\(A\\) with events \\(B\\) or \\(B^c\\)"
  },
  {
    "objectID": "slides/slides02.html#posterior-model",
    "href": "slides/slides02.html#posterior-model",
    "title": "Bayes Rule",
    "section": "Posterior Model",
    "text": "Posterior Model"
  },
  {
    "objectID": "slides/slides02.html#posterior-results",
    "href": "slides/slides02.html#posterior-results",
    "title": "Bayes Rule",
    "section": "Posterior Results",
    "text": "Posterior Results\n\\[P(B|A) = \\frac{P(B) L(B|A)}{P(A|B)P(B) + P(A|B^c)P(B^c)}\\] \\[P(B|A) = \\frac{.02 \\times .78}{.02\\times .78 + .98 \\times .05}\\] \\[P(B|A) = \\]"
  },
  {
    "objectID": "slides/slides02.html#the-posterior-distribution",
    "href": "slides/slides02.html#the-posterior-distribution",
    "title": "Bayes Rule",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\\(B\\)\n\\(B^c\\)\nTotal\n\n\n\n\nPrior\n\n\n\n\n\nLikelihood\n\n\n\n\n\nPosterior"
  },
  {
    "objectID": "slides/slides02.html#notation",
    "href": "slides/slides02.html#notation",
    "title": "Bayes Rule",
    "section": "Notation",
    "text": "Notation\nIdea: what if the values we’re interested in can’t be simplified into binary events\n\nGreek letters (\\(\\pi, \\beta, \\mu\\) etc) denote our primary variables of interest (sometimes called parameters)\nCapital letters near the end of the alphabet (\\(X, Y, Z\\)) denote random variables related to our data\nWe denote an observed outcome of \\(Y\\) using lower case \\(y\\)"
  },
  {
    "objectID": "slides/slides02.html#example-phd-admissions",
    "href": "slides/slides02.html#example-phd-admissions",
    "title": "Bayes Rule",
    "section": "Example: PhD admissions",
    "text": "Example: PhD admissions\nLet Y represent a random variable that represents the number of applicants admitted to a PhD program which has received applications from 5 prospective students. That is \\(\\Omega_Y = \\{0, 1, 2, 3, 4, 5\\}\\). We are interested in the parameter \\(\\pi\\) which represents the probability of acceptance to this program. For demonstrative purposes, we will only consider three possible values of \\(\\pi\\) as 0.2, 0.4, and 0.8."
  },
  {
    "objectID": "slides/slides02.html#prior-model-for-pi",
    "href": "slides/slides02.html#prior-model-for-pi",
    "title": "Bayes Rule",
    "section": "Prior model for \\(\\pi\\)",
    "text": "Prior model for \\(\\pi\\)\nYou consult with an expert who knows the specific PhD program well and the following is the prior distribution the expert suggests you use in your analysis.\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n.7\n.2\n.1\n\n\n\n\nExplain what this prior distribution means"
  },
  {
    "objectID": "slides/slides02.html#from-prior-to-posterior",
    "href": "slides/slides02.html#from-prior-to-posterior",
    "title": "Bayes Rule",
    "section": "From prior to posterior",
    "text": "From prior to posterior\n\nFor the two scenarios below, use your intuition to “guesstimate” the posterior (fill out each row based on ~vibes~ )\n\nThe program accepted five of five applicants?\nThe program accepted none of the five applicants?\n\n\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n\n\n\n\n\n(1): \\(f(\\pi | y = 5)\\)\n\n\n\n\n\n(2): \\(f(\\pi | y = 0)\\)"
  },
  {
    "objectID": "slides/slides02.html#intuition-vs-reality",
    "href": "slides/slides02.html#intuition-vs-reality",
    "title": "Bayes Rule",
    "section": "Intuition vs Reality",
    "text": "Intuition vs Reality\nYour intuition may not be Bayesian if:\n\n\n\n\nBayesian statistics is a balancing act! We will take both the prior and the data to get to the posterior. Don’t worry if your intuition was wrong. As we practice more, you will learn to think like a Bayesian."
  },
  {
    "objectID": "slides/slides02.html#the-binomial-model",
    "href": "slides/slides02.html#the-binomial-model",
    "title": "Bayes Rule",
    "section": "The Binomial Model",
    "text": "The Binomial Model\nLet random variable \\(Y\\) be the number of successes in n trials. Assume that the number of trials is fixed, the trials are independent and the probability of success in each trial is \\(\\pi\\). Then, the dependence of \\(Y\\) on \\(\\pi\\) can be modeled by the Binomial model with parameters \\(n\\) and \\(\\pi\\):"
  },
  {
    "objectID": "slides/slides02.html#section-4",
    "href": "slides/slides02.html#section-4",
    "title": "Bayes Rule",
    "section": "",
    "text": "conditional pmf:\n\\[f(y | \\pi) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}\\]\nif \\(\\pi = .2\\) and \\(y = 3\\):"
  },
  {
    "objectID": "slides/slides02.html#section-5",
    "href": "slides/slides02.html#section-5",
    "title": "Bayes Rule",
    "section": "",
    "text": "(or we can use R)\n\ndbinom(3, size = 5, prob = .2)\n\n[1] 0.0512"
  },
  {
    "objectID": "slides/slides02.html#section-6",
    "href": "slides/slides02.html#section-6",
    "title": "Bayes Rule",
    "section": "",
    "text": "Rather than doing this one by one, we can let R consider all different possible values of \\(y\\):\n\ndbinom(0:5, size = 5, prob = .2)\n\n[1] 0.32768 0.40960 0.20480 0.05120 0.00640 0.00032\n\n\n\n\n\n\\(y\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(f(y | \\pi = .2)\\)"
  },
  {
    "objectID": "slides/slides02.html#all-possibilities-for-pi",
    "href": "slides/slides02.html#all-possibilities-for-pi",
    "title": "Bayes Rule",
    "section": "All possibilities for \\(\\pi\\)",
    "text": "All possibilities for \\(\\pi\\)"
  },
  {
    "objectID": "slides/slides02.html#data",
    "href": "slides/slides02.html#data",
    "title": "Bayes Rule",
    "section": "Data",
    "text": "Data\nThe admissions committee has announced they have accepted 3/5 applicants"
  },
  {
    "objectID": "slides/slides02.html#likelihood-1",
    "href": "slides/slides02.html#likelihood-1",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "slides/slides02.html#likelihood-2",
    "href": "slides/slides02.html#likelihood-2",
    "title": "Bayes Rule",
    "section": "Likelihood",
    "text": "Likelihood\n\ndbinom(x = 3, size = 5, prob = 0.2)\n\n[1] 0.0512\n\n\n\ndbinom(x = 3, size = 5, prob = 0.4)\n\n[1] 0.2304\n\n\n\ndbinom(x = 3, size = 5, prob = 0.8)\n\n[1] 0.2048\n\n\n\n\n\nπ\n\n\n0.2\n\n\n0.4\n\n\n0.8\n\n\n\n\nL(π | y = 3)\n\n\n0.0512\n\n\n0.2304\n\n\n0.2048\n\n\n\nThe likelihood function \\(L(\\pi|y=3)\\) is the same as the conditional probability mass function \\(f(y|\\pi)\\) at the observed value \\(y = 3\\)."
  },
  {
    "objectID": "slides/slides02.html#pmf-vs-likelihood",
    "href": "slides/slides02.html#pmf-vs-likelihood",
    "title": "Bayes Rule",
    "section": "pmf vs likelihood",
    "text": "pmf vs likelihood\nWhen \\(\\pi\\) is known, the conditional pmf \\(f(\\cdot | \\pi)\\) allows us to compare the probabilities of different possible values of data \\(Y\\) (eg: \\(y_1\\) or \\(y_2\\)) occurring with \\(\\pi\\):\n\\[f(y_1|\\pi) \\; \\text{ vs } \\; f(y_2|\\pi) \\; .\\]\nWhen \\(Y=y\\) is known, the likelihood function \\(L(\\cdot | y) = f(y | \\cdot)\\) allows us to compare the relative likelihoods of different possible values of \\(\\pi\\) (eg: \\(\\pi_1\\) or \\(\\pi_2\\)) given that we observed data \\(y\\):\n\\[L(\\pi_1|y) \\; \\text{ vs } \\; L(\\pi_2|y) \\; .\\]"
  },
  {
    "objectID": "slides/slides02.html#bayes-rule-from-events-to-random-variables",
    "href": "slides/slides02.html#bayes-rule-from-events-to-random-variables",
    "title": "Bayes Rule",
    "section": "Bayes Rule: from events to random variables",
    "text": "Bayes Rule: from events to random variables\n\\(\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{marginal probability of data}}\\)"
  },
  {
    "objectID": "slides/slides02.html#normalizing-constant",
    "href": "slides/slides02.html#normalizing-constant",
    "title": "Bayes Rule",
    "section": "Normalizing constant",
    "text": "Normalizing constant\nTherefore \\(f(y=3)=\\)\n\ndbinom(3, size = 5, prob = .2) * .7 + \n  dbinom(3, size = 5, prob = .4) * .2 + \n  dbinom(3, size = 5, prob = .8) * .1 \n\n[1] 0.1024"
  },
  {
    "objectID": "slides/slides02.html#posterior",
    "href": "slides/slides02.html#posterior",
    "title": "Bayes Rule",
    "section": "Posterior",
    "text": "Posterior\n\n\n\nπ\n\n\n0.2\n\n\n0.4\n\n\n0.8\n\n\n\n\nf(π)\n\n\n0.7\n\n\n0.2\n\n\n0.1\n\n\n\n\nL(π | y = 3)\n\n\n0.0512\n\n\n0.2304\n\n\n0.2048\n\n\n\n\nf(π | y = 3)"
  },
  {
    "objectID": "slides/slides02.html#why-is-it-a-normalizing-constant",
    "href": "slides/slides02.html#why-is-it-a-normalizing-constant",
    "title": "Bayes Rule",
    "section": "Why is it a “normalizing constant”?",
    "text": "Why is it a “normalizing constant”?"
  },
  {
    "objectID": "slides/slides02.html#summary",
    "href": "slides/slides02.html#summary",
    "title": "Bayes Rule",
    "section": "Summary",
    "text": "Summary\nThree steps to a Bayesian analysis:\n\nConstruct a prior model for the variable of interest (possible values and the relative plausibility of each)\nUpon observing data \\(Y = y\\), define the likelihoood \\(L(\\pi |y) = P(y | \\pi)\\)\nBuild the posterior model of the variable of interest using Bayes Rule, which balances the prior and likelihood: \\[\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\propto \\text{prior} \\cdot \\text{likelihood}\\]"
  },
  {
    "objectID": "slides/slides01.html#plan-for-today",
    "href": "slides/slides01.html#plan-for-today",
    "title": "Welcome to Bayes!",
    "section": "Plan for today",
    "text": "Plan for today\n\nIntros\nIntro to Bayesian Thinking\nSyllabus"
  },
  {
    "objectID": "slides/slides01.html#about-me",
    "href": "slides/slides01.html#about-me",
    "title": "Welcome to Bayes!",
    "section": "About me",
    "text": "About me\n\n\n\nEllis (almost 1.5!)\nSecond year at Carleton\nTaught at Swarthmore for 5 years before moving to MN\nPhD in Statistics & Data Science from Carnegie Mellon University\nGrew up in Minnesota, went to St Ben’s as an undergrad"
  },
  {
    "objectID": "slides/slides01.html#my-crochet-journey-this-summer",
    "href": "slides/slides01.html#my-crochet-journey-this-summer",
    "title": "Welcome to Bayes!",
    "section": "My crochet journey this summer",
    "text": "My crochet journey this summer\n\n\n\n\n\nDecided I wanted to make a basket\nSkipped “practice these stitches” and just went for it\nDidn’t understand the pattern, watched a couple of youtube videos from random creators and went for it\nKept going even when I was very clearly doing something wrong"
  },
  {
    "objectID": "slides/slides01.html#my-crochet-journey-this-summer-1",
    "href": "slides/slides01.html#my-crochet-journey-this-summer-1",
    "title": "Welcome to Bayes!",
    "section": "My crochet journey this summer",
    "text": "My crochet journey this summer\n\n\n\n\n\nAsked an expert (my sister) for advice\nDid “practice problems” using easier material (yarn) and bigger hook\nFound a reliable internet source for videos and tutorials\nStarted being way more careful about checking my work and redoing things if I wasn’t sure"
  },
  {
    "objectID": "slides/slides01.html#intros",
    "href": "slides/slides01.html#intros",
    "title": "Welcome to Bayes!",
    "section": "Intros",
    "text": "Intros\n\nName\nSomething you did over the summer\nWhat makes you nervous about this class or term?\nSomething about yourself you are proud of\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides01.html#warm-up",
    "href": "slides/slides01.html#warm-up",
    "title": "Welcome to Bayes!",
    "section": "Warm Up",
    "text": "Warm Up\n\nWhat are the key takeaways from:\n\nStat250: Stat Inference\nStat230: Regression\n\nWhat do you already know about Bayesian statistics?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/slides01.html#statistical-models",
    "href": "slides/slides01.html#statistical-models",
    "title": "Welcome to Bayes!",
    "section": "Statistical models",
    "text": "Statistical models\nA statistical model consists of\n\na collection of random variables to describe observable data,\nthe possible joint distribution(s) of the random variables,\nand the parameters, \\(\\boldsymbol \\theta\\), that define those distributions\n\n\n\nMorris and DeGroot, 377"
  },
  {
    "objectID": "slides/slides01.html#r.a.-fisher",
    "href": "slides/slides01.html#r.a.-fisher",
    "title": "Welcome to Bayes!",
    "section": "R.A. Fisher",
    "text": "R.A. Fisher\n1890-1962\n\n\n\n\n\nVariance\nANOVA\nNull hypothesis\nMaximum likelihood estimation\np-value\nLots of contributions in genetics\nAlso a eugenecist"
  },
  {
    "objectID": "slides/slides01.html#neyman-pearson",
    "href": "slides/slides01.html#neyman-pearson",
    "title": "Welcome to Bayes!",
    "section": "Neyman & Pearson",
    "text": "Neyman & Pearson\n\n\n\n\n\nConfidence interval\nCorrelation\nRegression\nStandard deviation\nEffect size\n“Optimal” tests\n\\(\\alpha\\) and \\(\\beta\\)\nType I and II error"
  },
  {
    "objectID": "slides/slides01.html#frequentist-vs-bayesian",
    "href": "slides/slides01.html#frequentist-vs-bayesian",
    "title": "Welcome to Bayes!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian"
  },
  {
    "objectID": "slides/slides01.html#bayes",
    "href": "slides/slides01.html#bayes",
    "title": "Welcome to Bayes!",
    "section": "Bayes",
    "text": "Bayes\n\n\n\n\nAll this guy did was Bayes’ rule:\n\\(P(A|B) = \\frac{P(B|A) P(A)}{ P(B)}\\)"
  },
  {
    "objectID": "slides/slides01.html#frequentist-vs-bayesian-1",
    "href": "slides/slides01.html#frequentist-vs-bayesian-1",
    "title": "Welcome to Bayes!",
    "section": "Frequentist vs Bayesian",
    "text": "Frequentist vs Bayesian\n\n\nFrequentist\n\n“Classical” statistics\nProbability is a long-run frequency\nMake decisions based on a p-value\nType I and Type II errors\nConfidence intervals\n\n\nBayesian\n\nProbability is a subjective belief\nUpdate “prior” probabilities with data to obtain “posterior” probabilities\nMake decisions according to posterior probabilities\n“Credible” intervals"
  },
  {
    "objectID": "slides/slides01.html#q1",
    "href": "slides/slides01.html#q1",
    "title": "Welcome to Bayes!",
    "section": "Q1",
    "text": "Q1\nWhen flipping a fair coin, we say that “the probability of flipping Heads is 0.5.” How do you interpret this probability?\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a and b make sense."
  },
  {
    "objectID": "slides/slides01.html#q2",
    "href": "slides/slides01.html#q2",
    "title": "Welcome to Bayes!",
    "section": "Q2",
    "text": "Q2\nAn election is coming up and a pollster claims that candidate A has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate A will win roughly 90% of the time.\nCandidate A is much more likely to win than to lose.\nThe pollster’s calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1."
  },
  {
    "objectID": "slides/slides01.html#q3",
    "href": "slides/slides01.html#q3",
    "title": "Welcome to Bayes!",
    "section": "Q3",
    "text": "Q3\nConsider two claims.\n\nZuofu claims that he can predict the outcome of a coin flip. To test his claim, you flip a fair coin 10 times and he correctly predicts all 10.\nKavya claims that she can distinguish natural and artificial sweeteners. To test her claim, you give her 10 sweetener samples and she correctly identifies each.\n\nIn light of these experiments, what do you conclude?\n\nYou’re more confident in Kavya’s claim than Zuofu’s claim.\nThe evidence supporting Zuofu’s claim is just as strong as the evidence supporting Kavya’s claim."
  },
  {
    "objectID": "slides/slides01.html#q4",
    "href": "slides/slides01.html#q4",
    "title": "Welcome to Bayes!",
    "section": "Q4",
    "text": "Q4\nSuppose that during a recent doctor’s visit, you tested positive for a very rare disease. If you only get to ask the doctor one question, which would it be?\n\nWhat’s the chance that I actually have the disease?\nIf in fact I don’t have the disease, what’s the chance that I would’ve gotten this positive test result?"
  },
  {
    "objectID": "slides/slides01.html#results",
    "href": "slides/slides01.html#results",
    "title": "Welcome to Bayes!",
    "section": "Results",
    "text": "Results\nGive yourself:\n\n+1 for each A\n+3 for each B\n+2 for C on Q1\n-2 for C on Q2\n\nResults:\n\nScore &lt;5: your thinking is pretty frequentist\nScore &gt;9: your thinking is pretty Bayesian\n6-8: your leaning changes depending on the scenario"
  },
  {
    "objectID": "slides/slides01.html#tentative-schedule",
    "href": "slides/slides01.html#tentative-schedule",
    "title": "Welcome to Bayes!",
    "section": "Tentative schedule",
    "text": "Tentative schedule\n\n\n\n\nTopic\nChapters\nApprox. Duration\n\n\n\n\nIntro\n1-2\n1 week\n\n\nConjugate Analysis\n3-5\n2 weeks\n\n\nMCMC\n7\n1 weeks\n\n\nBayesian Regression\n9-14\n4 weeks\n\n\nHierarchical Models\n15\n2 weeks"
  },
  {
    "objectID": "slides/slides01.html#course-description",
    "href": "slides/slides01.html#course-description",
    "title": "Welcome to Bayes!",
    "section": "Course description",
    "text": "Course description\nThe Bayesian approach to statistics provides a powerful framework for incorporating prior knowledge into statistical analyses, updating this knowledge with data, and quantifying uncertainty in results. This course serves as a comprehensive introduction to Bayesian statistical inference and modeling, an alternative to the frequentist approach to statistics covered in previous classes. Topics include: Bayes’ Theorem; prior and posterior distributions; Bayesian regression; hierarchical models; and model adequacy and posterior predictive checks. Computational techniques will also be covered, including Markov Chain Monte Carlo methods, and modern Bayesian modeling packages in R."
  },
  {
    "objectID": "slides/slides01.html#course-objectives",
    "href": "slides/slides01.html#course-objectives",
    "title": "Welcome to Bayes!",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter completing this course, you should be be able to demonstrate your competency with:\n\nExplaining the differences between frequentist and Bayesian approaches to statistics and data analysis\nSelecting appropriate prior distributions and likelihood models for Bayesian analysis\nUsing modern software to fit Bayesian models and assess their performance\nInterpreting results from Bayesian analyses"
  },
  {
    "objectID": "slides/slides01.html#textbook",
    "href": "slides/slides01.html#textbook",
    "title": "Welcome to Bayes!",
    "section": "Textbook:",
    "text": "Textbook:\nBayes Rules! An Introduction to Applied Bayesian Modeling by Johnson, Ott, and Dogucu\nThe book is freely available at &lt;bayesrulesbook.com&gt;"
  },
  {
    "objectID": "slides/slides01.html#computing",
    "href": "slides/slides01.html#computing",
    "title": "Welcome to Bayes!",
    "section": "Computing:",
    "text": "Computing:\nWe’ll be using R and RStudio throughout the course. If you’ve downloaded R to your own computer from a different class, great! If not, there are instructions available on the course website"
  },
  {
    "objectID": "slides/slides01.html#what-will-you-do-in-this-course",
    "href": "slides/slides01.html#what-will-you-do-in-this-course",
    "title": "Welcome to Bayes!",
    "section": "What will you do in this course?",
    "text": "What will you do in this course?\nEach of the following components are important for your learning and therefore part of your final grade calculation:\n\n30% Homework due once per week\n\n10% Individual (graded on completion)\n10% Group (correctness and completion)\n10% Homework quizzes (correctness)\n\n10% Midterm project + presentation\n20% Final project + presentation\n20% Exam I\n20% Exam II"
  },
  {
    "objectID": "slides/slides01.html#what-will-a-typical-dayweek-look-like",
    "href": "slides/slides01.html#what-will-a-typical-dayweek-look-like",
    "title": "Welcome to Bayes!",
    "section": "What will a typical day/week look like?",
    "text": "What will a typical day/week look like?\n\n\nBefore class:\n\nRead a chapter\nCome with questions\nBe prepared to try what was covered\n\n\nIn class:\n\nLecture/demo\n\nSometimes review of chapter\nSometimes new\n\nHands-on work or coding in R\nTime for group assignments\n\n\nAfter class:\n\nFinish group assignments\nWork on individual assignments"
  },
  {
    "objectID": "slides/slides01.html#office-hours-tentative",
    "href": "slides/slides01.html#office-hours-tentative",
    "title": "Welcome to Bayes!",
    "section": "Office hours (tentative)",
    "text": "Office hours (tentative)\n\n\n\nDay\nTime\nType\nLocation\n\n\n\n\nMonday\n12:30-1:30\nDrop-in\nCMC 307\n\n\nTuesday\n2-3\nDrop-in\nCMC 307\n\n\nWednesday\n3-41\nDrop-in\nCMC 307\n\n\nFriday\n10-112\nDrop-in\nCMC 307\n\n\n\nI will also have at least 1 hour of individual appointments available on my calendar per week\nStat120 PriorityStat340 Priority"
  },
  {
    "objectID": "slides/slides01.html#communication",
    "href": "slides/slides01.html#communication",
    "title": "Welcome to Bayes!",
    "section": "Communication",
    "text": "Communication\n\n\nMoodle: assignments, slides, and grades\nSlack: homework questions, announcements, discussion\nEmail: personal matters, time-sensitive announcements\n\n\n\nSlack is the fastest way to reach me. I typically will respond to messages 3x per weekday. I try to respond to emails within 48 hours. I’m online sporadically on evenings and weekends to devote time to family and rest – I hope you also use this time to reset and recharge!"
  },
  {
    "objectID": "slides/slides01.html#advice-from-past-students",
    "href": "slides/slides01.html#advice-from-past-students",
    "title": "Welcome to Bayes!",
    "section": "Advice from past students:",
    "text": "Advice from past students:\n\nSTART HOMEWORKS EARLY\nDo the reading! The textbook is really great and can help give you a motivation for everything that’s happening in class.\nStart the problem sets early and go to office hours!\nask questions! you will inevitably not understand everything in the course at first pass.\nGo to her office hours!!!!!\nRead the book, and ask questions!\nTry to start the homework problems on the day that they are assigned.\nThis course is very fast-paced and intensive, so it may not be the right fit for everyone. Students who are not prepared for a compact schedule, or who are looking for an easier workload, may find it challenging. A solid foundation in statistics and a strong level of commitment are essential to keep up with the material and succeed in the class.\nAsk questions in class, Amanda has no problem reviewing a previously discussed concept.\nWork with other people in the class, collaboration is key\nStart the homework early! Give yourself time to get things done, to understand, and to pause. Don’t feel afraid to ask questions; she’s so accessible.\nSpend some time reviewing handouts before doing homework."
  },
  {
    "objectID": "slides/slides01.html#the-genius-myth",
    "href": "slides/slides01.html#the-genius-myth",
    "title": "Welcome to Bayes!",
    "section": "The “Genius Myth”",
    "text": "The “Genius Myth”\nIt’s sometimes easy to buy into the “genius myth” when it comes to math/stat courses: that you need to be a “math person” and have some innate mathematical ability in order to do well or become a statistics major. This could not be further from the truth! The best statisticians don’t necessarily have the “best” math or programming background, but are people that are able to formulate interesting questions and use math and programming to rigorously answer those questions. Many of the best statisticians I know became statisticians because they were initially interested in something else (biology, public health, psychology, neuroscience, physics, etc.) and realized that being able to answer important questions with data was not only valuable but fun and interesting. Being able to perform interesting statistical analyses is a skill that is learned, not an innate ability, and working hard at developing that skill is the point of this course."
  },
  {
    "objectID": "slides/slides01.html#academic-integrity",
    "href": "slides/slides01.html#academic-integrity",
    "title": "Welcome to Bayes!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nYou are expected to follow Carleton’s policies regarding academic integrity. I encourage you to discuss the homework problems with others and use the resources available to you to try to figure out tough problems. You should code and write up your solutions on your own. Exams must be done by yourself without communicating with others; all work must be your own. The use of textbook solution manuals (physical or online), course materials from other students, or materials from previous versions of this course are not allowed. Copying, paraphrasing, summarizing, or submitting work generated by anyone but yourself without proper attribution is considered academic dishonesty (this includes output from LLMs).\nPlease ask if you are unsure of whether or not your actions are complying with the assignment/exam/project instructions. Always default to acknowledging any help received. Cases of suspected academic dishonesty are handled by the Provost’s Office and I am obligated to report any suspected violations of this policy."
  },
  {
    "objectID": "slides/slides01.html#more-on-ai",
    "href": "slides/slides01.html#more-on-ai",
    "title": "Welcome to Bayes!",
    "section": "More on “AI”",
    "text": "More on “AI”\nLarge-language models (e.g. ChatGPT, Gemini, etc.) should only be used for help interpreting R’s error messages or suggestions for your own code once you have already attempted the problem. You should not copy and paste course material into or out of an AI text generator.\nI also have a few rules in place to protect my intellectual property. You may not record my lectures using tools such as Otter.ai or upload any video or audio recordings to generate transcripts or study notes. You may not upload my course materials (slides, assignment prompts, note sets, etc.) into AI tools or homework help sites (such as chegg).\n“AI” tools are new for all of us and it’s OK to have questions about what is and isn’t appropriate!"
  },
  {
    "objectID": "slides/slides01.html#diversity-inclusion",
    "href": "slides/slides01.html#diversity-inclusion",
    "title": "Welcome to Bayes!",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nWe all come to class with different backgrounds and experiences, and this diversity makes our class environment richer. We value diversity and inclusion, and are committed to a climate of mutual respect and full participation in and out of the classroom. This class strives to be a learning environment that is usable, equitable, inclusive and welcoming, regardless of race, ethnicity, religion, gender and gender identities, sexual orientation, ability, socioeconomic background, and nationality. If you anticipate or experience any barriers to learning, please discuss your concerns with me."
  },
  {
    "objectID": "slides/slides01.html#accomodations",
    "href": "slides/slides01.html#accomodations",
    "title": "Welcome to Bayes!",
    "section": "Accomodations",
    "text": "Accomodations\nCarleton College is committed to providing equitable access to learning opportunities for all students. The Office of Accessibility Resources (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability, please contact OAR@carleton.edu to arrange a confidential discussion regarding equitable access and reasonable accommodations. You are also welcome to contact me privately to discuss your academic needs. However, all disability-related accommodations must be arranged, in advance, through OAR."
  },
  {
    "objectID": "slides/slides01.html#title-ix",
    "href": "slides/slides01.html#title-ix",
    "title": "Welcome to Bayes!",
    "section": "Title IX",
    "text": "Title IX\nPlease be aware that all faculty are “responsible employees”, which means that if you tell me about a situation involving sexual harassment, sexual assault, dating violence, domestic violence, or stalking, I must share that information with the Title IX Coordinator. Although I have to make this notification, you will control how your case will be handled, including whether or not you wish to meet with the Title IX coordinator or pursue a formal complaint."
  },
  {
    "objectID": "slides/slides01.html#take-care-of-yourself",
    "href": "slides/slides01.html#take-care-of-yourself",
    "title": "Welcome to Bayes!",
    "section": "Take care of yourself",
    "text": "Take care of yourself\nDo your best to maintain a healthy lifestyle this semester by wearing a mask if you don’t feel well, eating a vegetable every day, exercising, avoiding excessive drug and alcohol use, getting enough sleep, and taking some time to relax. Your mental health is more important than your grade in this course. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. If you are experiencing mental health symptoms as a result of coursework, please speak with me so we can address the problem together."
  },
  {
    "objectID": "slides/slides04.html#back-to-graduate-school-applications",
    "href": "slides/slides04.html#back-to-graduate-school-applications",
    "title": "Beta-Binomial Model",
    "section": "Back to Graduate School Applications",
    "text": "Back to Graduate School Applications\nLast week we were trying to understand \\(\\pi\\) the acceptance rate of a graduate program in a specific department. We assumed a somewhat silly prior, and restricted \\(\\pi\\) to be 0.2, 0.4, or 0.8.\nThis time we will let \\(\\pi \\in [0,1]\\)."
  },
  {
    "objectID": "slides/slides04.html#section",
    "href": "slides/slides04.html#section",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "Continuous probability models\nLet \\(\\pi\\) be a continuous random variable with pdf \\(f(\\pi)\\). Then \\(f(\\pi)\\) has the following properties:\n\n\\(\\int_\\pi f(\\pi)d\\pi = 1\\), ie. the area under \\(f(\\pi)\\) is 1\n\\(f(\\pi) \\ge 0\\)\n\\(P(a &lt; \\pi &lt; b) = \\int_a^b f(\\pi) d\\pi\\) when \\(a \\le b\\)\n\nInterpreting \\(f(\\pi)\\):"
  },
  {
    "objectID": "slides/slides04.html#section-1",
    "href": "slides/slides04.html#section-1",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "For each of the student’s prior ideas for \\(\\pi\\) sketch the pdf of the prior. Your plot will not be exact since no exact values are given.\n\n\n\nMorteza thinks that it is extremely difficult to get into this program.\n\n\n\n\n\n\n\n\n\n\nErin does not have any strong opinions whether it is difficult or easy to get into this program."
  },
  {
    "objectID": "slides/slides04.html#section-2",
    "href": "slides/slides04.html#section-2",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "For each of the student’s prior ideas for \\(\\pi\\) sketch the pdf of the prior. Your plot will not be exact since no exact values are given.\n\n\n\nXuan thinks that it is either really easy or really hard to get into the program\n\n\n\n\n\n\n\n\n\n\nBeyoncé thinks that it is extremely easy to get into this program."
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model",
    "href": "slides/slides04.html#beta-prior-model",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\nLet \\(\\pi\\) be a random variable which can take any value between 0 and 1, ie. \\(\\pi \\in [0,1]\\). Then the variability in \\(\\pi\\) might be well modeled by a Beta model with shape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\):\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\]"
  },
  {
    "objectID": "slides/slides04.html#section-3",
    "href": "slides/slides04.html#section-3",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "The Beta model is specified by continuous pdf \\[\\begin{equation}\nf(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\;\\; \\text{ for } \\pi \\in [0,1]\n\\end{equation}\\]\nwhere \\(\\Gamma(z) = \\int_0^\\infty y^{z-1}e^{-y}dy\\) and \\(\\Gamma(z + 1) = z \\Gamma(z)\\).\nFun fact: when \\(z\\) is a positive integer, then \\(\\Gamma(z)\\) simplifies to \\(\\Gamma(z) = (z-1)!\\)."
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model-1",
    "href": "slides/slides04.html#beta-prior-model-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\pi \\sim \\text{Beta}(3, 8)\\)\n\\(f(\\pi) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}\\)\n\\(f(0.5) =\\)"
  },
  {
    "objectID": "slides/slides04.html#beta-prior-model-2",
    "href": "slides/slides04.html#beta-prior-model-2",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior model",
    "text": "Beta Prior model\n\\(\\pi \\sim \\text{Beta}(3, 8)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbeta(x = 0.5, \n      shape1 = 3, \n      shape2 = 8)\n\n[1] 0.703125"
  },
  {
    "objectID": "slides/slides04.html#plotting-beta-prior",
    "href": "slides/slides04.html#plotting-beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Plotting Beta prior",
    "text": "Plotting Beta prior"
  },
  {
    "objectID": "slides/slides04.html#plotting-beta-prior-with-bayesrules-package",
    "href": "slides/slides04.html#plotting-beta-prior-with-bayesrules-package",
    "title": "Beta-Binomial Model",
    "section": "Plotting Beta Prior with bayesrules package",
    "text": "Plotting Beta Prior with bayesrules package\n\nplot_beta(alpha = 3, beta = 8)"
  },
  {
    "objectID": "slides/slides04.html#beta-summaries",
    "href": "slides/slides04.html#beta-summaries",
    "title": "Beta-Binomial Model",
    "section": "Beta summaries",
    "text": "Beta summaries\n\\[E(\\pi) = \\frac{\\alpha}{\\alpha + \\beta}\\]\n\\[\\text{Mode}(\\pi) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}\\]\n\\[\\text{Var}(\\pi) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\]"
  },
  {
    "objectID": "slides/slides04.html#beta-summaries-with-bayesrules-package",
    "href": "slides/slides04.html#beta-summaries-with-bayesrules-package",
    "title": "Beta-Binomial Model",
    "section": "Beta summaries with bayesrules package",
    "text": "Beta summaries with bayesrules package\nUse the summarize_beta() function in the bayesrules package to find the mean, mode, and variance of various Beta distributions. Example:\n\nsummarize_beta(alpha = 5, beta = 7)\n\n       mean mode        var        sd\n1 0.4166667  0.4 0.01869658 0.1367354"
  },
  {
    "objectID": "slides/slides04.html#posterior-for-the-beta-binomial-model",
    "href": "slides/slides04.html#posterior-for-the-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Posterior for the Beta-Binomial model",
    "text": "Posterior for the Beta-Binomial model"
  },
  {
    "objectID": "slides/slides04.html#posterior-for-the-beta-binomial-model-1",
    "href": "slides/slides04.html#posterior-for-the-beta-binomial-model-1",
    "title": "Beta-Binomial Model",
    "section": "Posterior for the Beta-Binomial model",
    "text": "Posterior for the Beta-Binomial model"
  },
  {
    "objectID": "slides/slides04.html#conjugate-prior",
    "href": "slides/slides04.html#conjugate-prior",
    "title": "Beta-Binomial Model",
    "section": "Conjugate prior",
    "text": "Conjugate prior\nWe say that \\(f(\\pi)\\) is a conjugate prior for \\(L(\\pi|y)\\) if the posterior, \\(f(\\pi|y) \\propto f(\\pi)L(\\pi|y)\\), is from the same model family as the prior.\nThus, the Beta distribution is a conjugate prior for the Binomial likelihood model since the posterior also follows a Beta distribution."
  },
  {
    "objectID": "slides/slides04.html#admission-example",
    "href": "slides/slides04.html#admission-example",
    "title": "Beta-Binomial Model",
    "section": "Admission Example",
    "text": "Admission Example\n\nOriginal discrete prior:\n\n\n\n\\(\\pi\\)\n0.2\n0.4\n0.8\n\n\n\n\n\\(f(\\pi)\\)\n.7\n.2\n.1\n\n\n\n\nBeta approximation:\n\nplot_beta(3, 7, mean = TRUE, mode = TRUE)"
  },
  {
    "objectID": "slides/slides04.html#data-posterior",
    "href": "slides/slides04.html#data-posterior",
    "title": "Beta-Binomial Model",
    "section": "Data + Posterior",
    "text": "Data + Posterior\n3/5 students were admitted\n\\[ \\pi \\sim \\text{Beta}(3, 7)\\] \\[ \\pi | Y \\sim \\text{Beta}(3 + y, 7 + n -y)\\] \\[ \\pi | Y \\sim \\text{Beta}(6, 9)\\]"
  },
  {
    "objectID": "slides/slides04.html#posterior-summary",
    "href": "slides/slides04.html#posterior-summary",
    "title": "Beta-Binomial Model",
    "section": "Posterior summary",
    "text": "Posterior summary\n\nsummarize_beta(6,9)\n\n  mean      mode   var        sd\n1  0.4 0.3846154 0.015 0.1224745\n\nplot_beta(6, 9, mean = TRUE, mode = TRUE)"
  },
  {
    "objectID": "slides/slides04.html#balancing-act",
    "href": "slides/slides04.html#balancing-act",
    "title": "Beta-Binomial Model",
    "section": "Balancing act",
    "text": "Balancing act\n\nplot_beta_binomial(alpha = 3, beta = 7,  y = 3, n = 5)"
  },
  {
    "objectID": "slides/slides04.html#more-data-more-certainty",
    "href": "slides/slides04.html#more-data-more-certainty",
    "title": "Beta-Binomial Model",
    "section": "More data, more certainty",
    "text": "More data, more certainty\nWhat if 30/50 applicants get in?\n\nplot_beta_binomial(alpha = 3, beta = 7, y = 30, n = 50)"
  },
  {
    "objectID": "slides/slides04.html#more-more-data-more-more-certainty",
    "href": "slides/slides04.html#more-more-data-more-more-certainty",
    "title": "Beta-Binomial Model",
    "section": "More more data, more more certainty",
    "text": "More more data, more more certainty\nWhat if 300/500 applicants get in?\n\nplot_beta_binomial(alpha = 3, beta = 7, y = 300, n = 500)"
  },
  {
    "objectID": "slides/slides04.html#data-context",
    "href": "slides/slides04.html#data-context",
    "title": "Beta-Binomial Model",
    "section": "Data Context",
    "text": "Data Context\nLet \\(\\pi\\) represent the proportion of students who are admitted to the program.\n\\(Y | \\pi \\sim \\text{Binom}(n, \\pi)\\)"
  },
  {
    "objectID": "slides/slides04.html#prior",
    "href": "slides/slides04.html#prior",
    "title": "Beta-Binomial Model",
    "section": "Prior",
    "text": "Prior\n\nplot_beta(3, 7)"
  },
  {
    "objectID": "slides/slides04.html#data-and-the-posterior",
    "href": "slides/slides04.html#data-and-the-posterior",
    "title": "Beta-Binomial Model",
    "section": "Data and the Posterior",
    "text": "Data and the Posterior\n30/50 students are admitted\n\nsummarize_beta_binomial(3, 7, y = 30, n = 50)\n\n      model alpha beta mean      mode         var         sd\n1     prior     3    7 0.30 0.2500000 0.019090909 0.13816986\n2 posterior    33   27 0.55 0.5517241 0.004057377 0.06369754"
  },
  {
    "objectID": "slides/slides04.html#section-4",
    "href": "slides/slides04.html#section-4",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "plot_beta_binomial(3, 7, y = 30, n = 50)\n\n\n\n\n\n\n\n\nCan we simulate the posterior without knowing it exactly?"
  },
  {
    "objectID": "slides/slides04.html#section-5",
    "href": "slides/slides04.html#section-5",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "set.seed(84735)\nadmission_sim &lt;- data.frame(pi = rbeta(10000, 3, 7)) \nhead(admission_sim)\n\n         pi\n1 0.4245463\n2 0.2796543\n3 0.4484403\n4 0.3412382\n5 0.4517916\n6 0.6361449\n\nadmission_sim &lt;- admission_sim |&gt;\n  mutate(y = rbinom(10000, size = 50, prob = pi))"
  },
  {
    "objectID": "slides/slides04.html#section-6",
    "href": "slides/slides04.html#section-6",
    "title": "Beta-Binomial Model",
    "section": "",
    "text": "pi\ny\n\n\n\n\n0.4245463\n24\n\n\n0.2796543\n12\n\n\n0.4484403\n24\n\n\n0.3412382\n19\n\n\n0.4517916\n29\n\n\n0.6361449\n29\n\n\n0.3087587\n19\n\n\n0.7005417\n33\n\n\n0.3560422\n20\n\n\n0.3904306\n22"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes",
    "href": "slides/slides04.html#simulation-outcomes",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes",
    "text": "Simulation outcomes"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes-that-match-the-data",
    "href": "slides/slides04.html#simulation-outcomes-that-match-the-data",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes that match the data",
    "text": "Simulation outcomes that match the data"
  },
  {
    "objectID": "slides/slides04.html#simulation-outcomes-that-match-the-data-1",
    "href": "slides/slides04.html#simulation-outcomes-that-match-the-data-1",
    "title": "Beta-Binomial Model",
    "section": "Simulation outcomes that match the data",
    "text": "Simulation outcomes that match the data\n\nadmission_posterior &lt;- admission_sim |&gt; \n  filter(y == 30)\n\n\nnrow(admission_posterior)\n\n[1] 100\n\n\n\\(f(\\pi|y =30)\\)"
  },
  {
    "objectID": "slides/slides04.html#simulated-posterior",
    "href": "slides/slides04.html#simulated-posterior",
    "title": "Beta-Binomial Model",
    "section": "Simulated Posterior",
    "text": "Simulated Posterior\n\nggplot(admission_posterior, aes(x = pi)) + \n  geom_histogram(color = \"white\", binwidth = 0.025)"
  },
  {
    "objectID": "slides/slides04.html#simulated-posterior-density-estimate",
    "href": "slides/slides04.html#simulated-posterior-density-estimate",
    "title": "Beta-Binomial Model",
    "section": "Simulated posterior density estimate",
    "text": "Simulated posterior density estimate\n\nggplot(admission_posterior, aes(x = pi)) + \n  geom_density() + xlim(0,1)"
  },
  {
    "objectID": "slides/slides11.html#but-first-grid-approximation",
    "href": "slides/slides11.html#but-first-grid-approximation",
    "title": "Implementing Metropolis-Hastings",
    "section": "But first, grid approximation",
    "text": "But first, grid approximation\nConsider the Normal-Normal model for \\(\\mu\\) with \\(Y|\\mu \\sim N(\\mu, 1.3^2)\\) and \\(\\mu \\sim N(10, 1.2^2)\\). Suppose that you observe data \\((Y_1, Y_2, Y_3, Y_4) = (7.1, 8.9, 8.4, 8.6)\\)\n\nUtilize grid approximation with grid values \\(\\mu \\in \\{5, 6, 7, ..., 15\\}\\) to approximate the posterior of \\(\\mu\\)\n\nRecall that \\[L(\\mu | \\vec{y}) = \\prod f(y_i | \\mu) = \\prod \\phi(y_i, \\mu, 1.3^2)\\]\n\\[L(\\mu | \\vec{y}) \\propto \\phi(\\bar{y}, \\mu, \\frac{1.3^2}{n}) \\]"
  },
  {
    "objectID": "slides/slides11.html#code",
    "href": "slides/slides11.html#code",
    "title": "Implementing Metropolis-Hastings",
    "section": "Code",
    "text": "Code\n\ny &lt;- c(7.1, 8.9, 8.4, 8.6)\n\n# Step 1: Define a grid of mu values\ngrid_data   &lt;- data.frame(mu_grid = seq(from = 5, to = 15, by = 1))\n\n# Step 2: Evaluate the prior & likelihood at each mu. \ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dnorm(mu_grid, 10, 1.2),\n         likelihood = prod(dnorm(y, mu_grid, 1.3)), # likelihood is product of data points\n         likelihood_2 = dnorm(mean(y), mu_grid, 1.3/2) # based on joint distribution\n         ) \n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         unnormalized2 = likelihood_2 *prior,\n         posterior = unnormalized / sum(unnormalized), \n         posterior2 = unnormalized2/sum(unnormalized2))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\npost_sample2 &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior2, replace = TRUE)"
  },
  {
    "objectID": "slides/slides11.html#graphs",
    "href": "slides/slides11.html#graphs",
    "title": "Implementing Metropolis-Hastings",
    "section": "Graphs",
    "text": "Graphs\n\np1 &lt;- ggplot(post_sample, aes(x = mu_grid)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(title = \"V1 likelihood\")\n\np2 &lt;- ggplot(post_sample2, aes(x = mu_grid)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(title = \"V2 likelihood\")\n\np1 + p2"
  },
  {
    "objectID": "slides/slides11.html#which-one-is-correct",
    "href": "slides/slides11.html#which-one-is-correct",
    "title": "Implementing Metropolis-Hastings",
    "section": "Which one is correct?",
    "text": "Which one is correct?\n\nsummarize_normal_normal(10, 1.2, 1.3, y_bar = mean(y), n = length(y))\n\n      model     mean     mode       var        sd\n1     prior 10.00000 10.00000 1.4400000 1.2000000\n2 posterior  8.64698  8.64698 0.3266577 0.5715398"
  },
  {
    "objectID": "slides/slides11.html#what-happened",
    "href": "slides/slides11.html#what-happened",
    "title": "Implementing Metropolis-Hastings",
    "section": "What happened?",
    "text": "What happened?\n\nlikelihood = prod(dnorm(y, mu_grid, 1.3))\n\nBecause the vectors y and mu_grid have different lengths, R is “recycling” the shorter vector to match the longer one:\n\ndnorm(y[1], mu_grid[1], 1.3)\ndnorm(y[2], mu_grid[2], 1.3)\ndnorm(y[3], mu_grid[3], 1.3)\ndnorm(y[4], mu_grid[4], 1.3)\ndnorm(y[1], mu_grid[5], 1.3)\ndnorm(y[2], mu_grid[6], 1.3)\n\nand prod() takes the product of the entire 1000-element vector"
  },
  {
    "objectID": "slides/slides11.html#verbose-version",
    "href": "slides/slides11.html#verbose-version",
    "title": "Implementing Metropolis-Hastings",
    "section": "“verbose” version",
    "text": "“verbose” version\n\nlikelihood = dnorm(y[1], mu_grid, 1.3) * dnorm(y[2], mu_grid, 1.3) * dnorm(y[3], mu_grid, 1.3)* dnorm(y[4], mu_grid, 1.3)"
  },
  {
    "objectID": "slides/slides11.html#section",
    "href": "slides/slides11.html#section",
    "title": "Implementing Metropolis-Hastings",
    "section": "",
    "text": "grid_data   &lt;- data.frame(mu_grid = seq(from = 5, to = 15, by = 1)) |&gt;\n  mutate(prior = dnorm(mu_grid, 10, 1.2),\n         likelihood = dnorm(y[1], mu_grid, 1.3) * dnorm(y[2], mu_grid, 1.3) * dnorm(y[3], mu_grid, 1.3)* dnorm(y[4], mu_grid, 1.3), # likelihood is product of data points\n         unnormalized = likelihood * prior,\n         posterior = unnormalized/sum(unnormalized))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\nggplot(post_sample, aes(x = mu_grid)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(title = \"verbose likelihood\")"
  },
  {
    "objectID": "slides/slides11.html#map-version",
    "href": "slides/slides11.html#map-version",
    "title": "Implementing Metropolis-Hastings",
    "section": "map version",
    "text": "map version\n\nlikelihood = map_dbl(mu_grid, ~prod(dnorm(y, .x, 1.3)))"
  },
  {
    "objectID": "slides/slides11.html#section-1",
    "href": "slides/slides11.html#section-1",
    "title": "Implementing Metropolis-Hastings",
    "section": "",
    "text": "grid_data   &lt;- data.frame(mu_grid = seq(from = 5, to = 15, by = 1)) |&gt;\n  mutate(prior = dnorm(mu_grid, 10, 1.2),\n         likelihood = map_dbl(mu_grid, ~prod(dnorm(y, .x, 1.3))), \n         unnormalized = likelihood * prior,\n         posterior = unnormalized/sum(unnormalized))\n\n# Set the seed\nset.seed(84735)\n\n# Step 4: sample from the discretized posterior\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\nggplot(post_sample, aes(x = mu_grid)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(title = \"map likelihood\")"
  },
  {
    "objectID": "slides/slides11.html#working-on-the-log-scale",
    "href": "slides/slides11.html#working-on-the-log-scale",
    "title": "Implementing Metropolis-Hastings",
    "section": "working on the log scale",
    "text": "working on the log scale\n\ngrid_data   &lt;- data.frame(mu_grid = seq(from = 5, to = 15, length = 5000)) |&gt;\n  mutate(prior = dnorm(mu_grid, 10, 1.2),\n         likelihood = map_dbl(mu_grid, ~prod(dnorm(y, .x, 1.3))), \n         unnormalized = likelihood * prior,\n         posterior = unnormalized/sum(unnormalized))\n\ngrid_data |&gt; head(10) |&gt; gt::gt()\n\n\n\n\n\n\n\nmu_grid\nprior\nlikelihood\nunnormalized\nposterior\n\n\n\n\n5.000000\n5.646917e-05\n1.889473e-08\n1.066970e-12\n2.011112e-12\n\n\n5.002000\n5.686269e-05\n1.918763e-08\n1.091060e-12\n2.056520e-12\n\n\n5.004001\n5.725878e-05\n1.948489e-08\n1.115681e-12\n2.102927e-12\n\n\n5.006001\n5.765748e-05\n1.978657e-08\n1.140844e-12\n2.150356e-12\n\n\n5.008002\n5.805878e-05\n2.009273e-08\n1.166559e-12\n2.198827e-12\n\n\n5.010002\n5.846272e-05\n2.040343e-08\n1.192840e-12\n2.248363e-12\n\n\n5.012002\n5.886931e-05\n2.071874e-08\n1.219698e-12\n2.298987e-12\n\n\n5.014003\n5.927856e-05\n2.103873e-08\n1.247145e-12\n2.350722e-12\n\n\n5.016003\n5.969049e-05\n2.136345e-08\n1.275195e-12\n2.403591e-12\n\n\n5.018004\n6.010511e-05\n2.169298e-08\n1.303859e-12\n2.457620e-12"
  },
  {
    "objectID": "slides/slides11.html#working-on-the-log-scale-1",
    "href": "slides/slides11.html#working-on-the-log-scale-1",
    "title": "Implementing Metropolis-Hastings",
    "section": "working on the log scale",
    "text": "working on the log scale\n\ngrid_data   &lt;- data.frame(mu_grid = seq(from = 5, to = 15, length = 5000)) |&gt;\n  mutate(prior = dnorm(mu_grid, 10, 1.2, log = TRUE),\n         likelihood = map_dbl(mu_grid, ~sum(dnorm(y, .x, 1.3, log = TRUE))), \n         log_unnormalized = likelihood + prior,\n         unnormalized = exp(log_unnormalized - max(log_unnormalized)),\n         posterior = unnormalized/sum(unnormalized))\n\ngrid_data |&gt; head(10) |&gt; gt::gt()\n\n\n\n\n\n\n\nmu_grid\nprior\nlikelihood\nlog_unnormalized\nunnormalized\nposterior\n\n\n\n\n5.000000\n-9.781816\n-17.78438\n-27.56620\n1.440310e-09\n2.011112e-12\n\n\n5.002000\n-9.774871\n-17.76900\n-27.54387\n1.472830e-09\n2.056520e-12\n\n\n5.004001\n-9.767930\n-17.75363\n-27.52156\n1.506065e-09\n2.102927e-12\n\n\n5.006001\n-9.760991\n-17.73826\n-27.49925\n1.540032e-09\n2.150356e-12\n\n\n5.008002\n-9.754055\n-17.72291\n-27.47696\n1.574746e-09\n2.198827e-12\n\n\n5.010002\n-9.747121\n-17.70756\n-27.45468\n1.610223e-09\n2.248363e-12\n\n\n5.012002\n-9.740191\n-17.69223\n-27.43242\n1.646478e-09\n2.298987e-12\n\n\n5.014003\n-9.733263\n-17.67690\n-27.41016\n1.683530e-09\n2.350722e-12\n\n\n5.016003\n-9.726338\n-17.66158\n-27.38792\n1.721394e-09\n2.403591e-12\n\n\n5.018004\n-9.719416\n-17.64628\n-27.36569\n1.760088e-09\n2.457620e-12"
  },
  {
    "objectID": "slides/slides11.html#section-2",
    "href": "slides/slides11.html#section-2",
    "title": "Implementing Metropolis-Hastings",
    "section": "",
    "text": "set.seed(84735)\npost_sample &lt;- sample_n(grid_data, size = 10000, \n                        weight = posterior, replace = TRUE)\n\nggplot(post_sample, aes(x = mu_grid)) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(title = \"log-scale\")"
  },
  {
    "objectID": "slides/slides11.html#section-3",
    "href": "slides/slides11.html#section-3",
    "title": "Implementing Metropolis-Hastings",
    "section": "",
    "text": "Metropolis-Hastings Algorithm\n\n\nLet parameter \\(\\mu\\) have posterior \\(f(\\mu|y) \\propto f(\\mu) L(\\mu | y)\\). A Metropolis-Hastings Markov Chain \\(\\{\\mu^{(1)}, \\mu^{(2)}, ..., \\mu^{(n)}\\}\\) evolves as follows. Let \\(\\mu^{(i)} = \\mu\\) be the chains location at \\(i \\in \\{1, 2, ...., N-1\\}\\) and identify \\(\\mu^{(i+1)}\\) through a two step process:\n\nPropose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu'\\) from a proposal model with pdf \\(q(\\mu'|\\mu)\\)\nDecide whether to go there. Calculate the acceptance probability \\[\\alpha = \\min\\{1, \\frac{f(\\mu') L(\\mu'|y) q(\\mu|\\mu')}{f(\\mu) L(\\mu|y) q(\\mu'|\\mu)} \\}\\] Then,\n\n\\[\\mu^{(i+1)} = \\begin{cases} \\mu' & \\text{with probability } \\alpha \\\\ \\mu & \\text{with probability } 1-\\alpha \\end{cases}\\]"
  },
  {
    "objectID": "slides/slides11.html#example",
    "href": "slides/slides11.html#example",
    "title": "Implementing Metropolis-Hastings",
    "section": "Example",
    "text": "Example\n\\[\\mu \\sim N(0, 1^2)\\]\n\\[Y|\\mu \\sim N(\\mu, .75^2)\\]\nand suppose we observe one data point: \\(Y = 6.25\\).\nWe’ll implement Metropolis-Hastings with \\(q(\\mu' | \\mu) = Unif(\\mu - 1, \\mu + 1)\\)"
  },
  {
    "objectID": "slides/slides11.html#start-at-mu-3",
    "href": "slides/slides11.html#start-at-mu-3",
    "title": "Implementing Metropolis-Hastings",
    "section": "Start at \\(\\mu = 3\\)",
    "text": "Start at \\(\\mu = 3\\)\n\ncurrent &lt;- 3\nchain &lt;- c(current)\n\nPropose next step:\n\nset.seed(8)\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nproposal\n\n[1] 2.93259"
  },
  {
    "objectID": "slides/slides11.html#compute-alpha",
    "href": "slides/slides11.html#compute-alpha",
    "title": "Implementing Metropolis-Hastings",
    "section": "Compute \\(\\alpha\\)",
    "text": "Compute \\(\\alpha\\)\n\ndnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)\n\n[1] 1.624909e-07\n\ndnorm(current, 0, 1) * dnorm(6.25, current, 0.75)\n\n[1] 1.971928e-07\n\nalpha &lt;- min(1, (dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)) / (dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)))\nalpha\n\n[1] 0.8240205"
  },
  {
    "objectID": "slides/slides11.html#determine-next-stop",
    "href": "slides/slides11.html#determine-next-stop",
    "title": "Implementing Metropolis-Hastings",
    "section": "Determine next stop",
    "text": "Determine next stop\n\nnext_stop &lt;- sample(c(proposal, current),\n                    size = 1, prob = c(alpha, 1-alpha))\nnext_stop\n\n[1] 2.93259"
  },
  {
    "objectID": "slides/slides11.html#update-and-save-results",
    "href": "slides/slides11.html#update-and-save-results",
    "title": "Implementing Metropolis-Hastings",
    "section": "Update and save results",
    "text": "Update and save results\n\ncurrent &lt;- next_stop\ncurrent\n\n[1] 2.93259\n\nchain &lt;- c(chain, next_stop)\nchain\n\n[1] 3.00000 2.93259"
  },
  {
    "objectID": "slides/slides11.html#do-it-all-again-round-1",
    "href": "slides/slides11.html#do-it-all-again-round-1",
    "title": "Implementing Metropolis-Hastings",
    "section": "Do it all again (round 1)",
    "text": "Do it all again (round 1)\n\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nproposal\n\n[1] 3.531906\n\nalpha &lt;- min(1, (dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)) / (dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)))\nalpha\n\n[1] 1\n\nnext_stop &lt;- sample(c(proposal, current),\n                    size = 1, prob = c(alpha, 1-alpha))\nnext_stop\n\n[1] 3.531906\n\ncurrent &lt;- next_stop\ncurrent\n\n[1] 3.531906\n\nchain &lt;- c(chain, next_stop)\nchain\n\n[1] 3.000000 2.932590 3.531906"
  },
  {
    "objectID": "slides/slides11.html#do-it-all-again-round-2",
    "href": "slides/slides11.html#do-it-all-again-round-2",
    "title": "Implementing Metropolis-Hastings",
    "section": "Do it all again (round 2)",
    "text": "Do it all again (round 2)\n\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nproposal\n\n[1] 3.174925\n\nalpha &lt;- min(1, (dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)) / (dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)))\nalpha\n\n[1] 0.5266794\n\nnext_stop &lt;- sample(c(proposal, current),\n                    size = 1, prob = c(alpha, 1-alpha))\nnext_stop\n\n[1] 3.531906\n\ncurrent &lt;- next_stop\ncurrent\n\n[1] 3.531906\n\nchain &lt;- c(chain, next_stop)\nchain\n\n[1] 3.000000 2.932590 3.531906 3.531906"
  },
  {
    "objectID": "slides/slides11.html#do-it-all-again-round-3",
    "href": "slides/slides11.html#do-it-all-again-round-3",
    "title": "Implementing Metropolis-Hastings",
    "section": "Do it all again (round 3)",
    "text": "Do it all again (round 3)\n\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nproposal\n\n[1] 3.113653\n\nalpha &lt;- min(1, (dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)) / (dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)))\nalpha\n\n[1] 0.4552975\n\nnext_stop &lt;- sample(c(proposal, current),\n                    size = 1, prob = c(alpha, 1-alpha))\nnext_stop\n\n[1] 3.113653\n\ncurrent &lt;- next_stop\ncurrent\n\n[1] 3.113653\n\nchain &lt;- c(chain, next_stop)\nchain\n\n[1] 3.000000 2.932590 3.531906 3.531906 3.113653"
  },
  {
    "objectID": "slides/slides11.html#do-it-all-again-round-4",
    "href": "slides/slides11.html#do-it-all-again-round-4",
    "title": "Implementing Metropolis-Hastings",
    "section": "Do it all again (round 4)",
    "text": "Do it all again (round 4)\n\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nproposal\n\n[1] 3.651947\n\nalpha &lt;- min(1, (dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)) / (dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)))\nalpha\n\n[1] 1\n\nnext_stop &lt;- sample(c(proposal, current),\n                    size = 1, prob = c(alpha, 1-alpha))\nnext_stop\n\n[1] 3.651947\n\ncurrent &lt;- next_stop\ncurrent\n\n[1] 3.651947\n\nchain &lt;- c(chain, next_stop)\nchain\n\n[1] 3.000000 2.932590 3.531906 3.531906 3.113653 3.651947"
  },
  {
    "objectID": "slides/slides11.html#anatomy-of-a-function",
    "href": "slides/slides11.html#anatomy-of-a-function",
    "title": "Implementing Metropolis-Hastings",
    "section": "Anatomy of a function",
    "text": "Anatomy of a function\n\n\n\nA short but informative name, preferably a verb\nArguments of the function inside function()\nPlace the code you have developed in body of the function, a { block that immediately follows function(...).\n\n\n\nname_of_function &lt;- function(..arguments..) { \n  # do stuff with arguments \n  # last result will be returned \n}"
  },
  {
    "objectID": "slides/slides11.html#section-4",
    "href": "slides/slides11.html#section-4",
    "title": "Implementing Metropolis-Hastings",
    "section": "",
    "text": "one_mh_iteration &lt;- function(w, current){\n # STEP 1: Propose the next chain location\n proposal &lt;- runif(1, min = current - w, max = current + w)\n  \n # STEP 2: Decide whether or not to go there\n proposal_plaus &lt;- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)\n current_plaus  &lt;- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)\n alpha &lt;- min(1, proposal_plaus / current_plaus)\n next_stop &lt;- sample(c(proposal, current), \n                     size = 1, prob = c(alpha, 1-alpha))\n  \n # Return the results\n return(data.frame(proposal, alpha, next_stop))\n}"
  },
  {
    "objectID": "slides/slides11.html#four-rounds-of-metropolis-hastings",
    "href": "slides/slides11.html#four-rounds-of-metropolis-hastings",
    "title": "Implementing Metropolis-Hastings",
    "section": "Four rounds of Metropolis-Hastings",
    "text": "Four rounds of Metropolis-Hastings\n\nset.seed(8)\none_mh_iteration(w = 1, current = 3)\n\n  proposal     alpha next_stop\n1  2.93259 0.8240205   2.93259\n\none_mh_iteration(w = 1, current = 2.93259)\n\n  proposal alpha next_stop\n1 3.531906     1  3.531906\n\none_mh_iteration(w = 1, current = 3.531906)\n\n  proposal     alpha next_stop\n1 3.174924 0.5266792  3.531906\n\none_mh_iteration(w = 1, current = 3.531906)\n\n  proposal     alpha next_stop\n1 3.113653 0.4552973  3.113653\n\none_mh_iteration(w = 1, current = 3.113653)\n\n  proposal alpha next_stop\n1 3.651947     1  3.651947\n\n\nThis is better, but still requires tracking things “by hand”"
  },
  {
    "objectID": "slides/slides11.html#for-loops-in-r",
    "href": "slides/slides11.html#for-loops-in-r",
    "title": "Implementing Metropolis-Hastings",
    "section": "for loops in R",
    "text": "for loops in R\n\n\n\nPre-allocate storage for your results\nSet up a vector of elements to “iterate” over\nfor tells R to do the operation(s) once for each element in the vector\n\n\n\nresults &lt;- ... \n\nfor(element in vector){\n  # do something with element\n  # save result \n}"
  },
  {
    "objectID": "slides/slides11.html#mh_tour",
    "href": "slides/slides11.html#mh_tour",
    "title": "Implementing Metropolis-Hastings",
    "section": "mh_tour",
    "text": "mh_tour\n\nmh_tour &lt;- function(N, w){\n  current &lt;- 3 # 1. Start the chain at location 3\n  mu &lt;- rep(0, N) # 2. Pre-allocate storage\n  for(i in 1:N){    # 3. For each element (called i) in the vector 1:N\n    # Simulate one iteration\n    sim &lt;- one_mh_iteration(w = w, current = current)\n    mu[i] &lt;- sim$next_stop#  Record next location\n    current &lt;- sim$next_stop # Reset the current location\n  }\n  # 4. Return the chain locations\n  return(data.frame(iteration = c(1:N), mu))\n}"
  },
  {
    "objectID": "slides/slides11.html#mh_tour-1",
    "href": "slides/slides11.html#mh_tour-1",
    "title": "Implementing Metropolis-Hastings",
    "section": "mh_tour",
    "text": "mh_tour\n\nset.seed(84735)\nmh_simulation_1 &lt;- mh_tour(N = 5000, w = 1)\nmh_simulation_1\n\n     iteration       mu\n1            1 3.495377\n2            2 3.495377\n3            3 4.060496\n4            4 4.247776\n5            5 4.247776\n6            6 4.247776\n7            7 4.289121\n8            8 4.289121\n9            9 4.538466\n10          10 4.919950\n11          11 4.867772\n12          12 4.054966\n13          13 3.842897\n14          14 3.842897\n15          15 4.629231\n16          16 3.843265\n17          17 4.316998\n18          18 4.316998\n19          19 4.999819\n20          20 4.999819\n21          21 4.274847\n22          22 4.379170\n23          23 3.536714\n24          24 4.179085\n25          25 3.691921\n26          26 4.688855\n27          27 4.681411\n28          28 4.681411\n29          29 4.470692\n30          30 4.620998\n31          31 4.620998\n32          32 4.821884\n33          33 4.926892\n34          34 4.956213\n35          35 4.940036\n36          36 4.427729\n37          37 4.427729\n38          38 3.683357\n39          39 4.349073\n40          40 4.349073\n41          41 4.040095\n42          42 4.040095\n43          43 4.230572\n44          44 4.600440\n45          45 4.797525\n46          46 4.797525\n47          47 4.280820\n48          48 4.360083\n49          49 4.911368\n50          50 4.852261\n51          51 4.873567\n52          52 4.223682\n53          53 4.223682\n54          54 4.668651\n55          55 4.668651\n56          56 4.668651\n57          57 4.781760\n58          58 3.821693\n59          59 3.493641\n60          60 3.493641\n61          61 3.493641\n62          62 3.884056\n63          63 4.300062\n64          64 4.300062\n65          65 3.328843\n66          66 3.527962\n67          67 3.527962\n68          68 3.779240\n69          69 3.779240\n70          70 3.780996\n71          71 3.780996\n72          72 4.254152\n73          73 4.254152\n74          74 4.059470\n75          75 4.188956\n76          76 4.051575\n77          77 4.552475\n78          78 4.390812\n79          79 4.351688\n80          80 4.668447\n81          81 4.268557\n82          82 4.181995\n83          83 4.181995\n84          84 4.181995\n85          85 4.181995\n86          86 4.181995\n87          87 4.181995\n88          88 4.109672\n89          89 4.239377\n90          90 4.239377\n91          91 4.824161\n92          92 4.418431\n93          93 3.717404\n94          94 3.833107\n95          95 3.833107\n96          96 3.458852\n97          97 4.304572\n98          98 3.662805\n99          99 4.358960\n100        100 3.367187\n101        101 3.267318\n102        102 3.074715\n103        103 3.989731\n104        104 4.100588\n105        105 4.874696\n106        106 4.866345\n107        107 4.866345\n108        108 4.806430\n109        109 4.434466\n110        110 3.849844\n111        111 3.890983\n112        112 3.684306\n113        113 3.684306\n114        114 4.138475\n115        115 4.502090\n116        116 4.502090\n117        117 4.502090\n118        118 4.789789\n119        119 4.789789\n120        120 4.789789\n121        121 4.078076\n122        122 4.251318\n123        123 3.688380\n124        124 3.688380\n125        125 3.426818\n126        126 4.010000\n127        127 3.063371\n128        128 2.964325\n129        129 3.161435\n130        130 3.161435\n131        131 3.572916\n132        132 3.586693\n133        133 3.586693\n134        134 3.283902\n135        135 4.161770\n136        136 4.517252\n137        137 4.078432\n138        138 4.078432\n139        139 4.078432\n140        140 4.447494\n141        141 3.786528\n142        142 4.501295\n143        143 4.007232\n144        144 3.466900\n145        145 3.466900\n146        146 3.968057\n147        147 3.968057\n148        148 3.461595\n149        149 4.317831\n150        150 4.368273\n151        151 3.779106\n152        152 4.046772\n153        153 4.873271\n154        154 4.626228\n155        155 4.598262\n156        156 4.197095\n157        157 3.868719\n158        158 4.669313\n159        159 4.299140\n160        160 4.575573\n161        161 4.824692\n162        162 4.564282\n163        163 4.540293\n164        164 3.664378\n165        165 4.321152\n166        166 3.943034\n167        167 4.735397\n168        168 4.735397\n169        169 4.735397\n170        170 4.500015\n171        171 3.829047\n172        172 3.829047\n173        173 4.725888\n174        174 4.587409\n175        175 4.587409\n176        176 4.587409\n177        177 4.587409\n178        178 5.099503\n179        179 5.099503\n180        180 4.196799\n181        181 4.196799\n182        182 3.555545\n183        183 3.303441\n184        184 2.982426\n185        185 3.213581\n186        186 3.278571\n187        187 3.278571\n188        188 3.278571\n189        189 3.278571\n190        190 3.278571\n191        191 2.705338\n192        192 3.027613\n193        193 2.633847\n194        194 2.484903\n195        195 2.484903\n196        196 2.484903\n197        197 2.642887\n198        198 3.022875\n199        199 3.022875\n200        200 3.102603\n201        201 3.686670\n202        202 3.432549\n203        203 3.439926\n204        204 3.723400\n205        205 4.148887\n206        206 3.336801\n207        207 3.336801\n208        208 3.336801\n209        209 3.498253\n210        210 3.498253\n211        211 4.392258\n212        212 4.392258\n213        213 4.392258\n214        214 4.085626\n215        215 4.896237\n216        216 4.896237\n217        217 4.896237\n218        218 4.098371\n219        219 5.032143\n220        220 5.032143\n221        221 5.032143\n222        222 4.802038\n223        223 4.492092\n224        224 3.583201\n225        225 4.375874\n226        226 4.130088\n227        227 4.175770\n228        228 4.785041\n229        229 3.843940\n230        230 3.950791\n231        231 4.591781\n232        232 4.591781\n233        233 4.327596\n234        234 5.006056\n235        235 5.370013\n236        236 5.370013\n237        237 5.370013\n238        238 5.370013\n239        239 5.029932\n240        240 4.935276\n241        241 4.179308\n242        242 4.371599\n243        243 4.371599\n244        244 4.371599\n245        245 3.485206\n246        246 3.485206\n247        247 4.028000\n248        248 3.585162\n249        249 4.490985\n250        250 3.569923\n251        251 3.532921\n252        252 4.261754\n253        253 3.804775\n254        254 4.342763\n255        255 4.354485\n256        256 3.733083\n257        257 3.228354\n258        258 3.676738\n259        259 2.870882\n260        260 3.213805\n261        261 4.140461\n262        262 4.661177\n263        263 4.661177\n264        264 4.661177\n265        265 4.661177\n266        266 4.147741\n267        267 4.147741\n268        268 4.583048\n269        269 4.940091\n270        270 4.934305\n271        271 4.546098\n272        272 4.187681\n273        273 4.187681\n274        274 4.187681\n275        275 3.613630\n276        276 4.561734\n277        277 4.561734\n278        278 3.854234\n279        279 3.653688\n280        280 3.653688\n281        281 4.220867\n282        282 3.495115\n283        283 3.500743\n284        284 4.316703\n285        285 4.204683\n286        286 4.190194\n287        287 3.437507\n288        288 4.240020\n289        289 4.632538\n290        290 3.821359\n291        291 3.859858\n292        292 3.859858\n293        293 4.094144\n294        294 4.094144\n295        295 4.012004\n296        296 4.012004\n297        297 3.247150\n298        298 3.352283\n299        299 4.137156\n300        300 4.481057\n301        301 4.156216\n302        302 3.829761\n303        303 4.021829\n304        304 4.280820\n305        305 3.577093\n306        306 4.488123\n307        307 4.905710\n308        308 4.905710\n309        309 4.905710\n310        310 4.538205\n311        311 3.542928\n312        312 3.220672\n313        313 3.220672\n314        314 3.906610\n315        315 3.625830\n316        316 3.516210\n317        317 3.516210\n318        318 3.516210\n319        319 3.405890\n320        320 4.335709\n321        321 3.700011\n322        322 4.251093\n323        323 4.125515\n324        324 4.571934\n325        325 4.571934\n326        326 4.135308\n327        327 3.425461\n328        328 3.610846\n329        329 4.373565\n330        330 4.301301\n331        331 4.301301\n332        332 4.301301\n333        333 4.319508\n334        334 4.208150\n335        335 4.208150\n336        336 4.386695\n337        337 4.254910\n338        338 4.254910\n339        339 3.388520\n340        340 3.186441\n341        341 3.874856\n342        342 4.220098\n343        343 4.156070\n344        344 4.156070\n345        345 3.860536\n346        346 4.322119\n347        347 3.835370\n348        348 3.835370\n349        349 4.701002\n350        350 4.701002\n351        351 4.701002\n352        352 4.398217\n353        353 3.622017\n354        354 4.042207\n355        355 4.044158\n356        356 4.044158\n357        357 4.044158\n358        358 3.244171\n359        359 3.244171\n360        360 3.244171\n361        361 3.244171\n362        362 3.015580\n363        363 3.225149\n364        364 3.225149\n365        365 3.225149\n366        366 3.876821\n367        367 3.874613\n368        368 3.844240\n369        369 3.685250\n370        370 3.685250\n371        371 4.619042\n372        372 3.716451\n373        373 3.779932\n374        374 3.779932\n375        375 4.656138\n376        376 4.656138\n377        377 4.656138\n378        378 4.689923\n379        379 4.689923\n380        380 4.370310\n381        381 4.331785\n382        382 4.781290\n383        383 3.998273\n384        384 3.998273\n385        385 4.469536\n386        386 3.643572\n387        387 3.533610\n388        388 3.574168\n389        389 4.492307\n390        390 4.941083\n391        391 4.113330\n392        392 4.113330\n393        393 4.615402\n394        394 4.615402\n395        395 4.437704\n396        396 4.437704\n397        397 3.932785\n398        398 4.403139\n399        399 4.403139\n400        400 4.368314\n401        401 3.844164\n402        402 3.986889\n403        403 3.207135\n404        404 3.230388\n405        405 4.177697\n406        406 4.365081\n407        407 3.801211\n408        408 3.132024\n409        409 3.448442\n410        410 3.448442\n411        411 4.426070\n412        412 4.481196\n413        413 3.684190\n414        414 3.684190\n415        415 3.372533\n416        416 3.486818\n417        417 3.272268\n418        418 3.404171\n419        419 4.394728\n420        420 4.394728\n421        421 4.518140\n422        422 3.639642\n423        423 3.639642\n424        424 3.094675\n425        425 4.053954\n426        426 4.053954\n427        427 4.053954\n428        428 4.053954\n429        429 3.414586\n430        430 3.771182\n431        431 3.589519\n432        432 3.809596\n433        433 4.204490\n434        434 4.204490\n435        435 4.670682\n436        436 4.670682\n437        437 4.670682\n438        438 5.052043\n439        439 5.052043\n440        440 4.505513\n441        441 3.787187\n442        442 3.542476\n443        443 4.487155\n444        444 3.783282\n445        445 3.783282\n446        446 3.783282\n447        447 3.783282\n448        448 3.783282\n449        449 4.464769\n450        450 3.935119\n451        451 4.908981\n452        452 5.014038\n453        453 5.014038\n454        454 4.399893\n455        455 3.661593\n456        456 4.316776\n457        457 4.315979\n458        458 4.420200\n459        459 3.762675\n460        460 3.762675\n461        461 3.762675\n462        462 3.901845\n463        463 3.901845\n464        464 3.901845\n465        465 4.678030\n466        466 4.211583\n467        467 4.362544\n468        468 4.362544\n469        469 3.606644\n470        470 4.465292\n471        471 4.465292\n472        472 4.014775\n473        473 4.014775\n474        474 3.802409\n475        475 4.524733\n476        476 3.906435\n477        477 4.451970\n478        478 4.451970\n479        479 4.123520\n480        480 3.123924\n481        481 3.123924\n482        482 2.712415\n483        483 3.181435\n484        484 3.181435\n485        485 3.955556\n486        486 3.955556\n487        487 3.691097\n488        488 3.776608\n489        489 4.008150\n490        490 4.599326\n491        491 4.599326\n492        492 3.673844\n493        493 3.293114\n494        494 3.293114\n495        495 4.073933\n496        496 3.936516\n497        497 3.936516\n498        498 3.534206\n499        499 3.483430\n500        500 3.483430\n501        501 4.406248\n502        502 4.913519\n503        503 5.125397\n504        504 5.404144\n505        505 5.404144\n506        506 4.561072\n507        507 4.482502\n508        508 4.184904\n509        509 3.715025\n510        510 3.715025\n511        511 3.715025\n512        512 3.174609\n513        513 3.174609\n514        514 3.070889\n515        515 3.935781\n516        516 3.935781\n517        517 4.061699\n518        518 4.061699\n519        519 4.061699\n520        520 3.727314\n521        521 3.985771\n522        522 4.752171\n523        523 4.339188\n524        524 4.100689\n525        525 4.100689\n526        526 3.675355\n527        527 3.675355\n528        528 4.307914\n529        529 3.665428\n530        530 4.216894\n531        531 3.889872\n532        532 3.248164\n533        533 2.824387\n534        534 2.802191\n535        535 3.178240\n536        536 3.680044\n537        537 3.680044\n538        538 4.092507\n539        539 4.092507\n540        540 3.474042\n541        541 3.488420\n542        542 3.797215\n543        543 3.797215\n544        544 3.797215\n545        545 4.132450\n546        546 4.394167\n547        547 4.153387\n548        548 4.285232\n549        549 4.285232\n550        550 4.575872\n551        551 4.231047\n552        552 3.919616\n553        553 3.361057\n554        554 2.810364\n555        555 2.336742\n556        556 2.336742\n557        557 2.336742\n558        558 2.372233\n559        559 2.372233\n560        560 2.372233\n561        561 2.503272\n562        562 2.508065\n563        563 2.112172\n564        564 2.112172\n565        565 2.112172\n566        566 2.112172\n567        567 2.942376\n568        568 2.942376\n569        569 3.823058\n570        570 4.171479\n571        571 4.171479\n572        572 3.815215\n573        573 4.796965\n574        574 4.503910\n575        575 4.503910\n576        576 4.275432\n577        577 3.701334\n578        578 3.701334\n579        579 2.916686\n580        580 3.736539\n581        581 3.514773\n582        582 3.744601\n583        583 3.445719\n584        584 4.091798\n585        585 3.340584\n586        586 3.340584\n587        587 3.600943\n588        588 3.600943\n589        589 3.446181\n590        590 4.034488\n591        591 4.654445\n592        592 5.104952\n593        593 4.130850\n594        594 4.130850\n595        595 4.130850\n596        596 4.130850\n597        597 4.019754\n598        598 4.019754\n599        599 4.019754\n600        600 4.019754\n601        601 4.019754\n602        602 4.625131\n603        603 3.793971\n604        604 3.793971\n605        605 3.793971\n606        606 3.851183\n607        607 4.362899\n608        608 4.362899\n609        609 3.868660\n610        610 3.868660\n611        611 3.742066\n612        612 4.146093\n613        613 3.592380\n614        614 3.739237\n615        615 3.739237\n616        616 3.739237\n617        617 3.739237\n618        618 3.839678\n619        619 3.096682\n620        620 3.096682\n621        621 3.869243\n622        622 4.458705\n623        623 4.754726\n624        624 4.754726\n625        625 4.841962\n626        626 4.470938\n627        627 4.294294\n628        628 4.294294\n629        629 5.158888\n630        630 4.430674\n631        631 4.430674\n632        632 5.053898\n633        633 4.857023\n634        634 4.857023\n635        635 4.857023\n636        636 4.857023\n637        637 4.587770\n638        638 4.614246\n639        639 4.220323\n640        640 4.220323\n641        641 5.157545\n642        642 5.157545\n643        643 5.157545\n644        644 5.157545\n645        645 4.214400\n646        646 3.558453\n647        647 3.558453\n648        648 4.336706\n649        649 4.034915\n650        650 3.681421\n651        651 3.425655\n652        652 3.425655\n653        653 2.925351\n654        654 2.781998\n655        655 3.532627\n656        656 3.532627\n657        657 3.523843\n658        658 3.595417\n659        659 4.282173\n660        660 3.693431\n661        661 3.693431\n662        662 4.055027\n663        663 4.769036\n664        664 4.518976\n665        665 4.737618\n666        666 3.937952\n667        667 4.113164\n668        668 4.816257\n669        669 4.816257\n670        670 4.178165\n671        671 4.015863\n672        672 4.015863\n673        673 4.015863\n674        674 4.015863\n675        675 4.015863\n676        676 3.794947\n677        677 3.794947\n678        678 3.794947\n679        679 4.668309\n680        680 4.148566\n681        681 3.875682\n682        682 3.875682\n683        683 3.875682\n684        684 4.406480\n685        685 4.695461\n686        686 4.695461\n687        687 4.792903\n688        688 4.792903\n689        689 5.195069\n690        690 4.853294\n691        691 4.604240\n692        692 4.297441\n693        693 3.891713\n694        694 4.777501\n695        695 4.015205\n696        696 4.015205\n697        697 4.015205\n698        698 4.427694\n699        699 3.772296\n700        700 4.441724\n701        701 3.853056\n702        702 3.853056\n703        703 4.282181\n704        704 4.263105\n705        705 5.207689\n706        706 4.760868\n707        707 3.939738\n708        708 3.883318\n709        709 3.126788\n710        710 4.032509\n711        711 3.793278\n712        712 4.549958\n713        713 3.833588\n714        714 3.749533\n715        715 3.749533\n716        716 4.656218\n717        717 5.432463\n718        718 5.432463\n719        719 5.274635\n720        720 5.274635\n721        721 5.424436\n722        722 5.250035\n723        723 5.250035\n724        724 5.250035\n725        725 4.389804\n726        726 4.389804\n727        727 4.821098\n728        728 3.834943\n729        729 3.834943\n730        730 3.834943\n731        731 3.989233\n732        732 3.629274\n733        733 3.743298\n734        734 4.326722\n735        735 3.427184\n736        736 3.427184\n737        737 2.938889\n738        738 2.938889\n739        739 3.844963\n740        740 4.463619\n741        741 3.483594\n742        742 3.636439\n743        743 4.111519\n744        744 4.111519\n745        745 4.111519\n746        746 3.552209\n747        747 3.776676\n748        748 3.926741\n749        749 4.040168\n750        750 4.670694\n751        751 3.778085\n752        752 4.237158\n753        753 3.850951\n754        754 4.032218\n755        755 4.032218\n756        756 4.504179\n757        757 3.945085\n758        758 3.945085\n759        759 3.640790\n760        760 3.974820\n761        761 4.507166\n762        762 3.594449\n763        763 4.528077\n764        764 4.528077\n765        765 3.769859\n766        766 2.872785\n767        767 2.872785\n768        768 3.208215\n769        769 3.208215\n770        770 3.417366\n771        771 3.417366\n772        772 3.417366\n773        773 3.417366\n774        774 3.188731\n775        775 3.466660\n776        776 3.813506\n777        777 3.309569\n778        778 2.913815\n779        779 2.888828\n780        780 3.076160\n781        781 3.076160\n782        782 3.076160\n783        783 3.376151\n784        784 3.965846\n785        785 3.565094\n786        786 4.328360\n787        787 4.495833\n788        788 3.807508\n789        789 4.054889\n790        790 4.054889\n791        791 4.054889\n792        792 4.295893\n793        793 4.295893\n794        794 4.283788\n795        795 3.877103\n796        796 3.877103\n797        797 3.877103\n798        798 3.877103\n799        799 3.988192\n800        800 4.509789\n801        801 4.266080\n802        802 4.616652\n803        803 4.558833\n804        804 4.391088\n805        805 4.391088\n806        806 3.882082\n807        807 4.381909\n808        808 4.373398\n809        809 4.672220\n810        810 3.855867\n811        811 4.535540\n812        812 4.535540\n813        813 4.581379\n814        814 4.366865\n815        815 3.855023\n816        816 3.460111\n817        817 4.234576\n818        818 4.492705\n819        819 4.421435\n820        820 4.421435\n821        821 4.867861\n822        822 4.890388\n823        823 4.890388\n824        824 4.220136\n825        825 4.284045\n826        826 4.572039\n827        827 4.750213\n828        828 4.750213\n829        829 4.352590\n830        830 4.352590\n831        831 3.996333\n832        832 3.625446\n833        833 2.784832\n834        834 3.191245\n835        835 3.191245\n836        836 3.689628\n837        837 3.966941\n838        838 4.367621\n839        839 3.775309\n840        840 3.145145\n841        841 2.653292\n842        842 3.001626\n843        843 3.001626\n844        844 3.001626\n845        845 3.365216\n846        846 4.182327\n847        847 4.182327\n848        848 3.972220\n849        849 3.972220\n850        850 3.972220\n851        851 4.121505\n852        852 3.433940\n853        853 3.433940\n854        854 4.086985\n855        855 4.086985\n856        856 4.125790\n857        857 4.659145\n858        858 3.849656\n859        859 4.235081\n860        860 4.416978\n861        861 4.664141\n862        862 4.664141\n863        863 5.082941\n864        864 5.082941\n865        865 4.979750\n866        866 4.877787\n867        867 4.877787\n868        868 4.877787\n869        869 5.223185\n870        870 4.803052\n871        871 3.984327\n872        872 4.893315\n873        873 4.893315\n874        874 5.714100\n875        875 5.580386\n876        876 5.580386\n877        877 5.580386\n878        878 5.580386\n879        879 5.580386\n880        880 4.717907\n881        881 4.717907\n882        882 5.020185\n883        883 4.544984\n884        884 4.497713\n885        885 3.938406\n886        886 3.321960\n887        887 3.576006\n888        888 4.183903\n889        889 4.183903\n890        890 4.183903\n891        891 4.183903\n892        892 3.745150\n893        893 4.054115\n894        894 4.165390\n895        895 4.165390\n896        896 4.105277\n897        897 3.561658\n898        898 3.380371\n899        899 3.380371\n900        900 3.244688\n901        901 3.028660\n902        902 3.168962\n903        903 3.856513\n904        904 3.879720\n905        905 4.831072\n906        906 3.854171\n907        907 4.467740\n908        908 4.467740\n909        909 4.467740\n910        910 5.404083\n911        911 4.768986\n912        912 4.768986\n913        913 4.768986\n914        914 4.195794\n915        915 3.563806\n916        916 3.831091\n917        917 3.956544\n918        918 4.934658\n919        919 4.992112\n920        920 4.556169\n921        921 4.556169\n922        922 4.086617\n923        923 3.825978\n924        924 2.900886\n925        925 2.798696\n926        926 3.596835\n927        927 4.498125\n928        928 5.353880\n929        929 4.516264\n930        930 4.516264\n931        931 4.303315\n932        932 4.192841\n933        933 4.192841\n934        934 3.710564\n935        935 3.698163\n936        936 3.698163\n937        937 3.812080\n938        938 3.862243\n939        939 4.385856\n940        940 4.710098\n941        941 4.374291\n942        942 3.664906\n943        943 3.664906\n944        944 3.664906\n945        945 4.373420\n946        946 3.601915\n947        947 4.288163\n948        948 3.701090\n949        949 4.031237\n950        950 4.044281\n951        951 4.044281\n952        952 4.748724\n953        953 4.448163\n954        954 3.684749\n955        955 4.005413\n956        956 4.180919\n957        957 3.772047\n958        958 4.603614\n959        959 4.510260\n960        960 4.510260\n961        961 3.895654\n962        962 3.167235\n963        963 3.167235\n964        964 3.204292\n965        965 3.204292\n966        966 4.015071\n967        967 4.015071\n968        968 4.325855\n969        969 4.325855\n970        970 4.326799\n971        971 3.565411\n972        972 3.438741\n973        973 3.438741\n974        974 4.208365\n975        975 3.617114\n976        976 3.617114\n977        977 3.858926\n978        978 3.962277\n979        979 4.601514\n980        980 4.601514\n981        981 4.061617\n982        982 4.061617\n983        983 4.291492\n984        984 3.625616\n985        985 3.625616\n986        986 4.184822\n987        987 4.109083\n988        988 4.903170\n989        989 5.051133\n990        990 4.665438\n991        991 4.202858\n992        992 3.754538\n993        993 3.257053\n994        994 2.915053\n995        995 2.915053\n996        996 3.257543\n997        997 3.670154\n998        998 3.236360\n999        999 3.236360\n1000      1000 3.236360\n1001      1001 3.236360\n1002      1002 3.658160\n1003      1003 4.244403\n1004      1004 3.269727\n1005      1005 3.269727\n1006      1006 3.195231\n1007      1007 3.195231\n1008      1008 3.195231\n1009      1009 3.844708\n1010      1010 3.844708\n1011      1011 2.977810\n1012      1012 2.835355\n1013      1013 2.835355\n1014      1014 2.350116\n1015      1015 2.350116\n1016      1016 2.350116\n1017      1017 2.350116\n1018      1018 3.086784\n1019      1019 3.928866\n1020      1020 3.928866\n1021      1021 3.799334\n1022      1022 3.295321\n1023      1023 3.295321\n1024      1024 4.145564\n1025      1025 4.145564\n1026      1026 4.521988\n1027      1027 4.379050\n1028      1028 4.494545\n1029      1029 4.494545\n1030      1030 4.494545\n1031      1031 3.998812\n1032      1032 3.991359\n1033      1033 3.991359\n1034      1034 3.991359\n1035      1035 3.615464\n1036      1036 4.051579\n1037      1037 4.818003\n1038      1038 4.818003\n1039      1039 4.325982\n1040      1040 3.910715\n1041      1041 3.842405\n1042      1042 4.389819\n1043      1043 4.389819\n1044      1044 3.518761\n1045      1045 3.661744\n1046      1046 4.045918\n1047      1047 4.730969\n1048      1048 3.859032\n1049      1049 4.300143\n1050      1050 4.300143\n1051      1051 3.469658\n1052      1052 3.394547\n1053      1053 3.394547\n1054      1054 3.480900\n1055      1055 3.146684\n1056      1056 3.146684\n1057      1057 3.146684\n1058      1058 3.265957\n1059      1059 3.360181\n1060      1060 3.396330\n1061      1061 4.043504\n1062      1062 4.043504\n1063      1063 3.500077\n1064      1064 3.234326\n1065      1065 3.967965\n1066      1066 4.501975\n1067      1067 4.245195\n1068      1068 4.245195\n1069      1069 4.059587\n1070      1070 4.059587\n1071      1071 4.059587\n1072      1072 4.059587\n1073      1073 4.059587\n1074      1074 4.255936\n1075      1075 3.795969\n1076      1076 3.088558\n1077      1077 3.706220\n1078      1078 3.655306\n1079      1079 3.467549\n1080      1080 4.162962\n1081      1081 3.348083\n1082      1082 3.348083\n1083      1083 3.348083\n1084      1084 3.348083\n1085      1085 4.138644\n1086      1086 4.238343\n1087      1087 3.876203\n1088      1088 3.876203\n1089      1089 3.078335\n1090      1090 3.078335\n1091      1091 3.780605\n1092      1092 3.158990\n1093      1093 3.158990\n1094      1094 2.783502\n1095      1095 3.330235\n1096      1096 3.330235\n1097      1097 3.739375\n1098      1098 4.303247\n1099      1099 3.610061\n1100      1100 3.610061\n1101      1101 3.610061\n1102      1102 3.610061\n1103      1103 3.610061\n1104      1104 3.118777\n1105      1105 3.694071\n1106      1106 4.068656\n1107      1107 3.597132\n1108      1108 4.146719\n1109      1109 3.849006\n1110      1110 3.849006\n1111      1111 4.590909\n1112      1112 4.739145\n1113      1113 4.019058\n1114      1114 4.095077\n1115      1115 4.095077\n1116      1116 4.449495\n1117      1117 4.449495\n1118      1118 4.458315\n1119      1119 3.948213\n1120      1120 3.948213\n1121      1121 3.740571\n1122      1122 4.662176\n1123      1123 5.179471\n1124      1124 4.553129\n1125      1125 4.553129\n1126      1126 4.553129\n1127      1127 4.713344\n1128      1128 4.713344\n1129      1129 4.237501\n1130      1130 4.237501\n1131      1131 3.685090\n1132      1132 4.190771\n1133      1133 4.318913\n1134      1134 4.318913\n1135      1135 4.522751\n1136      1136 4.164609\n1137      1137 4.289780\n1138      1138 4.137768\n1139      1139 4.478331\n1140      1140 3.825363\n1141      1141 4.495932\n1142      1142 4.742742\n1143      1143 4.513902\n1144      1144 4.513902\n1145      1145 4.009118\n1146      1146 4.009118\n1147      1147 4.748613\n1148      1148 4.722446\n1149      1149 4.125800\n1150      1150 4.933036\n1151      1151 4.933036\n1152      1152 4.933036\n1153      1153 4.933036\n1154      1154 4.933036\n1155      1155 4.933036\n1156      1156 5.379972\n1157      1157 5.379972\n1158      1158 5.379972\n1159      1159 5.379972\n1160      1160 4.976852\n1161      1161 4.744969\n1162      1162 4.744969\n1163      1163 3.746641\n1164      1164 3.746641\n1165      1165 4.305960\n1166      1166 4.697524\n1167      1167 4.653350\n1168      1168 4.194057\n1169      1169 4.194057\n1170      1170 4.394988\n1171      1171 4.156738\n1172      1172 3.630093\n1173      1173 3.772512\n1174      1174 3.772512\n1175      1175 4.347002\n1176      1176 4.937715\n1177      1177 4.937715\n1178      1178 5.272882\n1179      1179 5.272882\n1180      1180 4.606083\n1181      1181 5.262034\n1182      1182 4.500896\n1183      1183 4.358645\n1184      1184 3.427018\n1185      1185 3.817028\n1186      1186 3.817028\n1187      1187 4.108191\n1188      1188 4.639707\n1189      1189 4.639707\n1190      1190 4.639707\n1191      1191 4.047504\n1192      1192 4.047504\n1193      1193 4.047504\n1194      1194 4.289207\n1195      1195 4.518286\n1196      1196 4.629929\n1197      1197 3.658229\n1198      1198 3.658229\n1199      1199 3.729190\n1200      1200 4.474897\n1201      1201 4.474897\n1202      1202 4.563824\n1203      1203 4.563824\n1204      1204 3.884314\n1205      1205 4.020343\n1206      1206 4.695136\n1207      1207 4.695136\n1208      1208 4.773498\n1209      1209 4.773498\n1210      1210 4.037525\n1211      1211 3.678599\n1212      1212 3.678599\n1213      1213 3.678599\n1214      1214 4.493919\n1215      1215 4.503218\n1216      1216 4.288096\n1217      1217 3.895830\n1218      1218 4.391941\n1219      1219 4.359185\n1220      1220 4.340272\n1221      1221 3.879291\n1222      1222 3.489372\n1223      1223 3.489372\n1224      1224 4.345961\n1225      1225 4.345961\n1226      1226 4.287265\n1227      1227 3.378094\n1228      1228 3.378094\n1229      1229 3.378094\n1230      1230 3.378094\n1231      1231 3.993294\n1232      1232 3.309183\n1233      1233 3.432157\n1234      1234 4.358611\n1235      1235 3.412697\n1236      1236 3.752775\n1237      1237 3.752775\n1238      1238 3.752775\n1239      1239 4.003299\n1240      1240 4.003299\n1241      1241 3.093897\n1242      1242 3.862297\n1243      1243 3.816434\n1244      1244 3.772006\n1245      1245 4.686852\n1246      1246 4.686852\n1247      1247 4.230162\n1248      1248 4.230162\n1249      1249 4.842122\n1250      1250 5.621636\n1251      1251 5.621636\n1252      1252 5.621636\n1253      1253 5.396979\n1254      1254 4.978590\n1255      1255 4.978590\n1256      1256 4.045424\n1257      1257 3.835321\n1258      1258 4.259081\n1259      1259 4.360686\n1260      1260 4.360686\n1261      1261 3.790096\n1262      1262 3.790096\n1263      1263 3.790096\n1264      1264 3.790096\n1265      1265 3.790096\n1266      1266 4.704296\n1267      1267 4.466859\n1268      1268 4.414813\n1269      1269 3.598534\n1270      1270 4.191219\n1271      1271 4.191219\n1272      1272 5.011109\n1273      1273 5.011109\n1274      1274 5.011109\n1275      1275 4.997761\n1276      1276 5.479545\n1277      1277 4.847538\n1278      1278 4.533093\n1279      1279 5.286299\n1280      1280 5.286299\n1281      1281 4.923012\n1282      1282 4.806612\n1283      1283 3.959028\n1284      1284 4.340241\n1285      1285 3.932544\n1286      1286 3.854116\n1287      1287 3.934806\n1288      1288 3.934806\n1289      1289 3.833610\n1290      1290 3.784374\n1291      1291 3.784374\n1292      1292 4.711182\n1293      1293 4.711182\n1294      1294 4.790225\n1295      1295 4.790225\n1296      1296 3.874682\n1297      1297 3.874682\n1298      1298 4.203180\n1299      1299 4.550790\n1300      1300 4.550790\n1301      1301 4.663265\n1302      1302 3.884740\n1303      1303 3.884740\n1304      1304 3.884740\n1305      1305 3.774086\n1306      1306 4.298138\n1307      1307 4.481404\n1308      1308 3.792296\n1309      1309 3.799225\n1310      1310 2.858162\n1311      1311 3.528023\n1312      1312 3.528023\n1313      1313 3.121877\n1314      1314 3.960247\n1315      1315 4.043145\n1316      1316 3.544455\n1317      1317 3.538508\n1318      1318 3.168165\n1319      1319 3.418650\n1320      1320 3.254318\n1321      1321 3.476240\n1322      1322 4.131689\n1323      1323 4.131689\n1324      1324 4.868699\n1325      1325 4.868699\n1326      1326 4.795215\n1327      1327 4.708671\n1328      1328 4.298022\n1329      1329 4.301737\n1330      1330 4.482439\n1331      1331 4.492415\n1332      1332 4.492415\n1333      1333 3.809826\n1334      1334 4.724805\n1335      1335 4.396179\n1336      1336 4.096226\n1337      1337 4.267984\n1338      1338 4.267984\n1339      1339 3.757906\n1340      1340 3.806807\n1341      1341 4.563722\n1342      1342 4.574484\n1343      1343 4.574484\n1344      1344 3.629111\n1345      1345 2.877506\n1346      1346 3.538087\n1347      1347 3.538087\n1348      1348 3.082050\n1349      1349 3.012733\n1350      1350 3.414436\n1351      1351 3.936000\n1352      1352 3.763067\n1353      1353 3.763067\n1354      1354 4.432060\n1355      1355 4.317389\n1356      1356 5.198888\n1357      1357 5.376058\n1358      1358 5.598222\n1359      1359 5.598222\n1360      1360 5.598222\n1361      1361 5.142452\n1362      1362 5.142452\n1363      1363 4.980242\n1364      1364 4.430026\n1365      1365 3.526106\n1366      1366 3.994995\n1367      1367 3.994995\n1368      1368 3.994995\n1369      1369 3.994995\n1370      1370 4.152179\n1371      1371 4.152179\n1372      1372 4.152179\n1373      1373 3.520611\n1374      1374 3.520611\n1375      1375 4.202689\n1376      1376 3.880598\n1377      1377 3.880598\n1378      1378 3.164542\n1379      1379 3.285195\n1380      1380 3.285195\n1381      1381 3.603426\n1382      1382 3.603426\n1383      1383 4.032126\n1384      1384 3.739741\n1385      1385 3.265478\n1386      1386 3.281359\n1387      1387 3.998952\n1388      1388 4.153139\n1389      1389 4.153139\n1390      1390 4.241053\n1391      1391 4.241053\n1392      1392 4.241053\n1393      1393 4.726530\n1394      1394 4.413772\n1395      1395 4.413772\n1396      1396 3.736702\n1397      1397 3.736702\n1398      1398 3.833495\n1399      1399 3.424398\n1400      1400 3.801600\n1401      1401 3.801600\n1402      1402 4.279947\n1403      1403 3.453267\n1404      1404 3.453267\n1405      1405 3.453267\n1406      1406 3.453267\n1407      1407 3.453267\n1408      1408 3.453267\n1409      1409 3.321559\n1410      1410 3.591647\n1411      1411 3.591647\n1412      1412 3.591647\n1413      1413 3.591647\n1414      1414 3.358537\n1415      1415 3.267718\n1416      1416 3.476437\n1417      1417 3.777801\n1418      1418 3.777801\n1419      1419 4.167690\n1420      1420 4.167690\n1421      1421 3.498083\n1422      1422 2.960859\n1423      1423 3.468456\n1424      1424 4.235248\n1425      1425 4.235248\n1426      1426 3.471640\n1427      1427 3.944533\n1428      1428 3.603604\n1429      1429 3.603604\n1430      1430 4.349870\n1431      1431 4.349870\n1432      1432 4.349870\n1433      1433 4.819062\n1434      1434 4.819062\n1435      1435 4.819062\n1436      1436 4.346834\n1437      1437 3.962527\n1438      1438 3.962527\n1439      1439 3.484911\n1440      1440 3.484911\n1441      1441 4.261162\n1442      1442 4.261162\n1443      1443 4.970323\n1444      1444 4.970323\n1445      1445 5.352120\n1446      1446 5.352120\n1447      1447 5.352120\n1448      1448 5.352120\n1449      1449 5.245250\n1450      1450 5.245250\n1451      1451 5.202280\n1452      1452 4.989549\n1453      1453 4.119569\n1454      1454 4.119569\n1455      1455 4.244743\n1456      1456 4.540354\n1457      1457 3.736294\n1458      1458 3.736294\n1459      1459 4.003365\n1460      1460 3.594451\n1461      1461 4.505685\n1462      1462 3.612225\n1463      1463 4.583427\n1464      1464 3.881979\n1465      1465 3.881979\n1466      1466 3.881979\n1467      1467 4.212091\n1468      1468 4.646394\n1469      1469 4.646394\n1470      1470 4.467509\n1471      1471 4.616544\n1472      1472 3.989190\n1473      1473 3.989190\n1474      1474 3.626374\n1475      1475 3.626374\n1476      1476 3.445570\n1477      1477 4.156116\n1478      1478 4.147802\n1479      1479 3.611184\n1480      1480 3.611184\n1481      1481 3.977293\n1482      1482 4.965265\n1483      1483 4.946186\n1484      1484 4.302063\n1485      1485 4.302063\n1486      1486 4.658155\n1487      1487 4.551234\n1488      1488 4.238241\n1489      1489 4.142874\n1490      1490 4.206548\n1491      1491 4.206548\n1492      1492 4.997264\n1493      1493 4.014246\n1494      1494 4.141750\n1495      1495 3.586130\n1496      1496 3.586130\n1497      1497 3.645481\n1498      1498 3.645481\n1499      1499 3.684383\n1500      1500 3.187283\n1501      1501 3.187283\n1502      1502 4.113630\n1503      1503 3.897649\n1504      1504 3.897649\n1505      1505 3.897649\n1506      1506 3.897649\n1507      1507 3.059222\n1508      1508 3.059222\n1509      1509 3.755450\n1510      1510 3.268724\n1511      1511 4.212034\n1512      1512 4.577169\n1513      1513 4.735911\n1514      1514 4.596780\n1515      1515 4.596780\n1516      1516 4.727390\n1517      1517 5.119336\n1518      1518 4.153925\n1519      1519 4.357148\n1520      1520 4.759256\n1521      1521 4.311923\n1522      1522 3.964425\n1523      1523 4.251734\n1524      1524 4.251734\n1525      1525 3.455051\n1526      1526 3.455051\n1527      1527 3.631680\n1528      1528 4.171160\n1529      1529 3.834426\n1530      1530 3.834426\n1531      1531 4.735582\n1532      1532 4.735582\n1533      1533 3.793286\n1534      1534 3.793286\n1535      1535 2.909969\n1536      1536 2.775716\n1537      1537 3.165988\n1538      1538 3.298009\n1539      1539 3.298009\n1540      1540 3.298009\n1541      1541 3.298009\n1542      1542 3.298009\n1543      1543 3.935875\n1544      1544 3.935875\n1545      1545 4.644062\n1546      1546 4.414839\n1547      1547 4.414839\n1548      1548 4.414839\n1549      1549 4.706686\n1550      1550 4.706686\n1551      1551 4.706686\n1552      1552 4.706686\n1553      1553 4.112072\n1554      1554 4.112072\n1555      1555 4.009856\n1556      1556 4.259517\n1557      1557 4.135738\n1558      1558 4.135738\n1559      1559 4.454507\n1560      1560 4.353626\n1561      1561 4.353626\n1562      1562 4.353626\n1563      1563 3.773068\n1564      1564 3.873828\n1565      1565 3.300667\n1566      1566 3.271155\n1567      1567 3.271155\n1568      1568 3.271155\n1569      1569 2.987839\n1570      1570 2.861000\n1571      1571 2.909309\n1572      1572 2.909309\n1573      1573 2.909309\n1574      1574 3.051776\n1575      1575 3.051776\n1576      1576 3.035802\n1577      1577 4.019790\n1578      1578 3.502427\n1579      1579 3.963252\n1580      1580 3.627674\n1581      1581 3.627674\n1582      1582 2.839538\n1583      1583 3.752396\n1584      1584 3.792054\n1585      1585 4.107377\n1586      1586 4.457337\n1587      1587 4.950349\n1588      1588 4.950349\n1589      1589 4.950349\n1590      1590 4.683318\n1591      1591 4.367695\n1592      1592 4.267138\n1593      1593 4.550103\n1594      1594 3.553523\n1595      1595 3.650669\n1596      1596 3.390417\n1597      1597 3.276441\n1598      1598 3.051275\n1599      1599 2.733983\n1600      1600 2.733983\n1601      1601 3.587701\n1602      1602 3.587701\n1603      1603 3.587701\n1604      1604 3.776816\n1605      1605 4.080670\n1606      1606 4.099581\n1607      1607 3.821164\n1608      1608 3.821164\n1609      1609 3.821164\n1610      1610 3.821164\n1611      1611 4.246461\n1612      1612 4.668646\n1613      1613 4.854233\n1614      1614 4.854233\n1615      1615 5.071468\n1616      1616 4.198933\n1617      1617 3.660255\n1618      1618 3.972705\n1619      1619 3.972705\n1620      1620 3.972705\n1621      1621 4.851319\n1622      1622 4.851319\n1623      1623 4.851319\n1624      1624 4.581810\n1625      1625 4.581810\n1626      1626 3.858824\n1627      1627 3.626615\n1628      1628 3.626615\n1629      1629 3.626615\n1630      1630 3.942276\n1631      1631 4.007457\n1632      1632 3.527747\n1633      1633 3.527747\n1634      1634 3.469504\n1635      1635 3.367243\n1636      1636 4.177504\n1637      1637 3.263582\n1638      1638 3.263582\n1639      1639 3.452611\n1640      1640 4.213145\n1641      1641 4.213145\n1642      1642 4.733240\n1643      1643 3.847755\n1644      1644 3.847755\n1645      1645 3.004179\n1646      1646 3.004179\n1647      1647 2.546723\n1648      1648 2.929321\n1649      1649 2.929321\n1650      1650 2.929321\n1651      1651 3.379436\n1652      1652 3.379436\n1653      1653 4.242506\n1654      1654 4.242506\n1655      1655 4.146294\n1656      1656 4.669044\n1657      1657 3.835168\n1658      1658 4.334989\n1659      1659 4.291817\n1660      1660 4.448277\n1661      1661 4.098032\n1662      1662 3.870617\n1663      1663 4.508329\n1664      1664 4.076289\n1665      1665 4.076289\n1666      1666 4.076289\n1667      1667 3.514407\n1668      1668 3.482458\n1669      1669 3.563768\n1670      1670 4.506380\n1671      1671 4.431305\n1672      1672 4.314337\n1673      1673 4.314337\n1674      1674 4.314337\n1675      1675 4.314337\n1676      1676 3.562367\n1677      1677 4.324236\n1678      1678 4.253060\n1679      1679 3.341149\n1680      1680 3.341149\n1681      1681 4.010093\n1682      1682 4.010093\n1683      1683 4.010093\n1684      1684 3.239338\n1685      1685 3.937556\n1686      1686 3.929993\n1687      1687 4.729399\n1688      1688 4.573975\n1689      1689 4.850180\n1690      1690 4.407050\n1691      1691 4.022631\n1692      1692 4.562159\n1693      1693 4.562159\n1694      1694 3.838014\n1695      1695 4.331876\n1696      1696 4.331876\n1697      1697 3.629871\n1698      1698 4.513214\n1699      1699 4.439725\n1700      1700 3.945108\n1701      1701 3.036338\n1702      1702 3.036338\n1703      1703 3.036338\n1704      1704 3.036338\n1705      1705 3.036338\n1706      1706 3.036338\n1707      1707 3.036338\n1708      1708 3.036338\n1709      1709 3.036338\n1710      1710 3.849271\n1711      1711 3.849271\n1712      1712 3.849271\n1713      1713 3.331557\n1714      1714 3.331557\n1715      1715 4.281900\n1716      1716 3.465647\n1717      1717 3.623415\n1718      1718 3.623415\n1719      1719 4.022145\n1720      1720 4.098346\n1721      1721 3.329099\n1722      1722 3.329099\n1723      1723 4.174368\n1724      1724 4.174368\n1725      1725 4.174368\n1726      1726 4.174368\n1727      1727 3.617894\n1728      1728 3.454355\n1729      1729 4.162007\n1730      1730 4.516594\n1731      1731 3.933215\n1732      1732 4.181492\n1733      1733 4.809045\n1734      1734 5.045899\n1735      1735 4.444456\n1736      1736 4.444456\n1737      1737 3.454109\n1738      1738 3.454109\n1739      1739 4.257450\n1740      1740 4.257450\n1741      1741 4.540898\n1742      1742 4.540898\n1743      1743 4.012674\n1744      1744 4.066031\n1745      1745 4.508836\n1746      1746 4.508836\n1747      1747 4.112257\n1748      1748 4.049773\n1749      1749 4.361816\n1750      1750 4.186616\n1751      1751 4.186616\n1752      1752 3.720627\n1753      1753 3.837456\n1754      1754 3.545175\n1755      1755 3.545175\n1756      1756 3.693815\n1757      1757 3.310957\n1758      1758 3.310957\n1759      1759 3.310957\n1760      1760 3.821822\n1761      1761 3.821822\n1762      1762 4.482334\n1763      1763 4.482334\n1764      1764 5.115757\n1765      1765 4.150925\n1766      1766 3.730598\n1767      1767 3.730598\n1768      1768 3.730598\n1769      1769 3.211251\n1770      1770 3.284186\n1771      1771 4.160315\n1772      1772 3.989147\n1773      1773 3.989147\n1774      1774 3.989147\n1775      1775 3.989147\n1776      1776 3.989147\n1777      1777 3.989147\n1778      1778 3.989147\n1779      1779 3.989147\n1780      1780 4.202026\n1781      1781 4.202026\n1782      1782 4.941516\n1783      1783 4.941516\n1784      1784 5.274917\n1785      1785 5.160295\n1786      1786 5.160295\n1787      1787 5.160295\n1788      1788 4.489548\n1789      1789 3.662149\n1790      1790 4.265634\n1791      1791 4.682552\n1792      1792 4.375363\n1793      1793 4.776945\n1794      1794 4.724165\n1795      1795 3.782580\n1796      1796 3.774498\n1797      1797 3.774498\n1798      1798 3.755735\n1799      1799 4.061606\n1800      1800 3.286704\n1801      1801 3.286704\n1802      1802 3.286704\n1803      1803 3.902356\n1804      1804 3.207116\n1805      1805 3.305168\n1806      1806 3.116397\n1807      1807 3.576241\n1808      1808 3.626056\n1809      1809 4.280108\n1810      1810 4.230266\n1811      1811 4.099839\n1812      1812 4.000116\n1813      1813 4.000116\n1814      1814 4.000116\n1815      1815 3.691575\n1816      1816 3.691575\n1817      1817 3.655051\n1818      1818 3.655051\n1819      1819 3.655051\n1820      1820 3.655051\n1821      1821 4.381144\n1822      1822 4.381144\n1823      1823 3.807256\n1824      1824 3.758719\n1825      1825 3.758719\n1826      1826 3.247581\n1827      1827 3.226999\n1828      1828 3.226999\n1829      1829 3.838245\n1830      1830 3.838245\n1831      1831 4.114103\n1832      1832 4.114103\n1833      1833 4.198244\n1834      1834 4.001962\n1835      1835 4.053124\n1836      1836 4.053124\n1837      1837 4.734567\n1838      1838 4.502045\n1839      1839 4.502045\n1840      1840 3.910583\n1841      1841 3.400916\n1842      1842 3.400916\n1843      1843 4.309256\n1844      1844 3.620495\n1845      1845 3.620495\n1846      1846 3.620495\n1847      1847 3.620495\n1848      1848 3.396042\n1849      1849 3.396042\n1850      1850 3.533398\n1851      1851 3.969294\n1852      1852 3.738403\n1853      1853 3.601712\n1854      1854 3.585607\n1855      1855 3.816000\n1856      1856 4.263288\n1857      1857 3.659976\n1858      1858 3.256220\n1859      1859 3.970578\n1860      1860 4.254666\n1861      1861 3.628593\n1862      1862 3.628593\n1863      1863 4.316207\n1864      1864 4.059585\n1865      1865 4.208700\n1866      1866 3.806840\n1867      1867 3.936109\n1868      1868 3.095099\n1869      1869 3.095099\n1870      1870 3.095099\n1871      1871 3.554308\n1872      1872 3.698042\n1873      1873 3.844632\n1874      1874 3.844632\n1875      1875 4.503609\n1876      1876 3.838263\n1877      1877 4.541919\n1878      1878 4.541919\n1879      1879 4.821982\n1880      1880 4.134549\n1881      1881 4.215147\n1882      1882 3.498565\n1883      1883 3.932112\n1884      1884 3.932112\n1885      1885 4.814269\n1886      1886 4.814269\n1887      1887 4.814269\n1888      1888 4.814269\n1889      1889 4.274442\n1890      1890 4.274442\n1891      1891 4.784159\n1892      1892 4.784159\n1893      1893 4.223389\n1894      1894 4.223389\n1895      1895 4.685813\n1896      1896 4.249828\n1897      1897 4.384759\n1898      1898 4.123087\n1899      1899 4.213408\n1900      1900 4.213408\n1901      1901 4.213408\n1902      1902 3.953762\n1903      1903 3.953762\n1904      1904 4.067938\n1905      1905 4.475005\n1906      1906 4.664522\n1907      1907 4.255374\n1908      1908 3.804725\n1909      1909 3.779515\n1910      1910 4.594057\n1911      1911 4.334497\n1912      1912 3.478729\n1913      1913 4.233748\n1914      1914 4.233748\n1915      1915 5.198893\n1916      1916 5.198893\n1917      1917 5.198893\n1918      1918 5.198893\n1919      1919 5.439942\n1920      1920 5.439942\n1921      1921 5.373350\n1922      1922 5.373350\n1923      1923 5.373350\n1924      1924 5.622274\n1925      1925 4.776190\n1926      1926 5.036494\n1927      1927 5.036494\n1928      1928 5.036494\n1929      1929 5.182696\n1930      1930 5.182696\n1931      1931 5.120435\n1932      1932 4.684268\n1933      1933 4.684268\n1934      1934 4.614099\n1935      1935 5.277460\n1936      1936 5.277460\n1937      1937 4.466646\n1938      1938 4.011084\n1939      1939 3.524116\n1940      1940 4.308082\n1941      1941 4.188476\n1942      1942 4.549351\n1943      1943 4.427960\n1944      1944 3.918300\n1945      1945 3.918300\n1946      1946 3.190528\n1947      1947 3.484236\n1948      1948 4.365404\n1949      1949 4.365404\n1950      1950 4.594195\n1951      1951 4.130990\n1952      1952 3.760522\n1953      1953 3.278121\n1954      1954 3.278121\n1955      1955 3.630639\n1956      1956 3.611354\n1957      1957 4.104160\n1958      1958 4.956939\n1959      1959 4.698216\n1960      1960 4.450920\n1961      1961 4.450920\n1962      1962 4.201724\n1963      1963 4.119269\n1964      1964 4.119269\n1965      1965 4.005172\n1966      1966 4.267919\n1967      1967 4.267919\n1968      1968 4.267919\n1969      1969 4.267919\n1970      1970 3.323870\n1971      1971 3.338312\n1972      1972 3.327527\n1973      1973 3.621855\n1974      1974 3.508994\n1975      1975 3.232424\n1976      1976 2.998022\n1977      1977 3.808059\n1978      1978 2.857658\n1979      1979 2.857658\n1980      1980 3.455995\n1981      1981 3.640025\n1982      1982 3.694681\n1983      1983 3.752256\n1984      1984 4.594859\n1985      1985 4.598184\n1986      1986 4.598184\n1987      1987 4.193044\n1988      1988 4.633564\n1989      1989 4.565194\n1990      1990 4.300075\n1991      1991 3.728338\n1992      1992 4.494440\n1993      1993 4.449497\n1994      1994 4.449497\n1995      1995 3.664691\n1996      1996 3.400924\n1997      1997 3.273890\n1998      1998 3.469806\n1999      1999 3.469806\n2000      2000 4.039767\n2001      2001 3.231901\n2002      2002 3.908329\n2003      2003 4.700430\n2004      2004 4.700430\n2005      2005 5.369350\n2006      2006 4.633199\n2007      2007 4.530147\n2008      2008 4.655619\n2009      2009 3.815651\n2010      2010 3.694710\n2011      2011 4.007528\n2012      2012 4.810786\n2013      2013 4.575124\n2014      2014 5.073405\n2015      2015 4.937798\n2016      2016 4.883655\n2017      2017 4.883655\n2018      2018 4.242209\n2019      2019 4.358315\n2020      2020 4.358315\n2021      2021 3.949797\n2022      2022 3.073943\n2023      2023 3.111780\n2024      2024 3.936609\n2025      2025 3.952968\n2026      2026 3.952968\n2027      2027 3.952968\n2028      2028 4.533502\n2029      2029 4.560112\n2030      2030 4.351982\n2031      2031 4.915140\n2032      2032 4.915140\n2033      2033 4.378660\n2034      2034 4.378660\n2035      2035 4.378660\n2036      2036 4.496927\n2037      2037 4.401944\n2038      2038 3.440204\n2039      2039 2.978973\n2040      2040 2.978973\n2041      2041 2.978973\n2042      2042 3.512510\n2043      2043 4.293857\n2044      2044 4.769911\n2045      2045 4.826683\n2046      2046 4.826683\n2047      2047 4.358217\n2048      2048 3.924808\n2049      2049 4.630215\n2050      2050 4.630215\n2051      2051 3.925909\n2052      2052 3.889399\n2053      2053 4.465160\n2054      2054 4.465160\n2055      2055 4.465160\n2056      2056 4.465160\n2057      2057 4.421269\n2058      2058 4.833969\n2059      2059 4.833969\n2060      2060 4.833969\n2061      2061 4.833969\n2062      2062 3.847065\n2063      2063 3.847065\n2064      2064 4.657670\n2065      2065 3.815979\n2066      2066 3.815979\n2067      2067 2.899833\n2068      2068 2.899833\n2069      2069 3.707693\n2070      2070 4.218137\n2071      2071 3.357479\n2072      2072 3.988402\n2073      2073 3.988402\n2074      2074 3.988402\n2075      2075 3.988402\n2076      2076 4.295069\n2077      2077 3.448358\n2078      2078 3.436219\n2079      2079 3.665282\n2080      2080 4.525121\n2081      2081 3.954665\n2082      2082 3.954665\n2083      2083 3.655241\n2084      2084 4.322050\n2085      2085 4.322050\n2086      2086 4.335633\n2087      2087 3.358248\n2088      2088 3.637473\n2089      2089 3.637473\n2090      2090 4.534304\n2091      2091 4.534304\n2092      2092 4.534304\n2093      2093 3.824654\n2094      2094 3.946421\n2095      2095 3.946421\n2096      2096 4.156458\n2097      2097 4.156458\n2098      2098 4.156458\n2099      2099 4.156458\n2100      2100 4.691834\n2101      2101 4.414255\n2102      2102 3.875976\n2103      2103 3.875976\n2104      2104 4.654825\n2105      2105 4.701734\n2106      2106 4.486304\n2107      2107 3.867437\n2108      2108 3.867437\n2109      2109 3.810126\n2110      2110 3.461875\n2111      2111 4.038310\n2112      2112 4.368408\n2113      2113 3.480789\n2114      2114 3.069464\n2115      2115 3.270769\n2116      2116 3.843495\n2117      2117 2.945096\n2118      2118 2.945096\n2119      2119 3.096346\n2120      2120 3.096346\n2121      2121 3.096346\n2122      2122 2.927157\n2123      2123 2.927157\n2124      2124 3.757944\n2125      2125 3.757944\n2126      2126 3.757944\n2127      2127 3.500429\n2128      2128 4.185864\n2129      2129 4.110340\n2130      2130 3.744070\n2131      2131 3.029550\n2132      2132 3.633144\n2133      2133 3.633144\n2134      2134 3.933346\n2135      2135 3.933346\n2136      2136 3.284034\n2137      2137 3.315660\n2138      2138 3.315660\n2139      2139 3.796212\n2140      2140 3.796212\n2141      2141 3.796212\n2142      2142 3.774117\n2143      2143 3.648129\n2144      2144 3.730881\n2145      2145 3.479671\n2146      2146 4.034039\n2147      2147 3.832777\n2148      2148 3.832777\n2149      2149 3.539483\n2150      2150 4.071537\n2151      2151 3.583573\n2152      2152 4.285893\n2153      2153 4.564926\n2154      2154 4.564926\n2155      2155 3.797626\n2156      2156 4.633817\n2157      2157 4.145397\n2158      2158 4.145397\n2159      2159 3.824522\n2160      2160 4.641402\n2161      2161 4.708643\n2162      2162 4.708643\n2163      2163 4.499733\n2164      2164 4.188024\n2165      2165 4.354006\n2166      2166 4.507703\n2167      2167 4.507703\n2168      2168 4.507703\n2169      2169 4.507703\n2170      2170 4.722514\n2171      2171 4.566053\n2172      2172 4.419417\n2173      2173 3.537332\n2174      2174 3.960823\n2175      2175 3.960823\n2176      2176 4.368516\n2177      2177 3.603464\n2178      2178 3.603464\n2179      2179 3.603464\n2180      2180 4.588007\n2181      2181 4.677794\n2182      2182 3.863102\n2183      2183 3.683104\n2184      2184 3.134687\n2185      2185 2.512093\n2186      2186 3.289616\n2187      2187 3.012360\n2188      2188 3.012360\n2189      2189 3.012360\n2190      2190 3.012360\n2191      2191 2.706767\n2192      2192 3.445596\n2193      2193 2.532852\n2194      2194 2.532852\n2195      2195 2.532852\n2196      2196 2.783690\n2197      2197 3.388756\n2198      2198 4.176971\n2199      2199 4.597452\n2200      2200 4.597452\n2201      2201 4.086995\n2202      2202 4.598551\n2203      2203 4.446457\n2204      2204 4.446457\n2205      2205 4.006549\n2206      2206 4.471402\n2207      2207 4.024182\n2208      2208 4.024182\n2209      2209 4.024182\n2210      2210 4.024182\n2211      2211 4.024182\n2212      2212 3.843677\n2213      2213 3.327519\n2214      2214 3.325632\n2215      2215 3.812902\n2216      2216 3.685772\n2217      2217 3.705646\n2218      2218 4.332567\n2219      2219 4.332567\n2220      2220 3.664737\n2221      2221 3.664737\n2222      2222 4.589396\n2223      2223 3.650594\n2224      2224 3.650594\n2225      2225 3.152759\n2226      2226 3.714857\n2227      2227 3.714857\n2228      2228 3.600461\n2229      2229 3.445216\n2230      2230 4.374993\n2231      2231 5.000085\n2232      2232 4.313016\n2233      2233 3.579641\n2234      2234 4.259261\n2235      2235 3.714594\n2236      2236 4.326947\n2237      2237 4.326947\n2238      2238 4.326947\n2239      2239 4.016437\n2240      2240 4.016437\n2241      2241 4.831691\n2242      2242 4.238933\n2243      2243 4.039132\n2244      2244 3.717403\n2245      2245 3.928360\n2246      2246 3.928360\n2247      2247 4.135933\n2248      2248 4.202926\n2249      2249 3.606355\n2250      2250 4.510002\n2251      2251 4.360211\n2252      2252 4.968590\n2253      2253 5.644301\n2254      2254 5.644301\n2255      2255 5.644301\n2256      2256 5.644301\n2257      2257 5.644301\n2258      2258 5.644301\n2259      2259 5.658561\n2260      2260 5.225880\n2261      2261 4.403182\n2262      2262 3.611725\n2263      2263 2.812763\n2264      2264 2.812763\n2265      2265 2.812763\n2266      2266 3.603569\n2267      2267 3.542054\n2268      2268 4.156596\n2269      2269 4.795260\n2270      2270 4.795260\n2271      2271 4.122190\n2272      2272 4.122190\n2273      2273 4.122190\n2274      2274 4.122190\n2275      2275 4.339064\n2276      2276 3.991756\n2277      2277 4.749769\n2278      2278 4.710585\n2279      2279 4.291506\n2280      2280 3.916362\n2281      2281 3.916362\n2282      2282 3.733122\n2283      2283 3.733122\n2284      2284 3.536476\n2285      2285 4.187158\n2286      2286 4.187158\n2287      2287 4.187158\n2288      2288 4.187158\n2289      2289 4.187158\n2290      2290 3.389163\n2291      2291 3.389163\n2292      2292 3.610378\n2293      2293 4.444331\n2294      2294 4.444331\n2295      2295 3.964370\n2296      2296 3.964370\n2297      2297 4.058548\n2298      2298 4.058548\n2299      2299 4.024031\n2300      2300 3.111646\n2301      2301 3.217465\n2302      2302 2.477831\n2303      2303 3.227904\n2304      2304 3.227904\n2305      2305 3.639036\n2306      2306 4.532969\n2307      2307 4.532969\n2308      2308 3.917476\n2309      2309 4.094351\n2310      2310 3.924371\n2311      2311 4.682422\n2312      2312 3.830830\n2313      2313 3.830830\n2314      2314 4.227792\n2315      2315 4.075475\n2316      2316 3.271135\n2317      2317 3.193966\n2318      2318 3.193966\n2319      2319 4.125173\n2320      2320 4.125173\n2321      2321 3.820688\n2322      2322 3.965020\n2323      2323 3.965020\n2324      2324 3.965020\n2325      2325 4.131073\n2326      2326 4.131073\n2327      2327 4.131073\n2328      2328 4.729521\n2329      2329 4.729521\n2330      2330 4.022684\n2331      2331 3.581263\n2332      2332 4.159257\n2333      2333 3.665112\n2334      2334 4.539248\n2335      2335 4.539248\n2336      2336 4.539248\n2337      2337 3.955483\n2338      2338 3.955483\n2339      2339 3.955483\n2340      2340 4.320130\n2341      2341 4.967670\n2342      2342 4.688293\n2343      2343 4.070808\n2344      2344 3.862544\n2345      2345 3.862544\n2346      2346 3.862544\n2347      2347 3.811859\n2348      2348 3.180581\n2349      2349 3.180581\n2350      2350 3.180581\n2351      2351 4.143828\n2352      2352 3.615065\n2353      2353 3.356721\n2354      2354 4.304929\n2355      2355 5.116207\n2356      2356 4.938697\n2357      2357 4.425158\n2358      2358 4.425158\n2359      2359 4.010062\n2360      2360 3.276177\n2361      2361 3.276177\n2362      2362 3.276177\n2363      2363 3.276177\n2364      2364 2.633689\n2365      2365 3.373501\n2366      2366 3.373501\n2367      2367 3.373501\n2368      2368 3.663452\n2369      2369 2.819268\n2370      2370 2.819268\n2371      2371 3.170951\n2372      2372 3.758159\n2373      2373 4.161832\n2374      2374 4.701236\n2375      2375 4.701236\n2376      2376 4.701236\n2377      2377 4.778741\n2378      2378 4.778741\n2379      2379 5.370650\n2380      2380 5.370650\n2381      2381 4.867282\n2382      2382 4.215417\n2383      2383 4.453908\n2384      2384 4.453908\n2385      2385 4.453908\n2386      2386 3.714856\n2387      2387 4.455417\n2388      2388 4.533110\n2389      2389 4.533110\n2390      2390 4.325755\n2391      2391 4.325755\n2392      2392 4.325755\n2393      2393 4.325755\n2394      2394 4.325755\n2395      2395 4.325755\n2396      2396 4.325755\n2397      2397 3.952925\n2398      2398 4.659914\n2399      2399 3.762724\n2400      2400 3.344681\n2401      2401 4.246427\n2402      2402 4.246427\n2403      2403 4.549646\n2404      2404 4.260899\n2405      2405 4.399963\n2406      2406 4.399963\n2407      2407 4.842930\n2408      2408 4.502989\n2409      2409 4.769976\n2410      2410 4.773826\n2411      2411 4.651835\n2412      2412 4.942983\n2413      2413 4.942983\n2414      2414 5.311863\n2415      2415 4.497725\n2416      2416 4.560493\n2417      2417 4.560493\n2418      2418 4.158380\n2419      2419 3.271061\n2420      2420 3.985668\n2421      2421 3.985668\n2422      2422 3.985668\n2423      2423 4.133937\n2424      2424 4.266213\n2425      2425 4.294632\n2426      2426 3.315990\n2427      2427 3.802853\n2428      2428 4.598615\n2429      2429 4.598615\n2430      2430 4.598615\n2431      2431 3.976663\n2432      2432 3.226021\n2433      2433 4.098669\n2434      2434 4.191902\n2435      2435 4.200169\n2436      2436 4.136214\n2437      2437 4.317207\n2438      2438 4.317207\n2439      2439 3.997905\n2440      2440 4.560983\n2441      2441 3.841201\n2442      2442 4.036393\n2443      2443 3.474614\n2444      2444 3.904372\n2445      2445 3.904372\n2446      2446 4.363490\n2447      2447 3.525741\n2448      2448 3.094702\n2449      2449 3.730596\n2450      2450 3.399693\n2451      2451 3.399693\n2452      2452 3.399693\n2453      2453 4.389556\n2454      2454 4.126166\n2455      2455 3.593594\n2456      2456 3.593594\n2457      2457 3.846218\n2458      2458 4.792151\n2459      2459 4.532397\n2460      2460 3.665343\n2461      2461 3.665343\n2462      2462 3.960484\n2463      2463 3.960484\n2464      2464 4.074500\n2465      2465 4.074500\n2466      2466 4.654154\n2467      2467 4.654154\n2468      2468 3.975043\n2469      2469 3.975043\n2470      2470 4.613798\n2471      2471 4.877919\n2472      2472 5.132626\n2473      2473 4.291282\n2474      2474 4.291282\n2475      2475 3.977609\n2476      2476 3.977609\n2477      2477 3.489926\n2478      2478 3.869892\n2479      2479 3.869892\n2480      2480 4.757053\n2481      2481 4.860919\n2482      2482 4.491495\n2483      2483 4.049894\n2484      2484 4.138062\n2485      2485 3.776395\n2486      2486 4.715900\n2487      2487 4.715900\n2488      2488 4.342433\n2489      2489 4.465369\n2490      2490 3.559941\n2491      2491 3.559941\n2492      2492 3.559941\n2493      2493 3.559941\n2494      2494 4.494584\n2495      2495 4.767591\n2496      2496 3.835001\n2497      2497 3.933362\n2498      2498 4.331088\n2499      2499 4.331088\n2500      2500 4.413101\n2501      2501 4.752045\n2502      2502 4.752045\n2503      2503 4.752045\n2504      2504 4.752045\n2505      2505 5.184188\n2506      2506 5.184188\n2507      2507 5.097957\n2508      2508 5.121673\n2509      2509 4.613728\n2510      2510 4.414923\n2511      2511 3.560464\n2512      2512 3.560464\n2513      2513 3.560464\n2514      2514 3.560464\n2515      2515 3.560464\n2516      2516 4.176206\n2517      2517 4.591729\n2518      2518 4.542847\n2519      2519 3.726864\n2520      2520 4.236010\n2521      2521 4.236010\n2522      2522 4.345719\n2523      2523 4.148179\n2524      2524 4.148179\n2525      2525 4.437736\n2526      2526 5.121507\n2527      2527 4.906050\n2528      2528 4.653775\n2529      2529 4.653775\n2530      2530 4.653775\n2531      2531 5.035251\n2532      2532 5.035251\n2533      2533 5.147910\n2534      2534 5.147910\n2535      2535 5.147910\n2536      2536 5.147910\n2537      2537 5.147910\n2538      2538 5.182715\n2539      2539 4.284185\n2540      2540 4.284185\n2541      2541 3.548544\n2542      2542 4.172594\n2543      2543 3.659866\n2544      2544 3.792108\n2545      2545 3.265250\n2546      2546 3.740109\n2547      2547 3.740109\n2548      2548 3.491865\n2549      2549 3.493217\n2550      2550 2.984206\n2551      2551 3.383862\n2552      2552 3.120814\n2553      2553 3.120814\n2554      2554 3.120814\n2555      2555 2.958459\n2556      2556 3.922007\n2557      2557 3.034576\n2558      2558 3.219700\n2559      2559 2.577978\n2560      2560 2.577978\n2561      2561 2.577978\n2562      2562 2.577978\n2563      2563 2.577978\n2564      2564 2.577978\n2565      2565 2.968323\n2566      2566 2.968323\n2567      2567 3.207246\n2568      2568 3.207246\n2569      2569 2.945811\n2570      2570 2.945811\n2571      2571 2.988855\n2572      2572 3.388629\n2573      2573 3.470297\n2574      2574 3.309405\n2575      2575 3.309405\n2576      2576 3.309405\n2577      2577 3.891791\n2578      2578 3.615097\n2579      2579 3.639147\n2580      2580 3.791725\n2581      2581 3.791725\n2582      2582 3.791725\n2583      2583 4.075953\n2584      2584 3.938511\n2585      2585 3.938511\n2586      2586 4.000770\n2587      2587 4.185057\n2588      2588 4.185057\n2589      2589 4.185057\n2590      2590 3.527988\n2591      2591 3.527988\n2592      2592 3.954691\n2593      2593 4.109168\n2594      2594 3.531542\n2595      2595 3.275584\n2596      2596 3.694593\n2597      2597 4.105629\n2598      2598 4.105629\n2599      2599 4.105629\n2600      2600 4.580704\n2601      2601 4.023996\n2602      2602 4.276290\n2603      2603 3.668969\n2604      2604 2.997287\n2605      2605 2.997287\n2606      2606 2.997287\n2607      2607 3.317286\n2608      2608 3.603545\n2609      2609 3.603545\n2610      2610 3.603545\n2611      2611 3.551206\n2612      2612 2.659478\n2613      2613 2.520593\n2614      2614 2.520593\n2615      2615 2.545637\n2616      2616 3.363017\n2617      2617 3.554736\n2618      2618 3.935788\n2619      2619 4.075806\n2620      2620 3.217072\n2621      2621 3.037010\n2622      2622 3.037010\n2623      2623 3.037010\n2624      2624 3.037010\n2625      2625 3.685777\n2626      2626 3.209691\n2627      2627 2.783837\n2628      2628 3.086607\n2629      2629 3.086607\n2630      2630 3.086607\n2631      2631 2.795618\n2632      2632 2.795618\n2633      2633 3.116863\n2634      2634 3.420560\n2635      2635 3.061248\n2636      2636 3.061248\n2637      2637 3.225517\n2638      2638 3.225517\n2639      2639 3.843803\n2640      2640 4.786621\n2641      2641 5.313812\n2642      2642 4.667271\n2643      2643 4.724872\n2644      2644 5.031114\n2645      2645 4.046719\n2646      2646 3.956584\n2647      2647 4.852866\n2648      2648 4.169355\n2649      2649 4.169355\n2650      2650 4.169355\n2651      2651 4.169355\n2652      2652 3.451034\n2653      2653 3.406918\n2654      2654 3.715822\n2655      2655 3.509565\n2656      2656 3.887319\n2657      2657 3.887319\n2658      2658 4.157842\n2659      2659 4.157842\n2660      2660 3.904178\n2661      2661 4.269518\n2662      2662 4.001667\n2663      2663 3.741107\n2664      2664 3.574941\n2665      2665 3.574941\n2666      2666 3.574941\n2667      2667 3.822062\n2668      2668 4.095120\n2669      2669 4.095120\n2670      2670 4.224115\n2671      2671 4.208177\n2672      2672 3.287335\n2673      2673 3.848692\n2674      2674 3.761617\n2675      2675 3.495359\n2676      2676 3.338946\n2677      2677 3.607135\n2678      2678 4.466415\n2679      2679 4.825899\n2680      2680 4.825899\n2681      2681 4.825899\n2682      2682 4.739022\n2683      2683 4.739022\n2684      2684 4.760797\n2685      2685 4.595512\n2686      2686 4.630101\n2687      2687 3.695539\n2688      2688 4.568242\n2689      2689 4.726714\n2690      2690 4.705127\n2691      2691 4.705127\n2692      2692 3.870293\n2693      2693 3.788618\n2694      2694 3.788618\n2695      2695 3.788618\n2696      2696 3.788618\n2697      2697 3.612996\n2698      2698 3.562355\n2699      2699 3.562355\n2700      2700 4.099611\n2701      2701 3.892663\n2702      2702 3.853966\n2703      2703 3.853966\n2704      2704 3.578220\n2705      2705 3.349905\n2706      2706 4.023433\n2707      2707 3.558984\n2708      2708 3.287672\n2709      2709 3.305763\n2710      2710 3.772827\n2711      2711 3.584787\n2712      2712 4.191950\n2713      2713 3.725323\n2714      2714 3.791037\n2715      2715 3.791037\n2716      2716 4.060114\n2717      2717 4.669177\n2718      2718 4.643598\n2719      2719 5.470710\n2720      2720 4.642366\n2721      2721 4.371968\n2722      2722 3.641871\n2723      2723 4.235068\n2724      2724 4.235068\n2725      2725 3.247439\n2726      2726 3.247439\n2727      2727 3.368980\n2728      2728 4.313412\n2729      2729 4.313412\n2730      2730 4.313412\n2731      2731 3.417566\n2732      2732 3.417566\n2733      2733 3.417566\n2734      2734 3.842114\n2735      2735 3.842114\n2736      2736 4.217523\n2737      2737 4.831303\n2738      2738 4.831303\n2739      2739 4.831303\n2740      2740 4.831303\n2741      2741 4.831303\n2742      2742 4.686162\n2743      2743 4.160347\n2744      2744 3.838167\n2745      2745 3.838167\n2746      2746 3.838167\n2747      2747 4.257752\n2748      2748 4.257752\n2749      2749 3.728306\n2750      2750 4.200260\n2751      2751 3.969145\n2752      2752 4.714771\n2753      2753 4.453685\n2754      2754 4.554513\n2755      2755 3.691279\n2756      2756 3.465253\n2757      2757 3.599778\n2758      2758 3.495374\n2759      2759 3.495374\n2760      2760 4.049812\n2761      2761 4.481905\n2762      2762 4.481905\n2763      2763 4.324870\n2764      2764 3.593193\n2765      2765 3.593193\n2766      2766 3.438000\n2767      2767 3.438000\n2768      2768 3.438000\n2769      2769 3.862744\n2770      2770 4.773612\n2771      2771 4.389530\n2772      2772 4.723005\n2773      2773 4.723005\n2774      2774 4.723005\n2775      2775 3.772802\n2776      2776 3.772802\n2777      2777 4.661345\n2778      2778 4.661345\n2779      2779 4.661345\n2780      2780 3.763646\n2781      2781 2.951649\n2782      2782 3.435586\n2783      2783 3.616027\n2784      2784 4.365157\n2785      2785 3.702750\n2786      2786 4.182293\n2787      2787 4.182293\n2788      2788 4.465491\n2789      2789 4.344238\n2790      2790 4.238114\n2791      2791 4.444982\n2792      2792 3.657798\n2793      2793 3.657798\n2794      2794 3.657798\n2795      2795 3.657798\n2796      2796 3.980332\n2797      2797 3.706275\n2798      2798 3.970335\n2799      2799 3.970335\n2800      2800 3.940009\n2801      2801 3.940009\n2802      2802 3.886522\n2803      2803 3.886522\n2804      2804 3.826224\n2805      2805 3.826224\n2806      2806 3.454154\n2807      2807 4.061879\n2808      2808 4.061879\n2809      2809 4.810768\n2810      2810 4.006683\n2811      2811 3.977908\n2812      2812 3.524700\n2813      2813 3.524700\n2814      2814 4.024116\n2815      2815 4.024116\n2816      2816 4.451145\n2817      2817 4.451145\n2818      2818 3.525636\n2819      2819 3.643369\n2820      2820 4.608847\n2821      2821 3.755470\n2822      2822 3.290582\n2823      2823 3.217068\n2824      2824 3.217068\n2825      2825 3.572897\n2826      2826 3.572897\n2827      2827 3.572897\n2828      2828 3.265090\n2829      2829 3.480221\n2830      2830 3.480221\n2831      2831 3.429766\n2832      2832 3.429766\n2833      2833 3.889839\n2834      2834 3.889839\n2835      2835 4.708364\n2836      2836 3.990565\n2837      2837 3.729592\n2838      2838 3.995430\n2839      2839 4.298894\n2840      2840 4.151134\n2841      2841 4.151134\n2842      2842 4.039781\n2843      2843 3.910032\n2844      2844 4.041714\n2845      2845 4.253044\n2846      2846 3.880165\n2847      2847 3.488182\n2848      2848 4.158253\n2849      2849 3.467327\n2850      2850 2.572482\n2851      2851 2.572482\n2852      2852 2.572482\n2853      2853 2.572482\n2854      2854 3.361763\n2855      2855 3.483081\n2856      2856 4.220151\n2857      2857 3.409059\n2858      2858 4.071189\n2859      2859 3.989560\n2860      2860 4.312271\n2861      2861 4.276433\n2862      2862 4.276433\n2863      2863 4.276433\n2864      2864 4.276433\n2865      2865 3.402676\n2866      2866 3.747350\n2867      2867 3.320016\n2868      2868 3.394320\n2869      2869 4.356037\n2870      2870 4.356037\n2871      2871 4.504327\n2872      2872 3.673285\n2873      2873 3.971524\n2874      2874 4.329596\n2875      2875 5.136040\n2876      2876 4.410184\n2877      2877 3.930338\n2878      2878 3.930338\n2879      2879 3.747518\n2880      2880 3.951197\n2881      2881 4.823225\n2882      2882 4.399515\n2883      2883 4.190312\n2884      2884 4.190312\n2885      2885 4.122248\n2886      2886 3.792675\n2887      2887 3.792675\n2888      2888 3.792675\n2889      2889 3.943775\n2890      2890 3.943775\n2891      2891 3.943775\n2892      2892 3.322170\n2893      2893 3.322170\n2894      2894 3.322170\n2895      2895 3.322170\n2896      2896 3.312541\n2897      2897 3.897381\n2898      2898 4.172158\n2899      2899 3.792575\n2900      2900 3.792575\n2901      2901 4.205960\n2902      2902 4.205960\n2903      2903 3.357517\n2904      2904 3.568602\n2905      2905 3.078215\n2906      2906 3.934079\n2907      2907 3.934079\n2908      2908 3.934079\n2909      2909 4.202099\n2910      2910 3.920533\n2911      2911 3.920533\n2912      2912 3.920533\n2913      2913 3.560663\n2914      2914 3.977531\n2915      2915 4.343447\n2916      2916 4.130229\n2917      2917 4.021337\n2918      2918 4.021337\n2919      2919 4.021337\n2920      2920 4.077172\n2921      2921 4.077172\n2922      2922 4.077172\n2923      2923 3.948413\n2924      2924 4.169166\n2925      2925 3.711539\n2926      2926 3.774279\n2927      2927 3.774279\n2928      2928 3.204777\n2929      2929 3.859419\n2930      2930 4.585339\n2931      2931 3.885800\n2932      2932 3.885800\n2933      2933 3.885800\n2934      2934 3.885800\n2935      2935 4.243659\n2936      2936 4.243659\n2937      2937 4.568840\n2938      2938 5.001238\n2939      2939 4.454899\n2940      2940 4.454899\n2941      2941 4.011287\n2942      2942 3.471107\n2943      2943 3.471107\n2944      2944 3.314645\n2945      2945 3.475967\n2946      2946 4.160781\n2947      2947 3.355858\n2948      2948 4.260075\n2949      2949 4.755433\n2950      2950 4.755433\n2951      2951 5.636620\n2952      2952 5.636620\n2953      2953 5.636620\n2954      2954 5.933092\n2955      2955 6.226708\n2956      2956 6.226708\n2957      2957 6.226708\n2958      2958 5.834230\n2959      2959 5.292344\n2960      2960 5.239481\n2961      2961 4.479226\n2962      2962 4.864671\n2963      2963 5.236781\n2964      2964 5.236781\n2965      2965 4.525143\n2966      2966 4.221388\n2967      2967 4.552658\n2968      2968 3.589783\n2969      2969 3.589783\n2970      2970 3.425959\n2971      2971 3.703963\n2972      2972 2.807041\n2973      2973 3.701859\n2974      2974 4.132775\n2975      2975 4.547621\n2976      2976 4.547621\n2977      2977 4.782518\n2978      2978 4.035400\n2979      2979 3.976290\n2980      2980 3.290970\n2981      2981 3.290970\n2982      2982 3.290970\n2983      2983 3.350704\n2984      2984 4.173002\n2985      2985 3.890229\n2986      2986 4.381871\n2987      2987 4.248648\n2988      2988 4.379967\n2989      2989 4.326092\n2990      2990 4.326092\n2991      2991 4.326092\n2992      2992 4.616425\n2993      2993 4.381357\n2994      2994 4.925536\n2995      2995 4.925536\n2996      2996 4.622550\n2997      2997 4.622550\n2998      2998 3.890670\n2999      2999 3.890670\n3000      3000 4.366677\n3001      3001 3.965664\n3002      3002 3.363880\n3003      3003 2.735384\n3004      3004 2.735384\n3005      3005 2.735384\n3006      3006 2.735384\n3007      3007 2.735384\n3008      3008 3.182225\n3009      3009 3.182225\n3010      3010 3.423699\n3011      3011 3.427147\n3012      3012 3.621317\n3013      3013 2.991101\n3014      3014 2.991101\n3015      3015 3.515894\n3016      3016 4.322905\n3017      3017 4.322905\n3018      3018 4.545675\n3019      3019 3.969813\n3020      3020 4.104695\n3021      3021 3.627034\n3022      3022 3.924373\n3023      3023 4.313623\n3024      3024 4.665971\n3025      3025 5.208282\n3026      3026 5.208282\n3027      3027 5.208282\n3028      3028 5.208282\n3029      3029 5.208282\n3030      3030 4.291778\n3031      3031 4.291778\n3032      3032 4.291778\n3033      3033 3.806451\n3034      3034 3.806451\n3035      3035 3.806451\n3036      3036 4.322779\n3037      3037 4.038829\n3038      3038 4.019079\n3039      3039 4.019079\n3040      3040 4.035981\n3041      3041 4.035981\n3042      3042 4.723282\n3043      3043 4.723282\n3044      3044 4.638791\n3045      3045 4.638791\n3046      3046 4.706134\n3047      3047 4.352809\n3048      3048 4.352809\n3049      3049 4.352809\n3050      3050 4.980929\n3051      3051 4.980929\n3052      3052 5.221848\n3053      3053 4.581981\n3054      3054 4.581981\n3055      3055 4.581981\n3056      3056 3.685168\n3057      3057 4.449204\n3058      3058 4.417955\n3059      3059 4.103429\n3060      3060 3.730989\n3061      3061 2.806797\n3062      3062 2.645980\n3063      3063 2.694439\n3064      3064 2.953186\n3065      3065 2.953186\n3066      3066 3.764186\n3067      3067 4.402637\n3068      3068 4.438239\n3069      3069 3.591755\n3070      3070 3.541160\n3071      3071 4.413309\n3072      3072 4.653778\n3073      3073 4.935154\n3074      3074 4.577882\n3075      3075 3.824414\n3076      3076 3.824414\n3077      3077 3.824414\n3078      3078 3.569258\n3079      3079 3.569258\n3080      3080 3.569258\n3081      3081 3.664186\n3082      3082 3.981668\n3083      3083 3.637084\n3084      3084 3.637084\n3085      3085 3.637084\n3086      3086 4.544306\n3087      3087 3.861907\n3088      3088 3.525931\n3089      3089 4.055476\n3090      3090 3.186792\n3091      3091 3.883790\n3092      3092 3.440646\n3093      3093 3.797823\n3094      3094 3.789931\n3095      3095 4.695268\n3096      3096 4.695268\n3097      3097 4.695268\n3098      3098 4.695268\n3099      3099 4.421718\n3100      3100 4.421718\n3101      3101 4.527338\n3102      3102 4.263075\n3103      3103 4.263075\n3104      3104 4.263075\n3105      3105 3.920827\n3106      3106 3.920827\n3107      3107 3.920827\n3108      3108 3.920827\n3109      3109 3.920827\n3110      3110 4.228344\n3111      3111 5.054576\n3112      3112 5.054576\n3113      3113 4.638968\n3114      3114 4.638968\n3115      3115 4.638968\n3116      3116 4.638968\n3117      3117 4.638968\n3118      3118 4.638968\n3119      3119 4.740436\n3120      3120 5.472280\n3121      3121 4.758347\n3122      3122 4.792474\n3123      3123 3.897980\n3124      3124 3.483163\n3125      3125 4.047586\n3126      3126 4.047586\n3127      3127 4.047586\n3128      3128 3.380036\n3129      3129 3.380036\n3130      3130 3.396488\n3131      3131 2.476680\n3132      3132 2.476680\n3133      3133 2.476680\n3134      3134 3.161270\n3135      3135 3.733116\n3136      3136 4.047999\n3137      3137 3.950406\n3138      3138 3.950406\n3139      3139 3.263089\n3140      3140 4.194557\n3141      3141 3.250465\n3142      3142 3.843785\n3143      3143 4.042099\n3144      3144 4.042099\n3145      3145 4.042099\n3146      3146 3.234176\n3147      3147 3.234176\n3148      3148 4.028120\n3149      3149 4.378127\n3150      3150 3.784810\n3151      3151 3.784810\n3152      3152 3.784810\n3153      3153 3.784810\n3154      3154 3.571008\n3155      3155 3.694048\n3156      3156 3.632367\n3157      3157 3.632367\n3158      3158 3.417516\n3159      3159 3.786006\n3160      3160 4.428242\n3161      3161 4.866107\n3162      3162 4.866107\n3163      3163 4.866107\n3164      3164 4.866107\n3165      3165 3.946356\n3166      3166 3.946356\n3167      3167 3.959629\n3168      3168 3.944866\n3169      3169 3.944866\n3170      3170 3.944866\n3171      3171 3.944866\n3172      3172 3.944866\n3173      3173 3.944866\n3174      3174 4.491431\n3175      3175 4.926534\n3176      3176 4.703794\n3177      3177 4.703794\n3178      3178 4.703794\n3179      3179 4.566124\n3180      3180 4.684003\n3181      3181 3.879504\n3182      3182 4.461107\n3183      3183 4.382181\n3184      3184 4.365306\n3185      3185 3.919643\n3186      3186 3.666906\n3187      3187 3.846904\n3188      3188 4.092768\n3189      3189 4.008851\n3190      3190 3.599038\n3191      3191 3.599038\n3192      3192 4.557799\n3193      3193 4.213221\n3194      3194 4.234219\n3195      3195 4.234219\n3196      3196 3.758853\n3197      3197 3.952718\n3198      3198 4.292320\n3199      3199 4.292320\n3200      3200 4.292320\n3201      3201 4.359264\n3202      3202 3.646266\n3203      3203 3.646266\n3204      3204 3.413219\n3205      3205 4.045582\n3206      3206 4.493674\n3207      3207 4.493674\n3208      3208 4.140214\n3209      3209 4.308546\n3210      3210 3.632283\n3211      3211 3.585124\n3212      3212 4.483983\n3213      3213 3.529360\n3214      3214 3.529360\n3215      3215 3.529360\n3216      3216 4.465924\n3217      3217 4.157819\n3218      3218 4.157819\n3219      3219 3.855416\n3220      3220 3.855416\n3221      3221 3.855416\n3222      3222 3.855416\n3223      3223 3.622623\n3224      3224 4.366740\n3225      3225 4.324361\n3226      3226 4.468531\n3227      3227 4.075677\n3228      3228 3.680341\n3229      3229 3.420725\n3230      3230 3.652138\n3231      3231 3.927496\n3232      3232 3.554992\n3233      3233 4.509310\n3234      3234 4.335290\n3235      3235 3.775448\n3236      3236 4.151606\n3237      3237 4.213886\n3238      3238 3.696580\n3239      3239 3.696580\n3240      3240 3.272495\n3241      3241 4.036895\n3242      3242 4.007265\n3243      3243 4.007265\n3244      3244 4.241189\n3245      3245 4.770278\n3246      3246 4.770278\n3247      3247 3.961484\n3248      3248 4.181509\n3249      3249 4.181509\n3250      3250 4.360115\n3251      3251 4.360115\n3252      3252 4.033207\n3253      3253 3.217320\n3254      3254 3.813795\n3255      3255 4.663671\n3256      3256 5.136457\n3257      3257 5.142352\n3258      3258 5.143241\n3259      3259 5.143241\n3260      3260 5.143241\n3261      3261 4.720487\n3262      3262 5.052889\n3263      3263 4.649192\n3264      3264 4.414632\n3265      3265 4.288764\n3266      3266 4.827386\n3267      3267 4.827386\n3268      3268 4.054411\n3269      3269 3.637939\n3270      3270 3.637939\n3271      3271 3.632165\n3272      3272 3.632165\n3273      3273 3.533518\n3274      3274 4.023533\n3275      3275 4.023533\n3276      3276 4.033455\n3277      3277 4.448349\n3278      3278 3.628084\n3279      3279 4.385879\n3280      3280 5.077779\n3281      3281 5.077779\n3282      3282 5.077779\n3283      3283 4.486283\n3284      3284 4.050747\n3285      3285 3.595380\n3286      3286 4.057394\n3287      3287 4.057394\n3288      3288 4.531189\n3289      3289 3.685747\n3290      3290 3.477279\n3291      3291 3.657973\n3292      3292 3.657973\n3293      3293 4.280597\n3294      3294 4.280597\n3295      3295 4.399309\n3296      3296 4.399309\n3297      3297 4.176719\n3298      3298 4.176719\n3299      3299 4.831323\n3300      3300 4.350474\n3301      3301 4.099719\n3302      3302 4.099719\n3303      3303 3.758184\n3304      3304 4.577518\n3305      3305 4.577518\n3306      3306 4.414970\n3307      3307 3.963207\n3308      3308 3.439715\n3309      3309 3.689117\n3310      3310 3.689117\n3311      3311 4.527531\n3312      3312 5.317318\n3313      3313 5.317318\n3314      3314 5.317318\n3315      3315 5.317318\n3316      3316 5.317318\n3317      3317 5.317318\n3318      3318 5.071351\n3319      3319 4.578623\n3320      3320 4.846870\n3321      3321 4.011604\n3322      3322 4.011604\n3323      3323 4.011604\n3324      3324 4.230111\n3325      3325 4.836285\n3326      3326 4.614465\n3327      3327 4.728436\n3328      3328 5.277715\n3329      3329 4.885518\n3330      3330 4.885518\n3331      3331 4.885518\n3332      3332 4.885518\n3333      3333 4.885518\n3334      3334 4.885518\n3335      3335 4.885518\n3336      3336 4.885518\n3337      3337 4.371108\n3338      3338 4.328640\n3339      3339 3.827866\n3340      3340 3.874661\n3341      3341 4.074553\n3342      3342 4.302429\n3343      3343 3.806662\n3344      3344 3.624048\n3345      3345 4.419256\n3346      3346 4.686057\n3347      3347 3.756256\n3348      3348 3.756256\n3349      3349 4.204936\n3350      3350 3.735120\n3351      3351 3.735120\n3352      3352 4.504038\n3353      3353 3.614192\n3354      3354 3.779718\n3355      3355 4.106838\n3356      3356 4.325876\n3357      3357 4.325876\n3358      3358 3.997556\n3359      3359 4.533380\n3360      3360 4.611354\n3361      3361 5.203969\n3362      3362 5.203969\n3363      3363 5.203969\n3364      3364 5.203969\n3365      3365 5.106439\n3366      3366 4.839723\n3367      3367 4.618909\n3368      3368 4.618909\n3369      3369 4.952935\n3370      3370 4.753002\n3371      3371 4.014982\n3372      3372 4.014982\n3373      3373 4.652537\n3374      3374 4.652537\n3375      3375 4.652537\n3376      3376 4.652537\n3377      3377 4.652537\n3378      3378 3.935345\n3379      3379 4.213162\n3380      3380 3.381599\n3381      3381 4.282767\n3382      3382 4.609680\n3383      3383 4.609680\n3384      3384 3.772533\n3385      3385 3.976311\n3386      3386 3.976311\n3387      3387 3.976311\n3388      3388 4.321527\n3389      3389 4.007294\n3390      3390 4.007294\n3391      3391 4.007294\n3392      3392 4.527303\n3393      3393 5.244939\n3394      3394 5.414609\n3395      3395 4.815766\n3396      3396 4.815766\n3397      3397 4.815766\n3398      3398 4.815766\n3399      3399 4.190991\n3400      3400 3.492150\n3401      3401 3.492150\n3402      3402 3.492150\n3403      3403 3.520973\n3404      3404 4.171274\n3405      3405 4.171274\n3406      3406 4.171274\n3407      3407 3.946944\n3408      3408 3.946944\n3409      3409 3.946944\n3410      3410 4.162795\n3411      3411 4.162795\n3412      3412 4.413041\n3413      3413 4.071557\n3414      3414 4.786061\n3415      3415 4.786061\n3416      3416 4.786061\n3417      3417 4.786061\n3418      3418 4.786061\n3419      3419 4.786061\n3420      3420 4.786061\n3421      3421 4.006409\n3422      3422 4.541556\n3423      3423 4.398390\n3424      3424 4.398390\n3425      3425 3.884015\n3426      3426 3.715112\n3427      3427 4.309954\n3428      3428 5.184760\n3429      3429 5.335071\n3430      3430 5.335071\n3431      3431 5.256681\n3432      3432 5.256681\n3433      3433 4.906577\n3434      3434 4.906577\n3435      3435 3.980768\n3436      3436 3.265307\n3437      3437 4.073899\n3438      3438 4.073899\n3439      3439 3.860950\n3440      3440 3.429458\n3441      3441 3.429458\n3442      3442 3.719089\n3443      3443 4.128121\n3444      3444 4.128121\n3445      3445 4.844782\n3446      3446 3.848179\n3447      3447 3.848179\n3448      3448 4.220849\n3449      3449 4.227809\n3450      3450 3.919649\n3451      3451 4.264360\n3452      3452 4.067447\n3453      3453 4.902723\n3454      3454 4.902723\n3455      3455 3.990945\n3456      3456 3.190937\n3457      3457 3.305282\n3458      3458 3.305282\n3459      3459 3.305282\n3460      3460 4.229868\n3461      3461 4.229868\n3462      3462 3.980757\n3463      3463 3.980757\n3464      3464 4.211714\n3465      3465 4.041640\n3466      3466 4.041640\n3467      3467 3.809529\n3468      3468 4.425143\n3469      3469 4.425143\n3470      3470 4.536999\n3471      3471 4.536999\n3472      3472 4.536999\n3473      3473 4.668063\n3474      3474 4.668063\n3475      3475 4.668063\n3476      3476 5.162612\n3477      3477 5.891670\n3478      3478 5.685308\n3479      3479 4.752332\n3480      3480 4.221418\n3481      3481 4.122009\n3482      3482 4.122009\n3483      3483 3.791984\n3484      3484 3.414160\n3485      3485 3.414160\n3486      3486 3.746490\n3487      3487 3.746490\n3488      3488 4.408195\n3489      3489 4.408195\n3490      3490 4.408195\n3491      3491 4.878958\n3492      3492 4.789486\n3493      3493 4.789486\n3494      3494 4.789486\n3495      3495 4.789486\n3496      3496 4.205372\n3497      3497 4.155288\n3498      3498 4.742724\n3499      3499 4.742724\n3500      3500 3.960834\n3501      3501 3.960834\n3502      3502 3.978454\n3503      3503 4.759727\n3504      3504 4.381349\n3505      3505 4.299413\n3506      3506 4.106470\n3507      3507 4.106470\n3508      3508 3.942122\n3509      3509 3.942122\n3510      3510 3.942122\n3511      3511 3.828424\n3512      3512 4.344026\n3513      3513 3.613134\n3514      3514 3.613134\n3515      3515 4.046806\n3516      3516 4.046806\n3517      3517 4.026829\n3518      3518 3.937093\n3519      3519 3.937093\n3520      3520 3.937093\n3521      3521 4.038864\n3522      3522 3.689686\n3523      3523 3.224955\n3524      3524 3.224955\n3525      3525 3.954983\n3526      3526 3.954983\n3527      3527 3.475444\n3528      3528 4.433479\n3529      3529 4.768757\n3530      3530 3.885067\n3531      3531 3.885067\n3532      3532 4.602822\n3533      3533 3.887282\n3534      3534 3.887282\n3535      3535 3.887282\n3536      3536 4.025765\n3537      3537 4.236752\n3538      3538 3.634063\n3539      3539 4.477395\n3540      3540 4.797778\n3541      3541 4.534996\n3542      3542 4.062570\n3543      3543 3.285573\n3544      3544 3.285573\n3545      3545 4.201732\n3546      3546 3.740717\n3547      3547 4.122174\n3548      3548 4.516308\n3549      3549 4.516308\n3550      3550 4.503186\n3551      3551 4.503186\n3552      3552 3.855283\n3553      3553 3.855283\n3554      3554 3.532838\n3555      3555 4.172821\n3556      3556 4.639390\n3557      3557 3.764449\n3558      3558 3.764449\n3559      3559 4.339318\n3560      3560 4.933887\n3561      3561 5.067990\n3562      3562 5.067990\n3563      3563 5.067990\n3564      3564 4.827518\n3565      3565 4.904058\n3566      3566 4.550695\n3567      3567 4.636211\n3568      3568 4.634882\n3569      3569 4.634882\n3570      3570 4.634882\n3571      3571 4.969451\n3572      3572 4.739602\n3573      3573 4.684790\n3574      3574 4.684790\n3575      3575 4.684790\n3576      3576 5.260831\n3577      3577 5.260831\n3578      3578 5.078445\n3579      3579 4.276845\n3580      3580 3.300898\n3581      3581 3.641396\n3582      3582 3.641396\n3583      3583 4.449068\n3584      3584 4.914625\n3585      3585 4.914625\n3586      3586 4.914625\n3587      3587 4.914625\n3588      3588 4.215337\n3589      3589 4.215337\n3590      3590 3.660306\n3591      3591 4.093286\n3592      3592 4.093286\n3593      3593 4.012927\n3594      3594 4.012927\n3595      3595 4.483617\n3596      3596 3.532632\n3597      3597 3.080235\n3598      3598 3.080235\n3599      3599 3.080235\n3600      3600 3.531831\n3601      3601 3.531831\n3602      3602 3.352905\n3603      3603 3.604940\n3604      3604 4.474823\n3605      3605 3.745182\n3606      3606 4.691434\n3607      3607 4.691434\n3608      3608 4.691434\n3609      3609 4.691434\n3610      3610 4.269058\n3611      3611 4.520641\n3612      3612 4.517800\n3613      3613 3.906655\n3614      3614 3.839996\n3615      3615 3.767312\n3616      3616 3.871815\n3617      3617 4.473162\n3618      3618 4.473162\n3619      3619 4.155890\n3620      3620 3.649539\n3621      3621 3.194757\n3622      3622 3.194757\n3623      3623 3.194757\n3624      3624 3.847933\n3625      3625 3.848348\n3626      3626 3.306616\n3627      3627 3.306616\n3628      3628 3.170336\n3629      3629 3.170336\n3630      3630 3.170336\n3631      3631 3.816828\n3632      3632 4.262730\n3633      3633 4.270289\n3634      3634 4.177959\n3635      3635 4.352821\n3636      3636 4.352821\n3637      3637 3.684689\n3638      3638 3.684689\n3639      3639 3.684689\n3640      3640 3.811271\n3641      3641 3.624666\n3642      3642 3.624666\n3643      3643 4.507925\n3644      3644 4.467861\n3645      3645 4.721970\n3646      3646 4.242808\n3647      3647 4.242808\n3648      3648 3.999737\n3649      3649 3.999737\n3650      3650 3.838420\n3651      3651 4.470256\n3652      3652 4.470256\n3653      3653 4.470256\n3654      3654 4.059731\n3655      3655 4.059731\n3656      3656 3.621022\n3657      3657 3.317269\n3658      3658 3.444473\n3659      3659 3.444473\n3660      3660 4.258744\n3661      3661 5.222175\n3662      3662 4.999503\n3663      3663 5.397399\n3664      3664 4.926298\n3665      3665 4.653249\n3666      3666 5.005839\n3667      3667 5.005839\n3668      3668 5.005839\n3669      3669 5.099828\n3670      3670 5.099828\n3671      3671 5.099828\n3672      3672 4.693063\n3673      3673 4.572079\n3674      3674 4.146925\n3675      3675 4.649174\n3676      3676 3.854513\n3677      3677 3.854513\n3678      3678 4.070454\n3679      3679 4.070454\n3680      3680 4.255189\n3681      3681 5.204132\n3682      3682 5.204132\n3683      3683 4.654074\n3684      3684 4.654074\n3685      3685 4.746907\n3686      3686 4.329693\n3687      3687 4.329693\n3688      3688 4.329693\n3689      3689 4.744052\n3690      3690 4.744052\n3691      3691 4.729781\n3692      3692 4.386051\n3693      3693 4.731181\n3694      3694 3.818241\n3695      3695 3.818241\n3696      3696 3.439655\n3697      3697 3.032284\n3698      3698 3.868142\n3699      3699 2.999866\n3700      3700 3.625819\n3701      3701 4.129978\n3702      3702 4.129978\n3703      3703 3.680742\n3704      3704 3.916833\n3705      3705 3.916833\n3706      3706 3.445194\n3707      3707 3.445194\n3708      3708 3.759594\n3709      3709 3.206285\n3710      3710 3.895967\n3711      3711 3.895967\n3712      3712 3.781344\n3713      3713 3.781344\n3714      3714 3.783889\n3715      3715 4.504649\n3716      3716 4.504649\n3717      3717 4.907713\n3718      3718 4.907713\n3719      3719 5.273294\n3720      3720 5.546099\n3721      3721 5.777780\n3722      3722 5.016410\n3723      3723 4.612983\n3724      3724 4.972001\n3725      3725 4.522267\n3726      3726 4.487320\n3727      3727 4.079751\n3728      3728 3.399608\n3729      3729 3.686077\n3730      3730 3.840701\n3731      3731 3.840701\n3732      3732 2.856167\n3733      3733 2.856167\n3734      3734 3.444178\n3735      3735 2.914038\n3736      3736 3.495114\n3737      3737 3.495114\n3738      3738 4.318175\n3739      3739 3.556535\n3740      3740 3.841927\n3741      3741 3.417328\n3742      3742 4.042129\n3743      3743 3.711064\n3744      3744 3.456276\n3745      3745 3.456276\n3746      3746 3.456276\n3747      3747 3.052647\n3748      3748 3.993535\n3749      3749 3.369062\n3750      3750 3.369062\n3751      3751 3.410436\n3752      3752 3.191616\n3753      3753 2.776187\n3754      3754 2.776187\n3755      3755 3.394811\n3756      3756 3.938976\n3757      3757 4.081468\n3758      3758 4.520386\n3759      3759 3.839850\n3760      3760 4.022377\n3761      3761 4.290640\n3762      3762 4.705305\n3763      3763 3.944538\n3764      3764 4.371889\n3765      3765 4.447539\n3766      3766 4.779091\n3767      3767 3.897833\n3768      3768 3.897833\n3769      3769 3.897833\n3770      3770 3.891358\n3771      3771 3.342896\n3772      3772 4.207592\n3773      3773 4.207592\n3774      3774 4.455768\n3775      3775 4.910551\n3776      3776 4.910551\n3777      3777 4.910551\n3778      3778 4.322658\n3779      3779 4.651100\n3780      3780 4.488418\n3781      3781 3.683289\n3782      3782 3.815905\n3783      3783 3.548198\n3784      3784 3.171398\n3785      3785 3.171398\n3786      3786 3.171232\n3787      3787 3.638478\n3788      3788 3.638478\n3789      3789 4.133647\n3790      3790 4.248220\n3791      3791 4.763007\n3792      3792 4.687307\n3793      3793 4.262190\n3794      3794 4.173136\n3795      3795 3.773234\n3796      3796 3.900109\n3797      3797 2.938508\n3798      3798 3.588745\n3799      3799 3.279123\n3800      3800 3.279123\n3801      3801 3.683203\n3802      3802 4.359788\n3803      3803 3.368039\n3804      3804 3.368039\n3805      3805 3.520042\n3806      3806 3.595221\n3807      3807 3.595221\n3808      3808 3.883424\n3809      3809 3.883424\n3810      3810 3.883424\n3811      3811 3.883424\n3812      3812 3.235093\n3813      3813 4.131905\n3814      3814 4.131905\n3815      3815 4.949195\n3816      3816 4.098723\n3817      3817 3.613837\n3818      3818 4.231978\n3819      3819 4.351492\n3820      3820 4.351492\n3821      3821 4.351492\n3822      3822 4.351492\n3823      3823 4.351492\n3824      3824 3.926001\n3825      3825 4.885510\n3826      3826 4.828797\n3827      3827 4.313873\n3828      3828 3.553853\n3829      3829 3.553853\n3830      3830 3.018146\n3831      3831 3.018146\n3832      3832 3.018146\n3833      3833 2.568847\n3834      3834 2.520825\n3835      3835 3.250112\n3836      3836 3.250112\n3837      3837 2.845302\n3838      3838 3.230616\n3839      3839 4.113880\n3840      3840 4.483933\n3841      3841 4.417285\n3842      3842 4.056458\n3843      3843 4.056458\n3844      3844 4.006030\n3845      3845 4.006030\n3846      3846 3.731884\n3847      3847 3.758886\n3848      3848 3.812394\n3849      3849 3.812394\n3850      3850 3.836265\n3851      3851 3.836265\n3852      3852 3.836265\n3853      3853 4.695948\n3854      3854 4.547170\n3855      3855 5.097709\n3856      3856 4.800281\n3857      3857 4.396526\n3858      3858 4.934392\n3859      3859 4.472344\n3860      3860 4.652910\n3861      3861 4.652910\n3862      3862 4.199560\n3863      3863 4.008459\n3864      3864 4.048314\n3865      3865 3.427583\n3866      3866 4.260273\n3867      3867 4.187571\n3868      3868 4.187571\n3869      3869 3.771220\n3870      3870 4.073017\n3871      3871 4.657976\n3872      3872 4.657976\n3873      3873 4.657976\n3874      3874 3.946093\n3875      3875 3.946093\n3876      3876 3.538536\n3877      3877 3.268712\n3878      3878 3.268712\n3879      3879 3.360387\n3880      3880 3.738909\n3881      3881 3.122835\n3882      3882 4.062872\n3883      3883 4.843719\n3884      3884 4.665646\n3885      3885 4.170103\n3886      3886 4.170103\n3887      3887 4.367459\n3888      3888 4.367459\n3889      3889 4.367459\n3890      3890 4.367459\n3891      3891 4.367459\n3892      3892 4.487053\n3893      3893 4.002148\n3894      3894 3.813542\n3895      3895 3.813542\n3896      3896 4.612856\n3897      3897 4.657931\n3898      3898 4.382399\n3899      3899 4.210006\n3900      3900 4.210006\n3901      3901 3.350143\n3902      3902 3.536664\n3903      3903 4.510388\n3904      3904 4.510388\n3905      3905 3.995494\n3906      3906 3.929810\n3907      3907 3.363788\n3908      3908 3.363788\n3909      3909 3.363788\n3910      3910 3.446302\n3911      3911 3.446302\n3912      3912 4.137925\n3913      3913 4.137925\n3914      3914 3.878966\n3915      3915 3.557326\n3916      3916 3.557326\n3917      3917 3.946929\n3918      3918 3.946929\n3919      3919 3.946929\n3920      3920 3.946929\n3921      3921 3.777265\n3922      3922 3.777265\n3923      3923 3.777265\n3924      3924 3.777265\n3925      3925 3.777265\n3926      3926 3.534602\n3927      3927 4.234678\n3928      3928 4.329557\n3929      3929 4.189141\n3930      3930 4.189141\n3931      3931 4.189141\n3932      3932 3.610306\n3933      3933 3.603550\n3934      3934 3.214442\n3935      3935 3.214442\n3936      3936 3.214442\n3937      3937 3.214442\n3938      3938 4.188447\n3939      3939 3.215355\n3940      3940 3.985143\n3941      3941 3.563212\n3942      3942 4.102180\n3943      3943 4.102180\n3944      3944 4.102180\n3945      3945 4.000466\n3946      3946 3.614680\n3947      3947 3.521044\n3948      3948 3.177885\n3949      3949 3.177885\n3950      3950 3.177885\n3951      3951 3.177885\n3952      3952 3.047918\n3953      3953 3.001981\n3954      3954 3.816682\n3955      3955 3.215930\n3956      3956 3.976838\n3957      3957 4.362164\n3958      3958 4.362164\n3959      3959 3.563409\n3960      3960 4.447572\n3961      3961 4.447572\n3962      3962 4.141811\n3963      3963 3.654285\n3964      3964 3.495567\n3965      3965 2.652937\n3966      3966 2.652937\n3967      3967 3.595724\n3968      3968 3.595724\n3969      3969 3.654145\n3970      3970 4.388798\n3971      3971 3.502208\n3972      3972 4.474991\n3973      3973 3.908959\n3974      3974 4.260299\n3975      3975 3.449380\n3976      3976 3.449380\n3977      3977 4.185935\n3978      3978 4.120459\n3979      3979 3.498074\n3980      3980 3.421378\n3981      3981 3.421378\n3982      3982 3.570639\n3983      3983 3.570639\n3984      3984 3.023914\n3985      3985 3.569713\n3986      3986 3.569713\n3987      3987 3.569713\n3988      3988 3.569713\n3989      3989 3.569713\n3990      3990 3.960952\n3991      3991 3.960952\n3992      3992 3.960952\n3993      3993 4.445538\n3994      3994 3.598768\n3995      3995 3.598768\n3996      3996 3.169909\n3997      3997 3.514965\n3998      3998 3.947027\n3999      3999 3.739306\n4000      4000 4.041626\n4001      4001 4.041626\n4002      4002 4.041626\n4003      4003 3.950333\n4004      4004 4.312192\n4005      4005 4.312192\n4006      4006 4.474046\n4007      4007 4.474046\n4008      4008 4.474046\n4009      4009 4.553087\n4010      4010 5.107968\n4011      4011 5.107968\n4012      4012 5.107968\n4013      4013 5.107968\n4014      4014 5.107968\n4015      4015 5.128813\n4016      4016 4.675617\n4017      4017 4.886032\n4018      4018 4.446265\n4019      4019 4.179377\n4020      4020 4.179377\n4021      4021 3.932296\n4022      4022 4.535224\n4023      4023 4.227161\n4024      4024 3.851229\n4025      4025 3.851229\n4026      4026 4.126672\n4027      4027 3.800192\n4028      4028 3.771227\n4029      4029 4.530337\n4030      4030 4.775592\n4031      4031 4.550269\n4032      4032 4.550269\n4033      4033 3.575821\n4034      4034 3.575821\n4035      4035 4.558504\n4036      4036 3.614770\n4037      4037 4.347068\n4038      4038 5.197253\n4039      4039 5.197253\n4040      4040 5.197253\n4041      4041 5.197253\n4042      4042 4.769135\n4043      4043 5.006588\n4044      4044 5.006588\n4045      4045 5.006588\n4046      4046 4.903507\n4047      4047 4.903507\n4048      4048 4.903507\n4049      4049 4.801895\n4050      4050 4.726698\n4051      4051 4.623544\n4052      4052 3.952481\n4053      4053 3.569387\n4054      4054 3.789329\n4055      4055 3.283543\n4056      4056 3.813440\n4057      4057 3.813440\n4058      4058 4.265379\n4059      4059 3.591156\n4060      4060 3.426614\n4061      4061 3.426614\n4062      4062 3.842418\n4063      4063 3.744736\n4064      4064 3.800898\n4065      4065 3.800898\n4066      4066 3.381079\n4067      4067 3.381079\n4068      4068 4.025604\n4069      4069 3.578282\n4070      4070 3.578282\n4071      4071 3.837102\n4072      4072 4.661582\n4073      4073 4.661582\n4074      4074 3.839373\n4075      4075 4.050913\n4076      4076 3.078418\n4077      4077 3.195758\n4078      4078 3.195758\n4079      4079 4.043924\n4080      4080 4.194006\n4081      4081 4.543564\n4082      4082 4.922001\n4083      4083 4.922001\n4084      4084 4.922001\n4085      4085 4.922001\n4086      4086 4.775869\n4087      4087 4.002557\n4088      4088 4.002557\n4089      4089 4.002557\n4090      4090 4.199898\n4091      4091 3.937479\n4092      4092 4.493290\n4093      4093 3.656738\n4094      4094 4.041504\n4095      4095 4.741356\n4096      4096 3.933424\n4097      4097 4.621076\n4098      4098 3.924437\n4099      4099 4.303328\n4100      4100 4.728505\n4101      4101 4.728505\n4102      4102 4.728505\n4103      4103 3.966010\n4104      4104 3.966010\n4105      4105 3.966010\n4106      4106 3.273059\n4107      4107 2.939756\n4108      4108 2.939756\n4109      4109 3.519241\n4110      4110 3.814445\n4111      4111 4.545468\n4112      4112 4.875780\n4113      4113 4.875780\n4114      4114 4.875780\n4115      4115 4.492776\n4116      4116 4.492776\n4117      4117 3.798200\n4118      4118 3.191106\n4119      4119 3.191106\n4120      4120 3.191106\n4121      4121 3.836301\n4122      4122 2.939989\n4123      4123 3.087876\n4124      4124 3.112169\n4125      4125 4.108833\n4126      4126 4.108833\n4127      4127 3.331675\n4128      4128 3.300545\n4129      4129 4.031747\n4130      4130 4.278090\n4131      4131 3.947976\n4132      4132 3.947976\n4133      4133 4.630164\n4134      4134 4.630164\n4135      4135 3.804122\n4136      4136 3.593171\n4137      4137 3.593171\n4138      4138 4.501932\n4139      4139 5.114715\n4140      4140 5.201153\n4141      4141 5.094616\n4142      4142 4.212722\n4143      4143 3.706554\n4144      4144 3.718049\n4145      4145 4.130902\n4146      4146 4.349551\n4147      4147 5.306450\n4148      4148 5.172043\n4149      4149 4.699723\n4150      4150 4.148230\n4151      4151 4.610185\n4152      4152 4.610185\n4153      4153 4.610185\n4154      4154 4.050556\n4155      4155 4.239414\n4156      4156 4.239414\n4157      4157 3.942260\n4158      4158 4.005673\n4159      4159 3.119558\n4160      4160 3.119558\n4161      4161 2.413470\n4162      4162 2.413470\n4163      4163 2.413470\n4164      4164 2.413470\n4165      4165 3.176763\n4166      4166 4.121583\n4167      4167 3.462288\n4168      4168 3.462288\n4169      4169 3.300324\n4170      4170 3.300324\n4171      4171 2.817179\n4172      4172 3.128779\n4173      4173 3.128779\n4174      4174 3.591740\n4175      4175 3.591740\n4176      4176 3.676296\n4177      4177 4.130889\n4178      4178 4.450920\n4179      4179 4.135136\n4180      4180 4.135136\n4181      4181 4.092444\n4182      4182 3.963999\n4183      4183 3.963999\n4184      4184 3.637764\n4185      4185 3.637764\n4186      4186 4.430438\n4187      4187 3.827754\n4188      4188 3.716732\n4189      4189 4.202814\n4190      4190 4.202814\n4191      4191 4.202814\n4192      4192 4.078359\n4193      4193 4.058037\n4194      4194 4.058037\n4195      4195 3.334211\n4196      4196 3.334211\n4197      4197 3.334211\n4198      4198 3.881660\n4199      4199 3.363501\n4200      4200 3.363501\n4201      4201 3.307058\n4202      4202 2.751032\n4203      4203 2.531125\n4204      4204 2.543965\n4205      4205 2.543965\n4206      4206 2.938007\n4207      4207 2.953486\n4208      4208 3.835081\n4209      4209 4.235447\n4210      4210 3.749324\n4211      4211 4.058931\n4212      4212 4.263546\n4213      4213 3.740720\n4214      4214 3.690008\n4215      4215 3.907262\n4216      4216 3.907262\n4217      4217 3.367126\n4218      4218 3.367126\n4219      4219 3.468494\n4220      4220 4.102210\n4221      4221 4.102210\n4222      4222 4.767206\n4223      4223 4.888572\n4224      4224 4.545554\n4225      4225 4.545554\n4226      4226 3.798091\n4227      4227 3.867273\n4228      4228 3.606892\n4229      4229 3.519349\n4230      4230 3.519349\n4231      4231 3.012833\n4232      4232 3.012833\n4233      4233 3.012833\n4234      4234 3.982545\n4235      4235 3.507661\n4236      4236 3.507661\n4237      4237 3.931228\n4238      4238 3.864686\n4239      4239 4.312429\n4240      4240 4.312429\n4241      4241 3.623468\n4242      4242 4.562714\n4243      4243 4.562714\n4244      4244 4.562714\n4245      4245 4.241131\n4246      4246 4.442011\n4247      4247 4.521407\n4248      4248 4.521407\n4249      4249 4.693673\n4250      4250 4.577887\n4251      4251 4.853357\n4252      4252 4.853357\n4253      4253 4.770343\n4254      4254 4.494324\n4255      4255 4.347745\n4256      4256 3.632940\n4257      4257 3.632940\n4258      4258 4.198059\n4259      4259 4.190550\n4260      4260 4.666345\n4261      4261 4.666345\n4262      4262 3.808784\n4263      4263 3.808784\n4264      4264 3.808784\n4265      4265 4.293011\n4266      4266 3.722328\n4267      4267 3.742206\n4268      4268 4.221737\n4269      4269 3.804457\n4270      4270 3.804457\n4271      4271 4.339470\n4272      4272 4.140649\n4273      4273 4.140649\n4274      4274 3.419761\n4275      4275 4.308728\n4276      4276 4.318935\n4277      4277 4.613504\n4278      4278 4.765482\n4279      4279 4.320224\n4280      4280 3.541802\n4281      4281 3.248320\n4282      4282 4.011037\n4283      4283 3.188231\n4284      4284 3.499060\n4285      4285 3.106881\n4286      4286 3.106881\n4287      4287 3.661654\n4288      4288 4.358996\n4289      4289 3.662899\n4290      4290 4.472561\n4291      4291 4.472561\n4292      4292 4.472561\n4293      4293 4.537299\n4294      4294 4.537299\n4295      4295 4.366719\n4296      4296 4.366719\n4297      4297 3.578809\n4298      4298 4.171344\n4299      4299 3.914013\n4300      4300 3.148961\n4301      4301 3.017007\n4302      4302 3.392342\n4303      4303 3.392342\n4304      4304 3.392342\n4305      4305 3.392342\n4306      4306 3.392342\n4307      4307 2.571506\n4308      4308 2.571506\n4309      4309 3.212003\n4310      4310 3.143884\n4311      4311 3.143884\n4312      4312 3.239793\n4313      4313 2.763724\n4314      4314 2.763724\n4315      4315 2.763724\n4316      4316 2.763724\n4317      4317 3.416056\n4318      4318 3.416056\n4319      4319 3.760337\n4320      4320 3.924752\n4321      4321 3.464638\n4322      4322 3.258067\n4323      4323 3.412831\n4324      4324 3.412831\n4325      4325 3.412831\n4326      4326 3.412831\n4327      4327 3.288146\n4328      4328 3.975262\n4329      4329 3.975262\n4330      4330 3.910463\n4331      4331 3.930380\n4332      4332 3.505232\n4333      4333 3.844832\n4334      4334 4.158894\n4335      4335 4.158894\n4336      4336 4.158894\n4337      4337 4.840755\n4338      4338 4.108391\n4339      4339 4.805598\n4340      4340 4.913975\n4341      4341 4.913975\n4342      4342 4.520579\n4343      4343 3.782398\n4344      4344 3.493733\n4345      4345 3.958308\n4346      4346 4.545926\n4347      4347 4.545926\n4348      4348 4.002389\n4349      4349 4.173500\n4350      4350 4.173500\n4351      4351 4.173500\n4352      4352 4.488988\n4353      4353 3.813050\n4354      4354 4.179624\n4355      4355 4.179624\n4356      4356 3.702565\n4357      4357 3.702565\n4358      4358 3.160537\n4359      4359 3.160537\n4360      4360 3.777139\n4361      4361 3.924713\n4362      4362 4.364667\n4363      4363 4.762497\n4364      4364 3.770433\n4365      4365 3.923626\n4366      4366 3.791008\n4367      4367 3.791008\n4368      4368 4.222553\n4369      4369 4.185737\n4370      4370 4.931379\n4371      4371 4.598562\n4372      4372 4.802419\n4373      4373 3.918547\n4374      4374 4.268852\n4375      4375 4.516411\n4376      4376 4.167002\n4377      4377 4.167002\n4378      4378 3.536163\n4379      4379 4.036440\n4380      4380 3.486664\n4381      4381 4.454748\n4382      4382 3.777674\n4383      4383 3.777674\n4384      4384 3.642218\n4385      4385 4.251998\n4386      4386 4.251998\n4387      4387 3.450035\n4388      4388 3.594318\n4389      4389 3.674539\n4390      4390 4.639440\n4391      4391 4.639440\n4392      4392 4.639440\n4393      4393 4.639440\n4394      4394 4.639440\n4395      4395 4.059831\n4396      4396 3.677420\n4397      4397 3.579556\n4398      4398 4.220774\n4399      4399 4.049426\n4400      4400 4.049426\n4401      4401 4.126399\n4402      4402 4.116113\n4403      4403 4.116113\n4404      4404 3.724063\n4405      4405 3.328532\n4406      4406 3.941843\n4407      4407 4.105140\n4408      4408 4.319086\n4409      4409 4.319086\n4410      4410 3.897242\n4411      4411 3.897242\n4412      4412 4.865743\n4413      4413 4.865743\n4414      4414 4.351157\n4415      4415 5.003156\n4416      4416 5.003156\n4417      4417 5.003156\n4418      4418 5.003156\n4419      4419 5.064571\n4420      4420 5.064571\n4421      4421 5.584396\n4422      4422 5.584396\n4423      4423 5.232697\n4424      4424 4.556344\n4425      4425 4.556344\n4426      4426 4.666162\n4427      4427 4.666162\n4428      4428 4.059649\n4429      4429 4.459426\n4430      4430 3.753728\n4431      4431 3.246111\n4432      4432 3.193076\n4433      4433 3.414340\n4434      4434 3.414340\n4435      4435 2.566039\n4436      4436 2.566039\n4437      4437 2.566039\n4438      4438 3.220278\n4439      4439 3.961054\n4440      4440 3.107585\n4441      4441 3.267632\n4442      4442 3.267632\n4443      4443 3.267632\n4444      4444 3.267632\n4445      4445 4.210845\n4446      4446 4.819878\n4447      4447 4.689346\n4448      4448 4.656235\n4449      4449 4.656235\n4450      4450 3.810631\n4451      4451 3.465642\n4452      4452 3.745871\n4453      4453 3.542820\n4454      4454 3.542820\n4455      4455 3.542820\n4456      4456 4.377332\n4457      4457 4.007620\n4458      4458 3.600558\n4459      4459 3.277094\n4460      4460 3.277094\n4461      4461 3.277094\n4462      4462 3.221247\n4463      4463 3.859297\n4464      4464 4.363538\n4465      4465 4.878903\n4466      4466 4.878903\n4467      4467 4.589258\n4468      4468 3.807977\n4469      4469 3.807977\n4470      4470 4.318157\n4471      4471 4.318157\n4472      4472 3.996927\n4473      4473 3.679691\n4474      4474 3.656863\n4475      4475 3.656863\n4476      4476 3.251642\n4477      4477 3.113078\n4478      4478 3.113078\n4479      4479 3.992167\n4480      4480 3.992167\n4481      4481 3.436625\n4482      4482 3.281995\n4483      4483 4.124656\n4484      4484 4.914965\n4485      4485 5.649210\n4486      4486 5.649210\n4487      4487 5.649210\n4488      4488 5.495632\n4489      4489 4.964133\n4490      4490 5.081892\n4491      4491 4.461864\n4492      4492 4.304350\n4493      4493 4.304350\n4494      4494 4.139114\n4495      4495 4.139114\n4496      4496 4.139114\n4497      4497 4.983860\n4498      4498 4.987819\n4499      4499 4.987819\n4500      4500 5.120343\n4501      4501 5.120343\n4502      4502 4.591728\n4503      4503 4.591728\n4504      4504 4.737546\n4505      4505 4.737546\n4506      4506 4.737546\n4507      4507 4.579610\n4508      4508 4.849638\n4509      4509 4.849638\n4510      4510 4.849638\n4511      4511 4.313904\n4512      4512 4.313904\n4513      4513 4.263220\n4514      4514 5.070220\n4515      4515 5.070220\n4516      4516 5.070220\n4517      4517 4.441579\n4518      4518 4.441579\n4519      4519 3.673144\n4520      4520 3.673144\n4521      4521 3.673144\n4522      4522 4.317112\n4523      4523 5.092454\n4524      4524 5.198315\n4525      4525 5.040879\n4526      4526 5.040879\n4527      4527 5.040879\n4528      4528 5.040879\n4529      4529 4.447038\n4530      4530 4.447038\n4531      4531 4.447038\n4532      4532 4.467313\n4533      4533 3.612370\n4534      4534 3.612370\n4535      4535 3.612370\n4536      4536 3.612370\n4537      4537 3.785352\n4538      4538 4.043420\n4539      4539 4.043420\n4540      4540 3.557096\n4541      4541 3.714871\n4542      4542 3.714871\n4543      4543 3.714871\n4544      4544 4.451720\n4545      4545 4.419294\n4546      4546 4.419294\n4547      4547 4.174029\n4548      4548 4.174029\n4549      4549 4.637012\n4550      4550 3.728632\n4551      4551 4.542127\n4552      4552 4.542127\n4553      4553 3.758469\n4554      4554 4.711790\n4555      4555 3.798442\n4556      4556 3.798442\n4557      4557 4.694796\n4558      4558 4.946131\n4559      4559 4.215403\n4560      4560 4.215403\n4561      4561 4.215403\n4562      4562 4.040645\n4563      4563 3.790247\n4564      4564 3.925087\n4565      4565 3.886630\n4566      4566 3.846079\n4567      4567 4.683416\n4568      4568 3.686762\n4569      4569 4.476324\n4570      4570 4.285588\n4571      4571 4.285588\n4572      4572 4.278706\n4573      4573 3.940191\n4574      4574 3.940191\n4575      4575 3.749923\n4576      4576 3.726552\n4577      4577 3.726552\n4578      4578 3.726552\n4579      4579 3.726552\n4580      4580 3.750743\n4581      4581 3.109490\n4582      4582 4.066677\n4583      4583 3.815190\n4584      4584 3.815190\n4585      4585 3.815190\n4586      4586 3.902123\n4587      4587 4.060543\n4588      4588 4.773345\n4589      4589 4.638966\n4590      4590 3.968072\n4591      4591 3.968072\n4592      4592 3.968072\n4593      4593 4.773630\n4594      4594 4.773630\n4595      4595 4.773630\n4596      4596 4.773630\n4597      4597 5.027346\n4598      4598 4.962266\n4599      4599 4.553973\n4600      4600 4.051785\n4601      4601 4.051785\n4602      4602 4.776574\n4603      4603 4.776574\n4604      4604 4.379002\n4605      4605 4.379002\n4606      4606 3.795242\n4607      4607 4.326537\n4608      4608 3.712085\n4609      4609 2.856604\n4610      4610 3.786621\n4611      4611 3.395281\n4612      4612 4.140892\n4613      4613 4.140892\n4614      4614 4.140892\n4615      4615 3.254754\n4616      4616 3.254754\n4617      4617 3.211874\n4618      4618 3.211874\n4619      4619 3.791664\n4620      4620 3.396259\n4621      4621 3.766204\n4622      4622 3.766204\n4623      4623 3.497325\n4624      4624 3.497325\n4625      4625 3.057501\n4626      4626 3.057501\n4627      4627 3.057501\n4628      4628 3.739089\n4629      4629 4.235403\n4630      4630 4.034510\n4631      4631 4.034510\n4632      4632 3.982695\n4633      4633 3.835281\n4634      4634 3.798712\n4635      4635 3.151836\n4636      4636 3.151836\n4637      4637 3.151836\n4638      4638 3.151836\n4639      4639 3.568200\n4640      4640 3.159090\n4641      4641 3.159090\n4642      4642 3.521518\n4643      4643 3.254111\n4644      4644 2.614648\n4645      4645 2.614648\n4646      4646 3.553200\n4647      4647 3.688736\n4648      4648 3.688736\n4649      4649 3.688736\n4650      4650 3.688736\n4651      4651 4.082657\n4652      4652 4.094656\n4653      4653 4.094656\n4654      4654 4.094656\n4655      4655 4.223656\n4656      4656 3.546471\n4657      4657 3.689854\n4658      4658 3.689854\n4659      4659 3.689854\n4660      4660 3.689854\n4661      4661 3.248550\n4662      4662 3.905443\n4663      4663 3.777484\n4664      4664 3.962199\n4665      4665 3.972570\n4666      4666 3.746432\n4667      4667 3.746432\n4668      4668 4.152148\n4669      4669 4.492517\n4670      4670 3.720864\n4671      4671 3.720864\n4672      4672 4.057112\n4673      4673 3.526247\n4674      4674 3.457074\n4675      4675 3.457074\n4676      4676 2.553638\n4677      4677 3.020948\n4678      4678 3.377251\n4679      4679 3.157090\n4680      4680 3.696234\n4681      4681 3.696234\n4682      4682 4.578836\n4683      4683 4.862889\n4684      4684 4.862889\n4685      4685 4.018237\n4686      4686 4.018237\n4687      4687 3.337876\n4688      4688 3.337876\n4689      4689 3.337876\n4690      4690 3.835954\n4691      4691 4.340146\n4692      4692 4.069809\n4693      4693 3.788761\n4694      4694 3.788761\n4695      4695 3.788761\n4696      4696 3.824843\n4697      4697 3.824843\n4698      4698 3.622374\n4699      4699 3.622374\n4700      4700 3.988366\n4701      4701 4.711678\n4702      4702 3.748998\n4703      4703 2.799461\n4704      4704 2.799461\n4705      4705 2.799461\n4706      4706 2.799461\n4707      4707 3.046775\n4708      4708 3.046775\n4709      4709 3.046775\n4710      4710 3.046775\n4711      4711 3.942333\n4712      4712 4.292024\n4713      4713 4.563616\n4714      4714 5.349413\n4715      4715 4.949548\n4716      4716 4.294058\n4717      4717 4.351417\n4718      4718 4.351417\n4719      4719 4.244167\n4720      4720 3.897694\n4721      4721 3.897694\n4722      4722 4.368165\n4723      4723 4.123736\n4724      4724 4.237359\n4725      4725 4.901631\n4726      4726 4.115123\n4727      4727 4.115123\n4728      4728 3.830769\n4729      4729 3.830769\n4730      4730 3.830769\n4731      4731 2.834804\n4732      4732 2.969859\n4733      4733 2.969859\n4734      4734 2.942554\n4735      4735 2.729123\n4736      4736 2.729123\n4737      4737 2.729123\n4738      4738 2.729123\n4739      4739 2.729123\n4740      4740 2.905927\n4741      4741 2.470595\n4742      4742 3.259182\n4743      4743 3.441287\n4744      4744 4.006182\n4745      4745 4.466818\n4746      4746 5.385420\n4747      4747 5.385420\n4748      4748 4.615484\n4749      4749 4.455227\n4750      4750 4.455227\n4751      4751 3.881306\n4752      4752 4.610616\n4753      4753 4.804504\n4754      4754 4.773679\n4755      4755 5.067584\n4756      4756 4.497950\n4757      4757 3.981187\n4758      4758 4.216648\n4759      4759 4.216648\n4760      4760 3.726653\n4761      4761 3.427066\n4762      4762 4.247182\n4763      4763 4.336788\n4764      4764 3.632643\n4765      4765 4.285023\n4766      4766 3.337857\n4767      4767 4.011764\n4768      4768 4.722435\n4769      4769 3.751283\n4770      4770 3.751283\n4771      4771 3.558835\n4772      4772 3.558835\n4773      4773 3.864593\n4774      4774 3.889092\n4775      4775 3.731720\n4776      4776 4.320327\n4777      4777 4.320327\n4778      4778 4.643683\n4779      4779 3.811553\n4780      4780 3.811553\n4781      4781 3.190383\n4782      4782 3.168487\n4783      4783 3.168487\n4784      4784 2.959984\n4785      4785 3.133561\n4786      4786 3.025249\n4787      4787 3.025249\n4788      4788 3.025249\n4789      4789 3.630521\n4790      4790 3.630521\n4791      4791 2.955449\n4792      4792 3.686376\n4793      4793 3.774678\n4794      4794 4.223212\n4795      4795 3.312320\n4796      4796 3.312320\n4797      4797 3.312320\n4798      4798 3.006497\n4799      4799 3.438605\n4800      4800 4.136336\n4801      4801 4.136336\n4802      4802 3.943360\n4803      4803 3.943360\n4804      4804 3.943360\n4805      4805 3.943360\n4806      4806 3.943360\n4807      4807 3.943360\n4808      4808 3.701636\n4809      4809 3.791265\n4810      4810 3.997246\n4811      4811 3.997246\n4812      4812 3.640497\n4813      4813 3.640497\n4814      4814 2.691107\n4815      4815 2.691107\n4816      4816 2.819108\n4817      4817 3.187300\n4818      4818 3.788135\n4819      4819 3.788135\n4820      4820 3.092733\n4821      4821 3.092733\n4822      4822 2.266449\n4823      4823 3.151594\n4824      4824 3.151594\n4825      4825 2.625621\n4826      4826 2.625621\n4827      4827 3.144582\n4828      4828 3.144582\n4829      4829 3.662160\n4830      4830 3.243298\n4831      4831 3.103966\n4832      4832 3.103966\n4833      4833 3.138567\n4834      4834 3.236649\n4835      4835 3.236649\n4836      4836 3.236649\n4837      4837 4.185368\n4838      4838 3.245600\n4839      4839 3.045996\n4840      4840 3.045996\n4841      4841 3.045996\n4842      4842 3.710411\n4843      4843 3.791289\n4844      4844 4.422365\n4845      4845 4.413320\n4846      4846 5.258937\n4847      4847 5.258937\n4848      4848 5.258937\n4849      4849 5.258937\n4850      4850 5.258937\n4851      4851 4.762817\n4852      4852 3.791591\n4853      4853 3.639014\n4854      4854 3.785645\n4855      4855 3.785645\n4856      4856 2.841862\n4857      4857 2.841862\n4858      4858 2.841862\n4859      4859 2.848812\n4860      4860 2.848812\n4861      4861 3.440905\n4862      4862 3.440905\n4863      4863 4.324956\n4864      4864 4.324956\n4865      4865 4.324956\n4866      4866 4.209304\n4867      4867 4.241175\n4868      4868 3.668482\n4869      4869 4.279025\n4870      4870 3.555930\n4871      4871 4.164776\n4872      4872 4.938571\n4873      4873 4.938571\n4874      4874 4.938571\n4875      4875 4.938571\n4876      4876 4.938571\n4877      4877 4.429863\n4878      4878 4.429863\n4879      4879 4.048504\n4880      4880 4.048504\n4881      4881 4.673718\n4882      4882 4.429344\n4883      4883 4.429344\n4884      4884 4.373489\n4885      4885 4.082995\n4886      4886 4.082995\n4887      4887 3.632477\n4888      4888 3.632477\n4889      4889 2.788050\n4890      4890 3.487909\n4891      4891 3.990406\n4892      4892 3.380752\n4893      4893 3.440670\n4894      4894 3.440670\n4895      4895 4.295628\n4896      4896 3.377986\n4897      4897 4.052582\n4898      4898 4.052582\n4899      4899 4.052582\n4900      4900 4.363150\n4901      4901 4.117074\n4902      4902 3.854373\n4903      4903 3.707691\n4904      4904 3.707691\n4905      4905 3.146468\n4906      4906 3.146468\n4907      4907 3.830725\n4908      4908 3.801975\n4909      4909 3.801975\n4910      4910 4.583888\n4911      4911 4.583888\n4912      4912 4.416315\n4913      4913 4.854911\n4914      4914 4.693407\n4915      4915 4.693407\n4916      4916 4.144444\n4917      4917 4.970362\n4918      4918 4.970362\n4919      4919 4.497230\n4920      4920 4.497230\n4921      4921 4.526850\n4922      4922 4.069659\n4923      4923 4.530837\n4924      4924 4.015467\n4925      4925 3.726984\n4926      4926 4.624478\n4927      4927 4.226937\n4928      4928 4.226937\n4929      4929 3.804274\n4930      4930 3.804274\n4931      4931 3.001337\n4932      4932 3.665944\n4933      4933 3.665944\n4934      4934 3.875322\n4935      4935 3.258277\n4936      4936 3.368829\n4937      4937 3.198459\n4938      4938 3.843047\n4939      4939 3.843047\n4940      4940 4.047742\n4941      4941 4.577135\n4942      4942 5.062792\n4943      4943 5.154496\n4944      4944 4.322231\n4945      4945 3.582822\n4946      4946 4.027024\n4947      4947 3.850846\n4948      4948 3.190460\n4949      4949 3.190460\n4950      4950 3.190460\n4951      4951 4.119772\n4952      4952 4.119772\n4953      4953 3.194251\n4954      4954 3.516918\n4955      4955 3.761548\n4956      4956 3.053790\n4957      4957 3.220962\n4958      4958 4.070391\n4959      4959 3.399774\n4960      4960 3.956637\n4961      4961 3.734336\n4962      4962 3.734336\n4963      4963 3.345003\n4964      4964 3.134806\n4965      4965 3.606663\n4966      4966 3.606663\n4967      4967 3.606663\n4968      4968 3.660412\n4969      4969 3.660412\n4970      4970 3.631610\n4971      4971 3.874438\n4972      4972 4.268108\n4973      4973 4.268108\n4974      4974 3.993982\n4975      4975 4.238419\n4976      4976 4.238419\n4977      4977 3.446317\n4978      4978 3.446317\n4979      4979 3.446317\n4980      4980 4.421378\n4981      4981 3.736694\n4982      4982 3.838587\n4983      4983 4.024999\n4984      4984 4.289882\n4985      4985 3.988060\n4986      4986 3.988060\n4987      4987 3.988060\n4988      4988 4.295560\n4989      4989 4.163227\n4990      4990 3.823372\n4991      4991 4.167957\n4992      4992 4.167957\n4993      4993 4.401179\n4994      4994 4.600079\n4995      4995 4.600079\n4996      4996 4.159560\n4997      4997 4.159560\n4998      4998 4.159560\n4999      4999 4.796712\n5000      5000 4.173470"
  },
  {
    "objectID": "slides/slides11.html#mh_tour-2",
    "href": "slides/slides11.html#mh_tour-2",
    "title": "Implementing Metropolis-Hastings",
    "section": "mh_tour",
    "text": "mh_tour"
  },
  {
    "objectID": "slides/slides11.html#tuning-the-sampler",
    "href": "slides/slides11.html#tuning-the-sampler",
    "title": "Implementing Metropolis-Hastings",
    "section": "Tuning the sampler",
    "text": "Tuning the sampler\n\nset.seed(84735)\nmh_simulation_1 &lt;- mh_tour(N = 5000, w = 1)\nset.seed(84735)\nmh_simulation_2 &lt;- mh_tour(N = 5000, w = .01)\nset.seed(84735)\nmh_simulation_3 &lt;- mh_tour(N = 5000, w = 100)"
  },
  {
    "objectID": "slides/slides11.html#which-is-w1-w.01-and-w100",
    "href": "slides/slides11.html#which-is-w1-w.01-and-w100",
    "title": "Implementing Metropolis-Hastings",
    "section": "Which is \\(w=1\\), \\(w=.01\\), and \\(w=100\\)?",
    "text": "Which is \\(w=1\\), \\(w=.01\\), and \\(w=100\\)?"
  },
  {
    "objectID": "slides/slides11.html#independence-sampling",
    "href": "slides/slides11.html#independence-sampling",
    "title": "Implementing Metropolis-Hastings",
    "section": "Independence Sampling",
    "text": "Independence Sampling\nIf the parameter we are estimating has a restricted range (eg \\(\\pi \\in [0,1]\\), \\(\\lambda &gt; 0\\)), we might not want to use a proposal model that depends on the current value (e.g. \\(\\pi' | \\pi \\sim Unif(-w,w)\\))\n\n\n\n\n\n\nIndependence Sampling Algorithm\n\n\nA special case of Metropolis-Hastings that uses hte same proposal model at each iteration, independent of the chains current location. The proposal model is \\(q(\\pi')\\) as opposed to \\(q(\\pi' | \\pi)\\). Thus, the acceptance probability simplifies to:\n\\[\\alpha = \\min \\{1, \\frac{f(\\pi') L(\\pi'|y) q(\\pi)}{f(\\pi) L(\\pi|y) q(\\pi')}\\}\\]"
  },
  {
    "objectID": "slides/slides13.html#posterior-inference",
    "href": "slides/slides13.html#posterior-inference",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n1. Estimation\n\n\n2. Hypothesis Testing\n\n\n3. Prediction"
  },
  {
    "objectID": "slides/slides13.html#running-example",
    "href": "slides/slides13.html#running-example",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Running Example",
    "text": "Running Example\n\nlibrary(bayesrules)\nlibrary(tidyverse)\ndata(\"pulse_of_the_nation\")\n\nCards Against Humanity’s “Pulse of the Nation” project (https://thepulseofthenation.com/) conducted monthly polls into people’s social and political views, as well as some silly things. This data includes 1,000 responses to a subset of questions included in the poll conducted in September 2017."
  },
  {
    "objectID": "slides/slides13.html#what-proportion-of-us-adults-do-not-believe-in-climate-change",
    "href": "slides/slides13.html#what-proportion-of-us-adults-do-not-believe-in-climate-change",
    "title": "Posterior estimation and hypothesis testing",
    "section": "What proportion of US adults do not believe in climate change?",
    "text": "What proportion of US adults do not believe in climate change?\n\npulse_of_the_nation %&gt;%\n  count(climate_change)\n\n# A tibble: 3 × 2\n  climate_change                    n\n  &lt;fct&gt;                         &lt;int&gt;\n1 Not Real At All                 150\n2 Real and Caused by People       655\n3 Real but not Caused by People   195\n\n\n\n\n\n\n\n\nWarm up\n\n\nLet \\(\\pi \\sim\\) Beta(1,2) and \\(Y | \\pi \\sim\\) Binom(n, \\(\\pi\\)). What is the distribution of the posterior \\(\\pi | Y\\)?"
  },
  {
    "objectID": "slides/slides13.html#section",
    "href": "slides/slides13.html#section",
    "title": "Posterior estimation and hypothesis testing",
    "section": "",
    "text": "plot_beta_binomial(alpha = 1, beta = 2, y = 150, n = 1000)"
  },
  {
    "objectID": "slides/slides13.html#bayesian-vs-frequentist-inference",
    "href": "slides/slides13.html#bayesian-vs-frequentist-inference",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Bayesian vs Frequentist inference",
    "text": "Bayesian vs Frequentist inference\nIn a frequentist analysis, \\(\\pi\\) is viewed as a fixed, unknown quantity. We make inferences about \\(\\pi\\) using data \\(Y\\)\n\nIn a Bayesian analysis, \\(\\pi\\) is viewed as a random variable. We make inferences about \\(\\pi\\) using the posterior, which combines the prior and likelihood"
  },
  {
    "objectID": "slides/slides13.html#example-a-quick-frequentist-analysis",
    "href": "slides/slides13.html#example-a-quick-frequentist-analysis",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Example: a quick frequentist analysis",
    "text": "Example: a quick frequentist analysis\nPoint estimate: 150/1000\n95% frequentist confidence interval:\n\\[0.15 \\pm 0.02 = (0.13, 0.17)\\]\n\nThere is a 95% chance that \\(\\pi\\) is between 0.13 and 0.17\n\n\n\nThere’s a 95% chance that the survey results we happened to get produced an interval that covers \\(\\pi\\)"
  },
  {
    "objectID": "slides/slides13.html#example-a-quick-frequentist-analysis-1",
    "href": "slides/slides13.html#example-a-quick-frequentist-analysis-1",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Example: a quick frequentist analysis",
    "text": "Example: a quick frequentist analysis\n\\(H_0: \\pi = 0.13\\)\n\\(H_A: \\pi &gt; 0.13\\)\np-value is 0.0333\n\nGiven our polling results, there’s a 3.3% chance that \\(\\pi\\) exceeds 13%\n\n\n\nIf in fact \\(\\pi\\) were only 0.13, there’s a 3.3% chance that we would have gotten a survey where 150 of 1000 people did not believe climate change is real"
  },
  {
    "objectID": "slides/slides13.html#rest-of-today",
    "href": "slides/slides13.html#rest-of-today",
    "title": "Posterior estimation and hypothesis testing",
    "section": "Rest of today",
    "text": "Rest of today\nExplore the Bayesian approach to estimation (with uncertainty) and hypothesis testing, using the pulse_of_the_nation data and the climate change question.\nBegin by using the exact posterior, and then extend the ideas to approximate posteriors sampled using MCMC\nIf you finish, please fill out the two surveys linked on the schedule before leaving class"
  },
  {
    "objectID": "slides/test.html#quarto",
    "href": "slides/test.html#quarto",
    "title": "Untitled",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "slides/test.html#bullets",
    "href": "slides/test.html#bullets",
    "title": "Untitled",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "slides/test.html#code",
    "href": "slides/test.html#code",
    "title": "Untitled",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "homework/hw05.html",
    "href": "homework/hw05.html",
    "title": "Individual HW05",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\nlibrary(rstan) # for MCMC\nlibrary(bayesplot) # for plotting\n\nBR Exercise 8.7\nFor each situation, find the appropriate credible interval using the “middle” approach\n\nA 99% credible interval of \\(\\lambda\\) where \\(\\lambda | y \\sim\\) Gamma(1,5)\nA 95% credible interval of \\(\\mu\\) where \\(\\mu | y \\sim N(10, 2^2)\\)\n\nAn 80% credible interval of \\(\\mu\\) where \\(\\mu | y \\sim N(-3, 1^2)\\)\n\nBR Exercise 8.8\nThere’s more than one approach to constructing a 95% credible interval. The “middle 95%” approach reports the range of the middle 95% of the posterior density, from the 2.5th to the 97.5th percentile. The “highest posterior density” approach reports the 95% of posterior values with the highest posterior densities.\n\nLet \\(\\lambda | y \\sim\\)Gamma(1,5). Construct the 95% highest posterior density credible interval for \\(\\lambda\\). Represent this interval on a sketch of the posterior pdf. Hint: The sketch itself will help you identify the appropriate CI. Do not try to find a solution that will generalize to any posterior density, just focus on this specific density.\nRepeat part a using the middle 95% approach.\nCompare the two intervals from parts a and b. Are they the same? If not, how do they differ and which is more appropriate here?\nLet \\(\\mu | y \\sim N(-13, 2^2)\\). Construct the 95% highest posterior density credible interval for \\(\\mu\\).\nRepeat part d using the middle 95% approach.\nCompare the two intervals from parts d and e. Are they the same? If not, why not?\nBR Exercise 8.21\nThe loon is a species of bird common to the Ontario region of Canada. Let \\(\\lambda\\) denote the typical number of loons observed by a birdwatcher across a 100-hour observation period. To learn about \\(\\lambda\\), we’ll utilize bird counts \\((Y_1, ..., Y_n)\\) collected in \\(n\\) different outings.\n\nExplain which Bayesian model is appropriate for this analysis: Beta-Binomial, Gamma-Poisson, or Normal-Normal.\nYour prior understanding is that the typical rate of loon sightings is 2 per 100 hours with a standard deviation of 1 per 100-hours. Specify an appropriate prior model for \\(\\lambda\\) and explain your reasoning.\nThe loons data in the {bayesrules} package contains loon counts in different 100-hour observation periods. How many data points do we have and what’s the average loon count per 100 hours?\nIn light of your prior and data, calculate and interpret a (middle) 95% posterior credible interval for \\(\\lambda\\). NOTE: You’ll first need to specify your posterior model of \\(\\lambda\\)\n\nBR Exercise 8.22\nLet’s continue our analysis of \\(\\lambda\\), the typical rate of loon sightings in a 100-hour observation period. You hypothesize that birdwatchers should anticipate a rate of less than 1 loon per observation period.\n\nState this as a formal hypothesis test (using \\(H_0\\), \\(H_a\\), and \\(\\lambda\\) notation)\n\nWhat decision might you make about these hypotheses utilizing the credible interval from the previous exercise?\nCalculate and interpret the posterior probability that your hypothesis is true.\nPutting this together, explain your conclusion about \\(\\lambda\\)\n\nQ5 (TBA Wed)\nQ6 (TBA Wed)",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW05"
    ]
  },
  {
    "objectID": "homework/gw02-sols.html",
    "href": "homework/gw02-sols.html",
    "title": "gw02-sols",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 2.13\nSee handwritten\n\npi &lt;- c(.4, .5, .6, .7)\nprior &lt;- c(.1, .2, .44, .26)\nlik &lt;- dbinom(47, size = 80, prob = pi)\npost_unnorm &lt;- prior*lik\npost_norm &lt;- prior*lik/sum(post_unnorm)\n\ntibble(pi, prior, lik, post_unnorm, post_norm)\n\n# A tibble: 4 × 5\n     pi prior      lik post_unnorm post_norm\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1   0.4  0.1  0.000301   0.0000301  0.000649\n2   0.5  0.2  0.0264     0.00527    0.114   \n3   0.6  0.44 0.0880     0.0387     0.834   \n4   0.7  0.26 0.00929    0.00242    0.0520  \n\n\nBR Exercise 2.18\nThe smallest Y where \\(P(\\pi = .6 |Y) &gt; .4\\) is 6\n\ntibble(\n  y = 0:10, # possible values of Y\n  p_y = dbinom(y, size = 10, prob = .6)) |&gt; # f(Y|pi = .6)\n  rowwise() |&gt;\n  mutate(\n    norm_constant = (.25*dbinom(y, 10, .2) + .5*dbinom(y, 10, .4) + .25*dbinom(y, 10, .6)), # Normalizing constant for each possible value of Y\n    posterior = (.25*p_y)/norm_constant # posterior: f(pi = .6 | Y)\n  )\n\n# A tibble: 11 × 4\n# Rowwise: \n       y      p_y norm_constant posterior\n   &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0 0.000105       0.0299   0.000877\n 2     1 0.00157        0.0877   0.00449 \n 3     2 0.0106         0.139    0.0191  \n 4     3 0.0425         0.168    0.0630  \n 5     4 0.111          0.175    0.159   \n 6     5 0.201          0.157    0.319   \n 7     6 0.251          0.120    0.523   \n 8     7 0.215          0.0752   0.715   \n 9     8 0.121          0.0356   0.850   \n10     9 0.0403         0.0109   0.928   \n11    10 0.00605        0.00156  0.966   \n\n\nBR Exercise 2.17\n\n# Define possible trees\ntrees &lt;- data.frame(type = c(\"mold\", \"not mold\"))\n\n# Define the prior model\nprior &lt;- c(0.18, .82)\n\n# Simulate 10000 trees \nset.seed(84735)\ntrees_sim &lt;- sample_n(trees, size = 10000, \n                        weight = prior, replace = TRUE)\n\ntrees_sim &lt;- trees_sim %&gt;% \n  mutate(data_model = case_when(type == \"mold\" ~ 0.8,\n                                type == \"not mold\" ~ 0.1))\n\n# Define  tree species\ndata &lt;- c(\"maple\", \"not maple\")\n\n# Simulate species\n\ntrees_sim &lt;- trees_sim %&gt;%\n  group_by(1:n()) %&gt;% \n  mutate(species = sample(data, size = 1, \n                        prob = c(data_model, 1-data_model)))\n\ntrees_sim %&gt;% \n  filter(species == \"maple\") %&gt;% \n  tabyl(type) %&gt;% \n  adorn_totals(\"row\")\n\n     type    n   percent\n     mold 1482 0.6327925\n not mold  860 0.3672075\n    Total 2342 1.0000000\n\n\nMLE\nIf \\(Y \\sim \\text{Binom}(n, \\pi)\\), show that \\(\\hat{\\pi}_{MLE} = \\frac{Y}{n}\\)\n(see handwritten)\nBR Exercise 3.1\nFor each part, use plot_beta and/or summarize_beta to justify your answer\nBR Exercise 3.12\nChoice of prior\nI am interviewing Carleton students about whether or not they have used (knowingly) used ChatGPT on coursework in a non-approved way. I think the proportion has a 90% chance of being less than .25.\n\nChoose an informative prior that you think is reasonable for this belief\n\nI then ask 20 students this question and 15 respond “yes”. Find the posterior using the prior from above, then using the 3 non/weakly informative priors below:\n\nUnif(0,1) prior\nBeta(2,2) prior\n“Reference” prior Beta(.5, .5)\n\nCompare the posteriors for each of the priors above. Do results change if we instead observe 150/200 students responding “yes”?\nBR 4.13\n(see handwritten)",
    "crumbs": [
      "Assignments",
      "Homework",
      "gw02-sols"
    ]
  },
  {
    "objectID": "homework/hw02.html",
    "href": "homework/hw02.html",
    "title": "Individual HW02",
    "section": "",
    "text": "BR Exercise 2.16\n\n\n\n\n\n\nNote\n\n\n\nIf the link from the textbook doesn’t work, try this link from the internet archive\n\n\n\n\nBR Exercise 2.21\n\n\nBR Exercise 3.8\n\n\nsummarize_beta_binomial\nWrite your own R function that does the same thing as summarize_beta_binomial. It’s OK if your output is formatted differently and you do not need to write formal checks.\n\n\n\n\n\n\nNote\n\n\n\nIf you need a refresher on writing functions in R, I recommend:\n\nR4DS Ch25 for an overview of functions in R with practice problems\nMy Stat220 slides for a quick refresher\n\n\n\n\n\nBR Exercise 3.9\n\n\nBR Exercise 3.10\n\n\nBR Exercise 4.10\n\n\nBR Exercise 4.14\n\n\nBR Exercise 4.18",
    "crumbs": [
      "Assignments",
      "Homework",
      "Individual HW02"
    ]
  },
  {
    "objectID": "activities/09-activity.html",
    "href": "activities/09-activity.html",
    "title": "Class Examples - Day 9",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\nlibrary(bayesrules)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\nAttaching package: 'rstan'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n- Online documentation and vignettes at mc-stan.org/bayesplot\n- bayesplot theme set to bayesplot::theme_default()\n   * Does _not_ affect other ggplot2 plots\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nExample 1\n\nfn_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real mu;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(mu, sigma);\n    mu ~ normal(0, 1000);\n    sigma ~ exponential(.0001);\n  }\n\"\n\nfn_sim &lt;- stan(model_code = fn_model, \n               data = list(N=2, y = c(-1,1)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\n\nui_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real alpha;\n    real beta;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(alpha + beta, sigma);\n    alpha ~ normal(0, 1000);\n    beta ~ normal(0, 1000);\n    sigma ~ exponential(1);\n  }\n\"\n\nui_sim &lt;- stan(model_code = ui_model, \n               data = list(N=100, y = rnorm(100)), \n               chains = 4, iter = 1000*2, seed = 84735)\n\n\ni_model &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] y;\n  }\n\n  parameters {\n    real alpha;\n    real beta;\n    real&lt;lower=0&gt; sigma;\n  }\n\n  model {\n    y ~ normal(alpha + beta, sigma);\n    alpha ~ normal(0, 1);\n    beta ~ normal(0, 1);\n    sigma ~ exponential(1);\n  }\n\"\n\ni_sim &lt;- stan(model_code = i_model, \n               data = list(N=100, y = rnorm(100)), \n               chains = 4, iter = 1000*2, seed = 84735)"
  },
  {
    "objectID": "computing/rstudio-stat340.html",
    "href": "computing/rstudio-stat340.html",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "",
    "text": "In Stat340, I will distribute homework assignment templates in.qmd format.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#uploading-files-to-maize",
    "href": "computing/rstudio-stat340.html#uploading-files-to-maize",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Uploading files to maize",
    "text": "Uploading files to maize\n\nClick the “upload” button in the files pane\nSelect the file from your local computer (likely in your Downloads folder if you downloaded from moodle)\nChoose the “Target Directory” – this is the folder on your maize account where the file will be saved. I recommend creating a separate folder for this class.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#accessing-maize-from-off-campus",
    "href": "computing/rstudio-stat340.html#accessing-maize-from-off-campus",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Accessing maize from off campus",
    "text": "Accessing maize from off campus\nIf you are using the maize server, you should be able to access it on campus with only your Carleton ID and password. If you plan to use the maize server and you plan to do any work off campus this term (e.g., while on a field trip, travel for athletics, or just sitting in Little Joy) you need to install Carleton’s VPN to have access.\nTo install the GlobalProtect VPN follow directions provided by ITS.",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/rstudio-stat340.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "href": "computing/rstudio-stat340.html#installing-latex-not-needed-if-you-are-using-the-maize-server",
    "title": "Using R/RStudio for Homework in Stat340",
    "section": "Installing LaTeX (not needed if you are using the maize server)",
    "text": "Installing LaTeX (not needed if you are using the maize server)\nIf you don’t already have a tex package installed on your computer, the easiest option to create pdf’s is to use the tinytex R package. This can be installed with the following R commands:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()  # install TinyTeX\n\nIf you’d like a standalone LaTeX package that will work with programs other than RStudio, you could install the basic installations of either:\nIf you’d like a stand alone LaTeX package, you could install the basic installations of either:\n\n\nMacTeX for Mac (3.2GB!)\n\nMiKTeX for Windows (190MB)",
    "crumbs": [
      "Computing",
      "Rstudio in Stat 340"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "groupwork/gw02.html",
    "href": "groupwork/gw02.html",
    "title": "Group Work 02",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\n\nBR Exercise 2.13\nBR Exercise 2.18\nBR Exercise 2.17\nMLE\nIf \\(Y \\sim \\text{Binom}(n, \\pi)\\), show that \\(\\hat{\\pi}_{MLE} = \\frac{Y}{n}\\)\nBR Exercise 3.1\nFor each part, use plot_beta and/or summarize_beta to justify your answer\nBR Exercise 3.12\nChoice of prior\nI am interviewing Carleton students about whether or not they have used (knowingly) used ChatGPT on coursework in a non-approved way. I think the proportion has a 90% chance of being less than .25.\n\nChoose an informative prior that you think is reasonable for this belief\n\nI then ask 20 students this question and 15 respond “yes”. Find the posterior using the prior from above, then using the 3 non/weakly informative priors below:\n\nUnif(0,1) prior\nBeta(2,2) prior\n“Reference” prior Beta(.5, .5)\n\nCompare the posteriors for each of the priors above. Do results change if we instead observe 150/200 students responding “yes”?\nBR 4.13\n\n\n\n\n\n\nNote\n\n\n\nFor part (a), you do not need to include your sketch in your submission!",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 02"
    ]
  },
  {
    "objectID": "groupwork/gw01.html",
    "href": "groupwork/gw01.html",
    "title": "Group Work 01",
    "section": "",
    "text": "Syllabus Q\nWhat is the weirdest/most unique policy or course component that you saw in the syllabus?\n\n\nBR Exercise 2.3\n\n\nBR Exercise 2.5\n\n\nBR Exercise 2.13\n\n\n\n\n\n\nImportant\n\n\n\nThis problem moved to HW2!",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 01"
    ]
  },
  {
    "objectID": "groupwork/gw05.html",
    "href": "groupwork/gw05.html",
    "title": "Group Work 03",
    "section": "",
    "text": "library(bayesrules) # R package for our textbook\nlibrary(tidyverse) # Collection of packages for tidying and plotting data\nlibrary(janitor) # Helper functions like tidy and tabyl\nlibrary(rstan)\nlibrary(broom)\n\n\n\n\n\n\n\nNote\n\n\n\nThe Monday problems are a selection from BR Exercises 8.15-8.17, packaged in a different format\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to remove #| eval: false from any code chunks if you copy the whole document using the “code” button\n\n\nAn exact analysis\nThis problem uses the pulse_of_the_nation data from slides13\n\ndata(\"pulse_of_the_nation\")\npulse_of_the_nation %&gt;%\n  count(climate_change)\n\n# A tibble: 3 × 2\n  climate_change                    n\n  &lt;fct&gt;                         &lt;int&gt;\n1 Not Real At All                 150\n2 Real and Caused by People       655\n3 Real but not Caused by People   195\n\n\n\nBayesians think of the entire Beta(151,852) posterior pdf as an estimate for \\(\\pi\\). But for communication purposes, it can be useful to report what values of \\(\\pi\\) are typical. Identify and calculate two possible posterior point estimates.\nThe posterior estimates above merely capture the typical posterior \\(\\pi\\) value, thus miss the bigger picture. It’s important to supplement these estimates with a posterior credible interval. (Bayesians use “credible” instead of “confidence”) Calculate a 95% posterior credible interval for \\(\\pi\\). Revisiting a plot of the posterior might spark some ideas, and you’ll need some R code of the rXXX, qXXX, dXXX, pXXX variety.\nHow can we interpret this interval, (a,b)?\nWhat would a strict frequentist say if you asked them “What’s the probability that \\(\\pi\\) lies within the Bayesian credible interval?”\nA researcher claims that more than 13% of people don’t believe in climate change. Using your interval from (b), what do you think about this claim?\nCalculate and interpret a posterior probability that helps you test this claim. You’ll need some R code of the rXXX, qXXX, dXXX, pXXX variety.\nThere’s no common cut-off / threshold (eg: 0.05) for interpreting Bayesian posterior probabilities, hence no binary conclusion. Better yet, Bayesian conclusions are more holistic and nuanced. With this in mind, summarize your conclusions about our hypothesis.\nBayes Factor\n\n\n\n\n\n\nNotePosterior Odds\n\n\n\nThe posterior odds for a hypothesis test \\(H_0\\) against \\(H_a\\) after observing data \\(Y=y\\) is\n\\[\\text{posterior odds} = \\frac{P(H_a | Y = y)}{P(H_0 | Y = y)}\\]\n\n\n\n\n\n\n\n\nNotePosterior Odds\n\n\n\nThe prior odds for a hypothesis test \\(H_0\\) against \\(H_a\\) is\n\\[\\text{posterior odds} = \\frac{P(H_a)}{P(H_0)}\\]\n\n\n\n\n\n\n\n\nNoteBayes Factor\n\n\n\nIn a hypothesis test of two competing hypotheses, \\(H_a\\) vs \\(H_0\\), the Bayes Factor is an odds ratio for \\(H_a\\):\n\\[\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P(H_a |Y) / P(H_0 | Y)}{P(H_a)/P(H_0)}\\]\n\n\nCalculate the prior odds, posterior odds, and Bayes Factor for the researcher’s claim that “more than 13% of people don’t believe in climate change”. What do these numbers tell you?\nBR Exercise 8.21\nNow, let’s explore how we can do estimation and hypothesis testing with an approximate posterior sample from MCMC.\n\nLoad the following packages, and run the following R/stan code to fit an MCMC approximation for the same problem. Make sure you understand all of the pieces of the model fitting code.\n\n\nlibrary(rstan)\nlibrary(bayesrules)\nlibrary(bayesplot)\nlibrary(broom)\n\n\n# Define the Beta-Binomial model in rstan notation\nclimate_model &lt;- \"\n  data {\n    real&lt;lower=0&gt; alpha;\n    real&lt;lower=0&gt; beta;\n    int&lt;lower=1&gt; n;\n    int&lt;lower=0, upper=n&gt; Y;\n  }\n\n  parameters {\n    real&lt;lower=0, upper=1&gt; pi;\n  }\n\n  model {\n    Y ~ binomial(n, pi);\n    pi ~ beta(alpha, beta);\n  }\n\"\n\n# Set the random number seed\nset.seed(84735)\n\n# SIMULATE the posterior\nclimate_sim &lt;- stan(\n  model_code = climate_model, \n  data = list(alpha = 1, beta = 2, Y = 150, n = 1000),\n  chains = 4, iter = 5000*2)\n\n\nGive the following plots a quick peak. Do you see any red flags?\n\n\nmcmc_trace(climate_sim, pars = \"pi\")\nmcmc_dens_overlay(climate_sim, pars = \"pi\")\nmcmc_dens(climate_sim, pars = \"pi\")\n\n\nThe four chains in climate_sim are currently stored as an array. Use the code below to store all the chains in a single data frame.\n\n\n# Store the array of 4 chains in 1 data frame\nclimate_chains &lt;- as.data.frame(\n  climate_sim,\n  pars = \"lp__\", include = FALSE)\n\n# Check out the results\ndim(climate_chains)\nhead(climate_chains)\n\n\nRecall your exact posterior point estimates of \\(\\pi\\) from earlier. Estimate these posterior features using your MCMC simulation. (How accurate are these estimates?) NOTE: The sample_mode() function in {bayesrules} calculates the mode of a sample.\n\n\nclimate_chains %&gt;%\n  summarize(___)\n\n\nRecall your exact 95% posterior credible interval for \\(\\pi\\) from earlier. Estimate this interval using your MCMC simulation. (How accurate is this estimate?)\nRecall your exact analysis of the claim that more than 13% of people don’t believe in climate change, i.e. \\(\\pi &gt; 0.13\\). Estimate the posterior probability of this claim using your MCMC simulation. (How accurate is this estimate?)\nPlay around with the following shortcut functions that can address some, but not all, of our posterior questions. These are applied directly to climate_sim, not climate_chains. Take notes what they do (leaving comments in your .qmd would be sufficient!)\n\n\n# What is the estimate? The posterior mean, median, or mode?\ntidy(climate_sim, conf.int = TRUE, conf.level = 0.95)\n\n# \nmcmc_areas(climate_sim, pars = \"pi\", prob = 0.95)\n\nTBA Wed\nTBA Wed",
    "crumbs": [
      "Assignments",
      "Groupwork",
      "Group Work 03"
    ]
  }
]