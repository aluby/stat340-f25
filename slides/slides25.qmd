---
title: Hierarchical Models are Exciting!
subtitle: Day 25
title-slide-attributes:
  data-background-color: "#e2583eff"
  data-slide-number: none
format: 
  revealjs:
    incremental: false
    scrollable: false
auto-stretch: true
editor_options: 
  chunk_output_type: inline
editor: 
  markdown: 
    wrap: 72
execute: 
  warning: false
  message: false
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(countdown)
library(tidyverse)
library(tidybayes)
library(bayesrules)
library(janitor)
library(patchwork)
library(rstan)
library(rstanarm)
library(bayesplot)
library(broom.mixed)
library(ggrepel)

theme_set(theme_minimal(base_size = 16, base_family = "Atkinson Hyperlegible"))
clrs <- viridis::viridis(6, end = .75, option = "plasma")
```

## Plan for today: 

1. "big idea" of hierarchical models
  - Motivating example: Cherry Blossom marathon times
  
2. Hierarchical linear models with no predictors
  - Motivating example: Spotify artists
  
## How do `net` running times change with age?

```{r}
data(cherry_blossom_sample)

ggplot(cherry_blossom_sample, aes(x = age, y = net, col = runner)) + 
  geom_point() + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## Problem: multiple data points for each runner

```{r}
ggplot(cherry_blossom_sample, aes(y = net, fill = runner, x = age)) + 
  geom_boxplot(alpha = .7) + 
  theme(legend.position = "none") + 
  scale_fill_viridis_d(end = .75, option = "plasma")
```

## "Complete pooling"

Put all observations across all groups into a single "pool" of data: 

```{r}
#| echo: false
ggplot(cherry_blossom_sample, aes(x = age, y = net)) + 
  geom_point() + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "Complete pooling"

$$Y_i | \beta_0, \beta_1, \sigma \sim N(\mu_i, \sigma^2) \text{ with } \mu_i = \beta_0 + \beta_1X_i$$

```{r}
complete_pooled_model <- stan_glm(
  net ~ age, 
  data = cherry_blossom_sample, family = gaussian, 
  prior_intercept = normal(0, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, refresh = 0, seed = 84735)

tidy(complete_pooled_model, conf.int = TRUE, conf.level = 0.80)
```

## 

```{r}
ggplot(cherry_blossom_sample, aes(x = age, y = net)) + 
  geom_point() + 
  geom_abline(slope = .268, intercept = 75.2, color = "darkorange") + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "No pooling"

Treat each group as a separate dataset and build a model for each

```{r}
#| echo: false
cherry_blossom_sample |>
  filter(runner %in% 25:36) |> 
  ggplot(aes(x = age, y = net, color = runner)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~runner) + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "No pooling"

Let $j$ denote each runner and $i$ denote each race. 

$$Y_{ij} | \beta_{0j}, \beta_{1j}, \sigma \sim N(\mu_{ij}, \sigma^2) \text{ with } \mu_{ij} = \beta_{0j} + \beta_{1j}X_{ij}$$

##

Based on this model, what do you anticipate that your running time will be at the age of 55?

```{r}
#| echo: false
cherry_blossom_sample |>
  filter(runner %in% c(1, 20, 22)) |> 
  ggplot(aes(x = age, y = net, color = runner)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~runner) + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## 

### Complete Pooling

![](img/complete_pool_diagram.png)

### No pooling

![](img/no_pool_diagram.png)

## 

::::: columns
::: {.column width="50%"}
### Complete Pooling

- Violates independence assumption
- Doesn't capture variability within groups
- Might produce misleading conclusions about the relationship and the significance
:::
::: {.column width="50%"}
### No pooling

- Can't reliably generalize to groups outside of our sample
- Assumes one group contains no relevant information about any other group
- Number of observations per group really matters
:::
:::::

## Partial Pooling

Though each group is unique, having been sampled from the same population, all groups are connected and thus might contain valuable information about one another 

![](img/partial_pool_diagram.png)


## 

Based on this model, what do you anticipate that your running time will be at the age of 55?


![](img/runners-ch-15-1.png)

# Hierarchical normal model with no predictors {.colorslide}

## What is the average spotify song popularity? 

### If a new song is posted, what is our "best guess" for its popularity?

```{r}
#| echo: false


data(spotify)

spotify <- spotify %>% 
  select(artist, title, popularity) %>% 
  mutate(artist = fct_reorder(artist, popularity, .fun = 'mean'))

ggplot(spotify, aes(x = popularity)) + 
  geom_density(bw = 2.5)
```

##

```{r}
spotify
```

## Data is grouped by artist

```{r}
#| fig-width: 4
#| fig-height: 8
#| echo: false
#| message: false
#| warning: false


library(ggridges)
ggplot(spotify, aes(x = popularity, y = artist, fill = artist)) + 
  geom_density_ridges() + 
  scale_fill_viridis_d(end = .75, option = "plasma") + 
  theme(legend.position = "none")
```


## 

### Complete pooling 

_Ignore_ artists and lump all songs together 

### No pooling  

_Separately_ analyze each artist and assume that one artist's data doesn't contain valuable information about another artist 
    
### Partial pooling (via Hierarchical structure)

Acknowledge the grouping structure, so that even though artists differ in popularity, they might share valuable information about each other _and_ about the broader population of artists.


## The hierarchy

Let $i$ index song, $j$ index artist

Layer 1: Model of how song popularity varies WITHIN artist $j$

$Y_{ij} | \mu_j, \sigma_y \sim N(\mu_j, \sigma_y^2)$


Layer 2: model of how the typical popularity varies BETWEEN artists $j$

$\mu_j | \mu, \sigma_\mu \sim N(\mu, \sigma_\mu^2)$


Layer 3: prior models for shared global parameters

$\mu, \sigma_y, \sigma_\mu$

##

__Notation alert__

- There's a difference between $\mu_j$ and $\mu$. When a parameter has a subscript $j$, it refers to a feature of group $j$. When a parameter _doesn't_ have a subscript $j$, it's the _global_ counterpart, i.e., the same feature across all groups.

- Subscripts signal the group or layer of interest. For example, $\sigma_y$ refers to the standard deviation of $Y$ values within each group, whereas $\sigma_\mu$ refers to the standard deviation of means $\mu_j$ from group to group.


## The hierarchy (another way to think about it)

Let $i$ index song, $j$ index artist

Layer 1: Model of how song popularity varies WITHIN artist $j$, as an offset from the pooled mean

$Y_{ij} | \mu_j, \sigma_y \sim N(\mu_j, \sigma_y^2)$ with $\mu_j = \mu + b_j$


Layer 2: model of how the typical popularity varies BETWEEN artists $j$

$b_j | \sigma_\mu \sim N(0, \sigma_\mu^2)$


Layer 3: prior models for shared global parameters

$\mu, \sigma_y, \sigma_\mu$

## Hierarchical model syntax in R/{rstanarm}/{lme4}

- `glmer` instead of `glm` (comes from generalized linear mixed-effects model)
- grouping structure goes within parentheses

Example: `y ~ (1 | group)`

Example: `y ~ (1 + x| group)`

## {rstanarm} spotify model

```{r cache=TRUE}
spotify_hierarchical <- stan_glmer(
  popularity ~ (1 | artist), 
  data = spotify, family = gaussian,
  prior_intercept = normal(50, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 5000*2, seed = 84735, refresh=FALSE)

```


- The prior for $\sigma_\mu$ is specified by `prior_covariance`. For this particular model, with only one set of artist-specific parameters $\mu_j$, this is equivalent to an Exp(1) prior. (We will learn more about `prior_covariance` next class).


## `pp_check`

```{r}
pp_check(spotify_hierarchical) + 
  xlab("popularity")

```

##

```{r}
# Store the simulation in a data frame
spotify_hierarchical_df <- as.data.frame(spotify_hierarchical)

# Check out the first 3 and last 3 parameter labels
spotify_hierarchical_df %>% 
  colnames() %>% 
  as.data.frame() %>% 
  slice(1:3, 45:47)
```


## "Fixed" effects

```{r}
tidy(spotify_hierarchical, effects = "fixed", 
     conf.int = TRUE, conf.level = 0.80)
```

```{r echo = FALSE}
post_sum <- tidy(spotify_hierarchical, effects = "fixed", 
                 conf.int = TRUE, conf.level = 0.80)
post_sum_sig <- tidy(spotify_hierarchical, effects = "ran_pars",
                     conf.int = TRUE, conf.level = 0.80)
```

`effects = fixed` means with "non-varying" or "global."

There's an 80% chance that the _average_ artist has a mean popularity rating between `r round(post_sum[1,4],1)` and `r round(post_sum[1,5],1)`.

## "Random" effects

To call up the posterior medians for $\sigma_y$ and $\sigma_\mu$, we can specify `effects = "ran_pars"`, i.e., `par`ameters related to `ran`domness or variability:

```{r}
tidy(spotify_hierarchical, effects = "ran_pars")
```

## Posterior analysis of group-specific parameters {.smaller}

$$\mu_j = \mu + b_j $$

Here, $b_j$ describes the _difference_ between artist $j$'s mean popularity and the global mean popularity.

```{r}
artist_summary <- tidy(spotify_hierarchical, effects = "ran_vals", 
                       conf.int = TRUE, conf.level = 0.80)
# Check out the results for the first & last 2 artists
artist_summary %>% 
  select(level, conf.low, conf.high) %>% 
  slice(1:2, 43:44)
```

## Getting artist means {.smaller}

```{r warning=FALSE}
# Get MCMC chains for each mu_j
artist_chains <- spotify_hierarchical %>%
  tidybayes::spread_draws(`(Intercept)`, b[,artist]) %>% 
  mutate(mu_j = `(Intercept)` + b) 
# Check it out
artist_chains %>% 
  head(4)
```

## Posterior summaries for $\mu_j$

```{r}
artist_summary_scaled <- artist_chains %>% 
  select(-`(Intercept)`, -b) %>% 
  tidybayes::mean_qi(.width = 0.80) %>% 
  mutate(artist = fct_reorder(artist, mu_j))

artist_summary_scaled %>% 
  select(artist, mu_j, .lower, .upper) %>% 
  head(4)
```

## Complete vs no vs partial pooling

```{r}
#| echo: false
#| cache: true


cp_spotify <-  stan_glm(
  popularity ~ 1, 
  data = spotify, family = gaussian, 
  prior_intercept = normal(50, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, refresh = 0, seed = 84735)

cp_spotify_ests = tidy(cp_spotify, conf.int = TRUE, conf.level = .8)

np_spotify <-  stan_glm(
  popularity ~ 0 + artist, 
  data = spotify, family = gaussian, 
  prior_intercept = normal(50, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, refresh = 0, seed = 84735)

np_spotify_ests = tidy(np_spotify, conf.int = TRUE, conf.level = .8) |>
  mutate(
    artist = str_replace(term, "artist", ""),
    artist = str_replace_all(artist, " ", "_")
  ) |>
  rename(np_est = estimate, 
         np_low = conf.low,
         np_high = conf.high) |>
  select(artist, np_est, np_low, np_high)
```

```{r}
#| echo: false


artist_summary_scaled |>
  mutate(
    artist = str_replace(artist, "artist:", ""),
    artist = fct_reorder(artist, mu_j),
    cp_est = cp_spotify_ests$estimate,
    cp_low = cp_spotify_ests$conf.low,
    cp_high = cp_spotify_ests$conf.high
  ) |>
  rename(
    pp_est = mu_j,
    pp_low = .lower,
    pp_high = .upper
  ) |>
  left_join(np_spotify_ests, by = c("artist")) |>
  select(-c(.width, .point, .interval)) |>
  pivot_longer(contains("pp") | contains("np") | contains("cp")) |>
  separate(name, into = c("model", "quantity")) |>
  pivot_wider(names_from = quantity, values_from = value) |>
  ggplot(aes(x = artist, y = est, ymin = low, ymax = high, col = model)) +
  geom_pointrange(position = position_dodge(width = 1)) +
  xaxis_text(angle = 45, hjust = 1) + 
  scale_fill_viridis_d(end = .75, option = "plasma") +
  labs(x = "", y = expression(mu[j]), col = "model")
```

## Posterior prediction {}

::::: columns
::: {.column width="50%"}
### If $j$ is observed group

Reflects two sources of variability: 

  - within-group sampling variability in $Y$
  - posterior variability in model parameters
    
$Y_{new, j}^{(i)} | \mu_j, \sigma_y \sim N(\mu_j^{(i)}, (\sigma_y^{(i)})^2)$

:::

::: {.column width="50%" }
### If $j'$ is **un**observed group 

Must reflect three sources of variability: 

  - within-group sampling variability in $Y$
  - posterior variability in model parameters
  - Level 2: variability in $\mu_j$ around $\mu$
  
$\mu_{j'} | \mu, \sigma_\mu \sim N(\mu^{(i)}, (\sigma_{\mu}^{(i)})^2)$
$Y_{new, j'} | \mu_{j'}, \sigma_y \sim N(\mu_{j'}^{(i)}, (\sigma_{y}^{(i)})^2)$
:::
::::


## 

```{r}
set.seed(84735)
prediction_shortcut <- posterior_predict(
  spotify_hierarchical,
  newdata = data.frame(artist = c("Frank Ocean", "One Armed Villain")))

mcmc_areas(prediction_shortcut, prob = 0.8) +
  ggplot2::scale_y_discrete(labels = c("Frank Ocean", "One Armed Villain"))
```