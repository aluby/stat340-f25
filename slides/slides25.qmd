---
title: Hierarchical Models are Exciting!
subtitle: Day 25
title-slide-attributes:
  data-background-color: "#e2583eff"
  data-slide-number: none
format: 
  revealjs:
    incremental: false
    scrollable: false
auto-stretch: true
editor_options: 
  chunk_output_type: inline
editor: 
  markdown: 
    wrap: 72
execute: 
  warning: false
  message: false
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(countdown)
library(tidyverse)
library(tidybayes)
library(bayesrules)
library(janitor)
library(patchwork)
library(rstan)
library(rstanarm)
library(bayesplot)
library(broom.mixed)
library(ggrepel)

theme_set(theme_minimal(base_size = 16, base_family = "Atkinson Hyperlegible"))
clrs <- viridis::viridis(6, end = .75, option = "plasma")
```

## Plan for today: 

1. "big idea" of hierarchical models
  - Motivating example: Cherry Blossom marathon times
  
2. Hierarchical linear models with no predictors
  - Motivating example: Spotify artists
  
## How do `net` running times change with age?

```{r}
data(cherry_blossom_sample)

ggplot(cherry_blossom_sample, aes(x = age, y = net, col = runner)) + 
  geom_point() + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## Problem: multiple data points for each runner

```{r}
ggplot(cherry_blossom_sample, aes(y = net, fill = runner, x = age)) + 
  geom_boxplot(alpha = .7) + 
  theme(legend.position = "none") + 
  scale_fill_viridis_d(end = .75, option = "plasma")
```

## "Complete pooling"

Put all observations across all groups into a single "pool" of data: 

```{r}
#| echo: false
ggplot(cherry_blossom_sample, aes(x = age, y = net)) + 
  geom_point() + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "Complete pooling"

$$Y_i | \beta_0, \beta_1, \sigma \sim N(\mu_i, \sigma^2) \text{ with } \mu_i = \beta_0 + \beta_1X_i$$

```{r}
complete_pooled_model <- stan_glm(
  net ~ age, 
  data = cherry_blossom_sample, family = gaussian, 
  prior_intercept = normal(0, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, refresh = 0, seed = 84735)

tidy(complete_pooled_model, conf.int = TRUE, conf.level = 0.80)
```

## 

```{r}
ggplot(cherry_blossom_sample, aes(x = age, y = net)) + 
  geom_point() + 
  geom_abline(slope = .268, intercept = 75.2, color = "darkorange") + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "No pooling"

Treat each group as a separate dataset and build a model for each

```{r}
#| echo: false
cherry_blossom_sample |>
  filter(runner %in% 25:36) |> 
  ggplot(aes(x = age, y = net, color = runner)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~runner) + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## "No pooling"

Let $j$ denote each runner and $i$ denote each race. 

$$Y_{ij} | \beta_{0j}, \beta_{1j}, \sigma \sim N(\mu_{ij}, \sigma^2) \text{ with } \mu_{ij} = \beta_{0j} + \beta_{1j}X_{ij}$$

##

Based on this model, what do you anticipate that your running time will be at the age of 55?

```{r}
#| echo: false
cherry_blossom_sample |>
  filter(runner %in% c(1, 20, 22)) |> 
  ggplot(aes(x = age, y = net, color = runner)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~runner) + 
  theme(legend.position = "none") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## 

### Complete Pooling

![](img/complete_pool_diagram.png)

### No pooling

![](img/no_pool_diagram.png)

## 

::::: columns
::: {.column width="50%"}
### Complete Pooling

- Violates independence assumption
- Doesn't capture variability within groups
- Might produce misleading conclusions about the relationship and the significance
:::
::: {.column width="50%"}
### No pooling

- Can't reliably generalize to groups outside of our sample
- Assumes one group contains no relevant information about any other group
- Number of observations per group really matters
:::
:::::

## Partial Pooling

Though each group is unique, having been sampled from the same population, all groups are connected and thus might contain valuable information about one another 

![](img/partial_pool_diagram.png)


## 

Based on this model, what do you anticipate that your running time will be at the age of 55?


![](img/runners-ch-15-1.png)

# Hierarchical normal model with no predictors {.colorslide}

## What is the average spotify song popularity? 

```{r}
#| echo: false


data(spotify)

spotify <- spotify %>% 
  select(artist, title, popularity) %>% 
  mutate(artist = fct_reorder(artist, popularity, .fun = 'mean'))

ggplot(spotify, aes(x = popularity)) + 
  geom_density(bw = 2.5)
```


## Data is grouped by artist

```{r}
#| fig-width: 4
#| fig-height: 8
#| echo: false


library(ggridges)
ggplot(spotify, aes(x = popularity, y = artist, fill = artist)) + 
  geom_density_ridges() + 
  scale_fill_viridis_d(end = .75, option = "plasma") + 
  theme(legend.position = "none")
```


## 

__Complete pooling__    

_Ignore_ artists and lump all songs together 

__No pooling__    

_Separately_ analyze each artist and assume that one artist's data doesn't contain valuable information about another artist 
    
__Partial pooling (via hierarchical models)__    

Acknowledge the grouping structure, so that even though artists differ in popularity, they might share valuable information about each other _and_ about the broader population of artists.


## The hierarchy

Layer 1:

$Y_{ij} | \mu_j, \sigma_y   \hspace{-0.075in} \sim \text{model of how song popularity varies WITHIN artist } j$


Layer 2:

$\mu_j | \mu, \sigma_\mu  \hspace{-0.075in} \sim \text{model of how the typical popularity} \mu_j \text{varies BETWEEN artists}$


Layer 3:

$\mu, \sigma_y, \sigma_\mu   \hspace{-0.075in} \sim \text{prior models for shared global parameters}$

##

- $\mu_j$ = mean song popularity for artist $j$; and
- $\sigma_y$ = __within-group variability__, i.e., the standard deviation in popularity from song to song within each artist.


##

Popularity varies from artist to artist.
We model this variability in mean popularity __between__ artists by assuming that the individual mean popularity levels, $\mu_j$, are _Normally_ distributed around $\mu$ with standard deviation $\sigma_\mu$

$$\mu_j | \mu, \sigma_\mu \sim N(\mu, \sigma^2_\mu)  .$$


- $\mu$ = the __global average__ of mean song popularity $\mu_j$ across all artists $j$, i.e., the mean popularity rating for the most average artist; and
- $\sigma_\mu$ = __between-group variability__, i.e., the standard deviation in mean popularity $\mu_j$ from artist to artist.

##

__Notation alert__

- There's a difference between $\mu_j$ and $\mu$. When a parameter has a subscript $j$, it refers to a feature of group $j$. When a parameter _doesn't_ have a subscript $j$, it's the _global_ counterpart, i.e., the same feature across all groups.

- Subscripts signal the group or layer of interest. For example, $\sigma_y$ refers to the standard deviation of $Y$ values within each group, whereas $\sigma_\mu$ refers to the standard deviation of means $\mu_j$ from group to group.


##

```{r cache=TRUE}
spotify_hierarchical <- stan_glmer(
  popularity ~ (1 | artist), 
  data = spotify, family = gaussian,
  prior_intercept = normal(50, 2.5, autoscale = TRUE),
  prior_aux = exponential(1, autoscale = TRUE),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 5000*2, seed = 84735, refresh=FALSE)

```

- To indicate that the `artist` variable defines the group structure of our data, as opposed to it being a predictor of `popularity`, the appropriate formula here is `popularity ~ (1 | artist)`.
- The prior for $\sigma_\mu$ is specified by `prior_covariance`. For this particular model, with only one set of artist-specific parameters $\mu_j$, this is equivalent to an Exp(1) prior. (We will learn more about `prior_covariance` next class).


##

```{r fig.height=5}
pp_check(spotify_hierarchical) + 
  xlab("popularity")

```

##

```{r}
# Store the simulation in a data frame
spotify_hierarchical_df <- as.data.frame(spotify_hierarchical)

# Check out the first 3 and last 3 parameter labels
spotify_hierarchical_df %>% 
  colnames() %>% 
  as.data.frame() %>% 
  slice(1:3, 45:47)
```

## Posterior Analysis of Global Parameters

- $\mu$ = `(Intercept)`
- $\sigma_y$ = `sigma`
- $\sigma_\mu^2$ = `Sigma[artist:(Intercept),(Intercept)]`.This is not a typo. The default output gives us information about the _standard deviation_ within artists ( $\sigma_y$ ) but the _variance_ between artists ( $\sigma_\mu^2$ ).

##

```{r}
tidy(spotify_hierarchical, effects = "fixed", 
     conf.int = TRUE, conf.level = 0.80)
```

```{r echo = FALSE}
post_sum <- tidy(spotify_hierarchical, effects = "fixed", 
                 conf.int = TRUE, conf.level = 0.80)
post_sum_sig <- tidy(spotify_hierarchical, effects = "ran_pars",
                     conf.int = TRUE, conf.level = 0.80)
```

Pay attention to `effects = fixed`, where "fixed" is synonymous with "non-varying" or "global."

Per the results, there's an 80% chance that the _average_ artist has a mean popularity rating between `r round(post_sum[1,4],1)` and `r round(post_sum[1,5],1)`.

##

To call up the posterior medians for $\sigma_y$ and $\sigma_\mu$, we can specify `effects = "ran_pars"`, i.e., `par`ameters related to `ran`domness or variability:

```{r}
tidy(spotify_hierarchical, effects = "ran_pars")
```

##

proportion of $\text{Var}(Y_{ij})$ that can be explained by differences in the observations within each group:

$$\frac{\sigma^2_y}{\sigma^2_\mu + \sigma^2_y}$$ 

<hr>

proportion of $\text{Var}(Y_{ij})$that can be explained by differences between groups

$$\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2_y}$$

##

These two sources of variability suggest that the popularity levels among multiple songs _by the same artist_ tend to have a moderate correlation near 0.54.

```{r}
15.1^2 / (15.1^2 + 14.0^2)
```

```{r echo = FALSE}
ratio <- round(round(post_sum_sig[1,3],1)^2 / (round(post_sum_sig[1,3],1)^2 + round(post_sum_sig[2,3],1)^2)*100)
```

Thinking of this another way, `r ratio`% of the variability in song popularity is explained by differences between artists, whereas `r 100-ratio`% is explained by differences among the songs within each artist:

```{r}
14.0^2 / (15.1^2 + 14.0^2)
```

## Posterior analysis of group-specific parameters

$$\mu_j = \mu + b_j $$

Here, $b_j$ describes the _difference_ between artist $j$'s mean popularity and the global mean popularity.

##

```{r}
artist_summary <- tidy(spotify_hierarchical, effects = "ran_vals", 
                       conf.int = TRUE, conf.level = 0.80)
# Check out the results for the first & last 2 artists
artist_summary %>% 
  select(level, conf.low, conf.high) %>% 
  slice(1:2, 43:44)
```