---
title: Classification 
subtitle: Day 22
title-slide-attributes:
  data-background-color: "#e2583eff"
  data-slide-number: none
format: 
  revealjs:
    incremental: false
    scrollable: false
auto-stretch: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
execute: 
  warning: false
  message: false
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(countdown)
library(tidyverse)
library(tidybayes)
library(bayesrules)
library(janitor)
library(patchwork)
library(rstan)
library(rstanarm)
library(bayesplot)
library(broom.mixed)
library(ggrepel)
library(ggridges)

theme_set(theme_minimal(base_size = 16, base_family = "Atkinson Hyperlegible"))
```

## Example: was this answer generated by ChatGPT?

```{r}
#| echo: false
#| layout-ncol: 2

set.seed(11025)
n_samples_each = 100
ai_emdash = .64
human_emdash = .08
ai_meanlength = 125
human_meanlength = 85
ai_sdlength = 25
human_sdlength = 40

ai_generated <- tibble(
  ai_labels = rep(c(0,1), each = n_samples_each)
) |>
  mutate(
    prob = ifelse(ai_labels, ai_emdash, human_emdash),
    length_mu = ifelse(ai_labels, ai_meanlength, human_meanlength),
    length_sd = ifelse(ai_labels, ai_sdlength, ai_sdlength),
    emdash = rbinom(n(), size = 1, prob = prob),
    length = rnorm(n(), mean = length_mu, sd = length_sd)
  ) 

ai_generated |>
  select(ai_labels, emdash, length) |> sample_n(6)

p1 <- ggplot(ai_generated, aes(x = length, fill = factor(ai_labels), y = factor(ai_labels))) + 
  geom_density_ridges() + 
  labs(y = "ai_labels") + 
  theme(legend.position = "none") + 
  scale_fill_viridis_d(end = .75, option = "plasma")

p2 <- ggplot(ai_generated, aes(x = factor(ai_labels), fill = factor(emdash))) + 
  geom_bar() + 
  labs(x = "ai_labels", fill = "emdash") + 
  scale_fill_viridis_d(end = .75, option = "plasma")

p1/p2
```

::: callout-warning
## Fake data alert! 
:::

## Fit the logistic regression. model

```{r}
ai_detector <- stan_glm(ai_labels ~ emdash + length,
                        data = ai_generated, 
                        family = binomial,
                        prior_intercept = normal(-2.5, 0.5),
                        chains = 4, iter = 5000*2, 
                        seed = 84735, refresh =0)
```


## Check convergence

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 2

mcmc_trace(ai_detector)
mcmc_dens_overlay(ai_detector)
```

## Posterior predictive check

```{r}
proportion_ai <- function(x){mean(x==1)}

pp_check(ai_detector, plotfun = "stat", stat = "proportion_ai")
```

## Posterior prediction by hand

```{r}
ai_pred <- posterior_predict(ai_detector, newdata = ai_generated)
dim(ai_pred)
ai_pred[1:10, 1:10]
```

## 

```{r}
ai_classifications <- ai_generated |>
  select(-c(prob, length_mu, length_sd)) |>
  mutate(
    ai_prob = colMeans(ai_pred),
    ai_class = as.numeric(ai_prob >= 0.5) 
  ) 

head(ai_classifications)
```

## Confusion Matrix

```{r}
ai_classifications |>
  tabyl(ai_labels, ai_class) |>
  adorn_totals(c("row", "col"))
```

## Classification Accuracy Metrics

::: callout-note
## Classification Accuracy

Proportion of all $Y$ observations that are accurately classified
:::

::: callout-note
## Sensitivity (true positive rate)

Proportion of all $Y=1$ observations that are accurately classified
:::

::: callout-note
## Specificity (true negative rate)

Proportion of all $Y=0$ observations that are accurately classified
:::

## Example: different classification rules

::::: columns
::: {.column width="50%"}

```{r}
ai_generated |>
  mutate(
    ai_prob = colMeans(ai_pred),
    ai_class = as.numeric(ai_prob >= 0.9) 
  ) |>
  tabyl(ai_labels, ai_class) |>
  adorn_totals(c("row", "col"))
```
:::

::: {.column width="50%"}

```{r}
ai_generated |>
  mutate(
    ai_prob = colMeans(ai_pred),
    ai_class = as.numeric(ai_prob >= 0.1) 
  ) |>
  tabyl(ai_labels, ai_class) |>
  adorn_totals(c("row", "col"))
```

:::
::::


# Naive Bayes {.colorslide}

## Goal: classify penguin species on Palmer archipelago

$$Y = \begin{cases} \text{Adelie} \\ \text{Chinstrap} \\ \text{Gentoo} \end{cases}$$

using 

$$X_1 = \begin{cases} 1 & \text{if above avg weight (>4200g)} \\ 0 & \text{otherwise} \end{cases}$$

## *Prior* to observing any characteristics:

```{r}
penguins_bayes |>
  tabyl(species)
```

We'll use this as our **prior probability model**

## Classifying `species` based on `above_average_weight`

```{r}
penguins_bayes |> 
  drop_na(above_average_weight) |>
  ggplot(aes(x = species, fill = above_average_weight)) + 
  geom_bar(position = "fill") 
```

## 

```{r}
penguins_bayes |> 
  drop_na(above_average_weight) |>
  tabyl(species, above_average_weight) |>
  adorn_percentages() |> 
  adorn_ns()
```

::: callout-tip
## Practice

If we come across a below average weight penguin ($X_1 = 0$), what is the likelihood of each species?
:::

## 

::: callout-tip
## Practice

Verify the following posterior model for $Y$ given $X_1 = 0$

| Y          | A     | C     | G     | Total |
|------------|-------|-------|-------|-------|
| P(Y\|X1=0) | 0.654 | 0.315 | 0.031 | 1     |

:::


## What if predictor is quantitative?

::: callout-tip
## Practice

If we observe a penguin with a 50mm bill: 

- Which species is most likely? 
- How might we calculate $L(Y | X_2 = 50)$?

:::


```{r}
#| echo: false


ggplot(penguins_bayes, aes(x = bill_length_mm, fill = species)) +
  geom_density(alpha = 0.7) +
  geom_vline(xintercept = 50, linetype = "dashed") + 
  scale_fill_viridis_d(end = .75, option = "plasma")
```

## "Naive" Bayes assumes quantitative predictors are continuous and conditionally Normal: {.smaller}

$$X_2 | (Y=A) \sim N(\mu_A, \sigma_A^2)$$

$$X_2 | (Y=C) \sim N(\mu_C, \sigma_C^2)$$

$$X_2 | (Y=G) \sim N(\mu_G, \sigma_G^2)$$

```{r}
penguins_bayes |>
  group_by(species) |>
  summarize(mean = mean(bill_length_mm, na.rm = TRUE), 
            sd = sd(bill_length_mm, na.rm = TRUE))
```

## Assumed likelihoods

```{r}
#| echo: false


ggplot(penguins_bayes, aes(x = bill_length_mm, color = species)) + 
  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), 
                aes(color = "Adelie")) +
  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),
                aes(color = "Chinstrap")) +
  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),
                aes(color = "Gentoo")) + 
  geom_vline(xintercept = 50, linetype = "dashed") + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## $L(Y | X_2 = 50)$

```{r}
# L(Y = A | X_2 = 50)
dnorm(50, mean = 38.8, sd = 2.66)

# L(Y = C | X_2 = 50)
dnorm(50, mean = 48.8, sd = 3.34)

# L(Y = G | X_2 = 50)
dnorm(50, mean = 47.5, sd = 3.08)
```

## Prior $\times$ Likeilihood $\propto$ Posterior

Our prior understanding was that the penguin was most likely an Adelie. But the data (a 50mm bill length) is most consistent with Chinstraps. The posterior model balances these 2 conflicting pieces of evidence:

| Y           | A      | C      | G      | Total |
|-------------|--------|--------|--------|-------|
| P(Y)        |        |        |        |       |
| L(Y\|X2=50) |        |        |        |       |
| P(Y\|X2=50) | 0.0002 | 0.3972 | 0.6026 | 1     |

## Two predictors

```{r}
#| echo: false


ggplot(penguins_bayes, aes(x = bill_length_mm, y = flipper_length_mm, col = species)) + 
  geom_point() + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

::: callout-tip
## Check-in

If we only had `bill_length_mm`, which species would we have trouble distinguishing? What if we only had `flipper_length_mm`?
:::

## "Naive" Bayes assumes predictors are **conditionally independent**

$$f(x_2, x_3 | y) = f(x_2 | y) f(x_3 |y)$$

$$L(Y|X_2, X_3) =$$

## If we see a penguin with a 50mm long bill and a 195mm long flipper, what is the likelihood of each species? 

```{r}
penguins_bayes |>
  group_by(species) |>
  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), 
            sd = sd(flipper_length_mm, na.rm = TRUE))
```

## 

```{r}
# L(Y = A | X_2 = 50)L(Y = A | X_3 = 195)
dnorm(50, mean = 38.8, sd = 2.66) * dnorm(195, mean = 190, sd = 6.54)

# L(Y = C | X_2 = 50)L(Y = C | X_3 = 195)
dnorm(50, mean = 48.8, sd = 3.34) * dnorm(195, mean = 196, sd = 7.13)
 
# L(Y = G | X_2 = 50)L(Y = G | X_3 = 195)
dnorm(50, mean = 47.5, sd = 3.08) * dnorm(195, mean = 217, sd = 6.48)
```

## Balancing with the prior

| Y           | A      | C      | G      | Total |
|-------------|--------|--------|--------|-------|
| P(Y)        |        |        |        |       |
| L(Y\|X2=50, X3=195) |        |        |        |       |
| P(Y\|X2=50, X3=195) | 0.0003 | 0.9944 | 0.0052 | 1     |

## Shortcut: `naiveBayes()` in {e1071} {.smaller}

```{r}
library(e1071)
naive_model <- naiveBayes(species ~ bill_length_mm + flipper_length_mm, 
                          data = penguins_bayes)
naive_model
```

##

```{r}
predict(naive_model, 
        newdata = data.frame(bill_length_mm = 50, flipper_length_mm = 195), 
        type = "raw")

predict(naive_model, 
        newdata = data.frame(bill_length_mm = 50, flipper_length_mm = 195), 
        type = "class")
```

## In-sample confusion matrix

```{r}
penguins_bayes %>% # for some reason, only works with fancy pipe
  mutate(
    predicted = predict(naive_model, newdata = .)
  ) %>%
  tabyl(species, predicted) %>%
  adorn_percentages("row") %>% 
  adorn_pct_formatting(digits = 2) %>%
  adorn_ns()
```

## Cross-validation

```{r}
set.seed(84735)
naive_classification_summary_cv(
  model = naive_model, 
  data = penguins_bayes, 
  y = "species", 
  k = 10)$cv
```

## Classification Regions

```{r}
grid_data <- expand_grid(
    bill_length_mm = seq(30, 60, length = 100),
    flipper_length_mm = seq(170, 240, length = 100)) %>%
  mutate(classification = predict(naive_model, newdata = .))

head(grid_data)
```

## Classification Regions

```{r}
#| output-location: slide


ggplot(grid_data, aes(x = flipper_length_mm, 
                      y = bill_length_mm, 
                      color = classification)) +
  geom_point(alpha = 0.2) + 
  geom_point(data = penguins_bayes, aes(x = flipper_length_mm, 
                                        y = bill_length_mm, 
                                        color = species)) + 
  scale_color_viridis_d(end = .75, option = "plasma")
```

## Comparison to only 1 predictor

```{r}
naive_model_1 <- naiveBayes(species ~ bill_length_mm , 
                          data = penguins_bayes)
```

```{r}
#| echo: false
grid_data <- expand_grid(
    bill_length_mm = seq(30, 60, length = 100),
    flipper_length_mm = seq(170, 240, length = 100)) %>%
  mutate(classification = predict(naive_model_1, newdata = .))

ggplot(grid_data, aes(x = flipper_length_mm, 
                      y = bill_length_mm, 
                      color = classification)) +
  geom_point(alpha = 0.2) + 
  geom_point(data = penguins_bayes, aes(x = flipper_length_mm, 
                                        y = bill_length_mm, 
                                        color = species)) + 
  scale_color_viridis_d(end = .75, option = "plasma")
```
