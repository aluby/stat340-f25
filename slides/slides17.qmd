---
title: Evaluating Regression Models
subtitle: Day 17
title-slide-attributes:
  data-background-color: "#e2583eff"
  data-slide-number: none
format: 
  revealjs:
    incremental: false
    scrollable: false
auto-stretch: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
execute: 
  warning: false
  message: false
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(countdown)
library(tidyverse)
library(bayesrules)
library(janitor)
library(patchwork)
library(rstan)
library(bayesplot)
library(broom.mixed)

theme_set(theme_minimal(base_size = 16, base_family = "Atkinson Hyperlegible"))
```

## Plan for today

- Exam recap
- Evaluating regression models

## What is the purpose of exams? 

- Isolate the knowledge that you have in your brain that you can recall "on the fly", as opposed to with reference materials
- Provide the instructor with feedback about what concepts/topics the class as a whole understands, and which did not go as well. 
- Provide the student with feedback about what concepts/topics you understand, and which might need more time/practice
- Result in scores that summarize your performance about a (biased) selection of course material on a specific day
- Major contributor to your final course grade, which should provide a signal to outsiders about your mastery of the course content

## How I grade exams {.smaller}

- Take a first pass through all the problems
  - For open-ended questions, isolate key points that I'm looking for, and try to give out 50% credit if you hit the most important idea
  - Even if open-ended answer is very very close, take off a small amount of points for even small mistakes
- Check results of each question. If either test "fails", go back and look at the question and perhaps reassign point distribution. Sometimes make question a bonus point
  - multiple choice: majority of class got it right
  - open-ended: majority of class earned majority of the points
- Check overall distribution: 
  - Is the median at least in the upper 80's? 
  - Is the mean not too far from the median?
  - Is the middle 80% in a reasonable range? 

## Exam results


- Mean: 84.2%
- Median: 86.3%

```{r}
#| echo: false 

y <- c(56, 54.75, 42, 55.25, 56.25, 56, 52.75, 56.5, 45, 46.25, 56.5, 53.75, 63, 46.25, 43.75, 47.5, 55, 63, 57, 57.25, 62, 52.75, 60.5)/64

ggplot() + 
  stat_density(aes(x = y), adjust = 1.5, alpha = .8, fill = "darkblue") +
  xlim(c(0,1))
```


## How I interpret exam grades: {.smaller}

- **High 80s and above**: great job! You demonstrated a really solid understanding of the material. You might have made a few small mistakes or 1-2 larger errors, but I do not have major concerns about your understanding.
- **70 to mid-80s**: You probably had some high points and some weak points on the exam. I am not overly concerned about your learning, but there is room for improvement. A good final course grade is still well within your reach, but you might want to make some changes in how you're preparing for exams. 
- **Below 70**: There are either multiple key concepts that you are missing, or you had a bad exam day. To end up with a good final grade in the class, you'll want to make major changes to how you're approaching coursework and/or preparing for the next exam

## Corrections are designed to: 

- give you a chance to revisit key concepts that you missed on the exam
- mitigate the impact that a bad exam day has on your final grade

**If your score was below 80%**: 

- you can submit written corrections to earn back 30% of the points you lost, with corrected grades capped at 80% 

**If your score was below 88%**: 

- you can take a 15 minute make-up quiz on Friday of Week 7 to earn back 30% of the points you lost, with corrected grades capped at 88%


# Evaluating regression models {.colorslide}

## Last time: normal regression model

Normal regression model: 

$$Y_i | \beta_0, \beta_1, \sigma \sim N(\mu_i, \sigma^2)$$

$$\mu_i = \beta_0 + \beta_1 X_i$$


$$\beta_0 \sim N(m_0, s_0^2)$$

$$\beta_1 \sim N(m_1, s_1^2)$$

$$\sigma \sim \text{Exp}(l)$$

## Last time: bikeshare rides against feels like temperature

$$\beta_{0c} \sim N(5000,1000^2)$$

$$\beta_1 \sim N(100,40)$$

$$\sigma \sim \text{Exp}(1)$$

## Setup prior model with {rstanarm}

```{r}
library(rstanarm)
bike_prior <- stan_glm(rides ~ temp_feel, 
                       data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 1000),
                       prior = normal(100, 40), 
                       prior_aux = exponential(1),
                       prior_PD = TRUE,
                       chains = 4, iter = 5000*2, seed = 84735, refresh = 0)
```

## Check prior understanding

::: callout-warning
## Warning! 

Slightly different from code in book
:::

```{r}
library(tidybayes)
bikes |>
  add_epred_draws(bike_prior, ndraws = 200) |>
  ggplot(aes(x = temp_feel, y = rides)) +
    geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
```

## Data

```{r}
ggplot(bikes, aes(x = temp_feel, y = rides)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

## Fit with {rstanarm}

```{r}
bike_posterior <- update(bike_prior, prior_PD = FALSE)

tidy(bike_posterior, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```

## Simulated model lines

```{r}
bikes %>%
  add_epred_draws(bike_posterior, ndraws = 50) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
    geom_line(aes(y = .epred, group = .draw), alpha = 0.15) +
    geom_point(size = 0.05)
```

## Posterior prediction: "by hand"

If we know tomorrow is going to be 70 degrees in DC, what is a plausible range for the actual number of rides?

```{r}
#| output-location: slide


# Store the MCMC chains as a data frame
 bike_chains <- as.data.frame(bike_posterior)

# Simulate the posterior predictive model
 set.seed(84735)
 predict_70 <- bike_chains %>% 
   mutate(y_prediction = rnorm(20000, mean = `(Intercept)` + temp_feel * 70, sd = sigma))
 head(predict_70, 3)
```

## Posterior prediction: "by hand"

If we know tomorrow is going to be 70 degrees in DC, what is a plausible range for the actual number of rides?
  
```{r}
# Plot the posterior predictive model
 ggplot(predict_70, aes(x = y_prediction)) + 
   geom_density()
```

## Posterior prediction: shortcut

```{r}
# Simulate a set of predictions
set.seed(84735)
shortcut_prediction <- posterior_predict(
    bike_posterior, newdata = data.frame(temp_feel = 70))

# Construct a 90% posterior credible interval
posterior_interval(shortcut_prediction, prob = .8)

# Plot the approximate predictive model
mcmc_areas(shortcut_prediction, prob = .8)
```

## Posterior predictive check to assess model fit

We can also use posterior predictions *on the entire original dataset* to evaluate how well our model is performing. 

```{r}
first_set <- head(bike_chains, 1)

beta_0 <- first_set$`(Intercept)`
beta_1 <- first_set$temp_feel
sigma  <- first_set$sigma

set.seed(84735)
one_simulation <- bikes %>% 
  mutate(mu = beta_0 + beta_1 * temp_feel,
         simulated_rides = rnorm(500, mean = mu, sd = sigma)) %>% 
  select(temp_feel, rides, simulated_rides)

one_simulation
```

##

```{r}
ggplot(one_simulation, aes(x = simulated_rides)) + 
  geom_density(color = "lightblue") + 
  geom_density(aes(x = rides), color = "darkblue")
```

## PPCheck: shortcut

```{r}
pp_check(bike_posterior, nreps = 50) + 
  xlab("rides")
```
